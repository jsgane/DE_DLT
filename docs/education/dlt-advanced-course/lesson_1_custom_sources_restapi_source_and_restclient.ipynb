{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKD-8-XUjqU4"
      },
      "source": [
        "# **Building custom sources with [dlt REST API source](https://dlthub.com/docs/devel/dlt-ecosystem/verified-sources/rest_api/basic) and [RESTClient](https://dlthub.com/docs/devel/general-usage/http/rest-client)** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dlt-hub/dlt/blob/master/docs/education/dlt-advanced-course/lesson_1_custom_sources_restapi_source_and_restclient.ipynb) [![GitHub badge](https://img.shields.io/badge/github-view_source-2b3137?logo=github)](https://github.com/dlt-hub/dlt/blob/master/docs/education/dlt-advanced-course/lesson_1_custom_sources_restapi_source_and_restclient.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45PK4eXnVs16"
      },
      "source": [
        "# New section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdZWEzlGHX5b"
      },
      "source": [
        "# **Recap**\n",
        "\n",
        "In the **[dlt Fundamentals](https://github.com/dlt-hub/dlthub-education/tree/main/courses/dlt_fundamentals_dec_2024)** course, we learned two primary ways to build sources for REST APIs:\n",
        "\n",
        "1. **Using low-level dlt decorators** (`@dlt.source` and `@dlt.resource`) with [`RESTClient`](https://dlthub.com/docs/devel/general-usage/http/rest-client).\n",
        "2. **Using the built-in [`rest_api` source](https://dlthub.com/docs/devel/dlt-ecosystem/verified-sources/rest_api/basic)** with declarative configuration.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0eLEawNHZcL"
      },
      "source": [
        "\n",
        "### **1. Building sources with low-level dlt decorators**\n",
        "\n",
        "We constructed a custom source for the **GitHub API** using the `RESTClient` class, decorators like `@dlt.resource` and `@dlt.source`, and manual pagination handling.\n",
        "\n",
        "\n",
        "#### **Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pxl3VQtnHnRO"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install dlt[duckdb]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UnzHItdmHhnF",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60cc4d73-a6a3-45e1-f38f-fbe459aa8a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-12-04 09:29:57,500|[WARNING]|306|133375044358144|dlt|validate.py|verify_normalized_table:91|In schema `github_source`: The following columns in table 'github_events' did not receive any data during this load and therefore could not have their types inferred:\n",
            "  - payload__issue__milestone\n",
            "  - payload__issue__type\n",
            "  - payload__issue__active_lock_reason\n",
            "  - payload__issue__performed_via_github_app\n",
            "\n",
            "Unless type hints are provided, these columns will not be materialized in the destination.\n",
            "One way to provide type hints is to use the 'columns' argument in the '@dlt.resource' decorator.  For example:\n",
            "\n",
            "@dlt.resource(columns={'payload__issue__milestone': {'data_type': 'text'}})\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline rest_client_github load step completed in 4.71 seconds\n",
            "1 load package(s) were loaded to destination duckdb and into dataset rest_client_data_20251204092859\n",
            "The duckdb destination used duckdb:////content/rest_client_github.duckdb location to store data\n",
            "Load package 1764840539.824584 is LOADED and contains no failed jobs\n"
          ]
        }
      ],
      "source": [
        "from typing import Iterator, Any, Iterable\n",
        "import os\n",
        "import dlt\n",
        "from dlt.common.typing import TDataItems, TDataItem\n",
        "from dlt.sources import DltResource\n",
        "from dlt.sources.helpers import requests\n",
        "from dlt.sources.helpers.rest_client import RESTClient\n",
        "from dlt.sources.helpers.rest_client.auth import BearerTokenAuth\n",
        "from dlt.sources.helpers.rest_client.paginators import HeaderLinkPaginator\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "os.environ[\"ACCESS_TOKEN\"] = userdata.get(\"SECRET_KEY\")\n",
        "\n",
        "\n",
        "@dlt.source\n",
        "def github_source(access_token: str = dlt.secrets.value) -> Iterable[DltResource]:\n",
        "    client = RESTClient(\n",
        "        base_url=\"https://api.github.com\",\n",
        "        auth=BearerTokenAuth(token=access_token),\n",
        "        paginator=HeaderLinkPaginator(),\n",
        "    )\n",
        "\n",
        "    @dlt.resource\n",
        "    def github_events() -> Iterator[TDataItems]:\n",
        "        for page in client.paginate(\"orgs/dlt-hub/events\"):\n",
        "            yield page\n",
        "\n",
        "    @dlt.resource\n",
        "    def github_stargazers() -> Iterator[TDataItems]:\n",
        "        for page in client.paginate(\"repos/dlt-hub/dlt/stargazers\"):\n",
        "            yield page\n",
        "\n",
        "    return github_events, github_stargazers\n",
        "\n",
        "\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"rest_client_github\",\n",
        "    destination=\"duckdb\",\n",
        "    dataset_name=\"rest_client_data\",\n",
        "    dev_mode=True,\n",
        ")\n",
        "\n",
        "load_info = pipeline.run(github_source())\n",
        "print(load_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeZoTWeSLndh"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "### **2. Building sources with `rest_api` source**\n",
        "\n",
        "The **`rest_api` source** provides a higher-level, declarative approach to building sources for REST APIs. It's particularly suited for REST APIs with predictable structures and behaviors.\n",
        "\n",
        "\n",
        "#### **Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IIb5BiRyLqww",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b70251c8-ff42-4931-e990-082094fae26c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-12-04 09:46:36,107|[WARNING]|306|133375044358144|dlt|validate.py|verify_normalized_table:91|In schema `rest_api`: The following columns in table 'issues' did not receive any data during this load and therefore could not have their types inferred:\n",
            "  - milestone\n",
            "  - closed_at\n",
            "  - type\n",
            "  - active_lock_reason\n",
            "  - pull_request__merged_at\n",
            "  - closed_by\n",
            "  - performed_via_github_app\n",
            "  - milestone__due_on\n",
            "  - milestone__closed_at\n",
            "\n",
            "Unless type hints are provided, these columns will not be materialized in the destination.\n",
            "One way to provide type hints is to use the 'columns' argument in the '@dlt.resource' decorator.  For example:\n",
            "\n",
            "@dlt.resource(columns={'milestone': {'data_type': 'text'}})\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline rest_api_github load step completed in 1.90 seconds\n",
            "1 load package(s) were loaded to destination duckdb and into dataset rest_api_data_20251204094510\n",
            "The duckdb destination used duckdb:////content/rest_api_github.duckdb location to store data\n",
            "Load package 1764841510.5344028 is LOADED and contains no failed jobs\n"
          ]
        }
      ],
      "source": [
        "import dlt\n",
        "from dlt.sources.rest_api import RESTAPIConfig, rest_api_source\n",
        "\n",
        "config: RESTAPIConfig = {\n",
        "    \"client\": {\n",
        "        \"base_url\": \"https://api.github.com\",\n",
        "        \"auth\": {\n",
        "            \"token\": dlt.secrets[\"access_token\"],  # Access token configured above\n",
        "        },\n",
        "        \"paginator\": \"header_link\",\n",
        "    },\n",
        "    \"resources\": [\n",
        "        {\n",
        "            \"name\": \"issues\",\n",
        "            \"endpoint\": {\n",
        "                \"path\": \"repos/dlt-hub/dlt/issues\",\n",
        "                \"params\": {\"state\": \"open\"},\n",
        "            },\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"issue_comments\",\n",
        "            \"endpoint\": {\n",
        "                \"path\": \"repos/dlt-hub/dlt/issues/{issue_number}/comments\",\n",
        "                \"params\": {\n",
        "                    \"issue_number\": {\n",
        "                        \"type\": \"resolve\",\n",
        "                        \"resource\": \"issues\",\n",
        "                        \"field\": \"number\",\n",
        "                    },\n",
        "                },\n",
        "            },\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"contributors\",\n",
        "            \"endpoint\": {\"path\": \"repos/dlt-hub/dlt/contributors\"},\n",
        "        },\n",
        "    ],\n",
        "}\n",
        "\n",
        "git_source = rest_api_source(config)\n",
        "\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"rest_api_github\",\n",
        "    destination=\"duckdb\",\n",
        "    dataset_name=\"rest_api_data\",\n",
        "    dev_mode=True,\n",
        ")\n",
        "\n",
        "load_info = pipeline.run(git_source)\n",
        "print(load_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPbMniZ23Ulu"
      },
      "source": [
        "# **REST API Client by `dlt`**\n",
        "\n",
        "`dlt`’s REST API Client is the low level abstraction that powers the REST API Source. You can use it in your imperative code for more automation and brevity, if you do not wish to use the higher level declarative interface."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVJ-VU_53Ul4"
      },
      "source": [
        "If you don't like black boxes and prefer lower-level building blocks, then our `RESTClient` is perfect for you!\n",
        "\n",
        "The `RESTClient` class offers an Pythonic interface for interacting with RESTful APIs, including features like:\n",
        "\n",
        "- automatic pagination,\n",
        "- various authentication mechanisms,\n",
        "- customizable request/response handling.\n",
        "\n",
        "### What you’ll learn\n",
        "\n",
        "- How to authenticate with your API key\n",
        "- How to fetch paginated results using `RESTClient`\n",
        "- How to build a custom `@dlt.source`\n",
        "- How to run the pipeline and inspect the data\n",
        "\n",
        "For more information, read `dlt` [REST API Client](https://dlthub.com/devel/general-usage/http/rest-client) official documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je4-sJYNw22j"
      },
      "source": [
        "## **1. Creating a RESTClient instance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "wepadjopwiqT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a7828b4-5e97-4e97-9375-f22661f1c35f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3432',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3432/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3432/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3432/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/pull/3432',\n",
              "  'id': 3693985220,\n",
              "  'node_id': 'PR_kwDOGvRYu863CAnY',\n",
              "  'number': 3432,\n",
              "  'title': 'Add config value for Runtime CLI invite code',\n",
              "  'user': {'login': 'ivasio',\n",
              "   'id': 25770064,\n",
              "   'node_id': 'MDQ6VXNlcjI1NzcwMDY0',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/25770064?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/ivasio',\n",
              "   'html_url': 'https://github.com/ivasio',\n",
              "   'followers_url': 'https://api.github.com/users/ivasio/followers',\n",
              "   'following_url': 'https://api.github.com/users/ivasio/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/ivasio/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/ivasio/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/ivasio/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/ivasio/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/ivasio/repos',\n",
              "   'events_url': 'https://api.github.com/users/ivasio/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/ivasio/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': None,\n",
              "  'assignees': [],\n",
              "  'milestone': None,\n",
              "  'comments': 1,\n",
              "  'created_at': '2025-12-04T10:40:57Z',\n",
              "  'updated_at': '2025-12-04T11:01:50Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'CONTRIBUTOR',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'draft': False,\n",
              "  'pull_request': {'url': 'https://api.github.com/repos/dlt-hub/dlt/pulls/3432',\n",
              "   'html_url': 'https://github.com/dlt-hub/dlt/pull/3432',\n",
              "   'diff_url': 'https://github.com/dlt-hub/dlt/pull/3432.diff',\n",
              "   'patch_url': 'https://github.com/dlt-hub/dlt/pull/3432.patch',\n",
              "   'merged_at': None},\n",
              "  'body': '- Added new config value\\r\\n- Returned default beta API paths',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3432/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3432/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3431',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3431/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3431/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3431/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/pull/3431',\n",
              "  'id': 3693675330,\n",
              "  'node_id': 'PR_kwDOGvRYu863A85E',\n",
              "  'number': 3431,\n",
              "  'title': 'Fix: merge_columns not removing compound properties that should be removed',\n",
              "  'user': {'login': 'anuunchin',\n",
              "   'id': 88698977,\n",
              "   'node_id': 'MDQ6VXNlcjg4Njk4OTc3',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/88698977?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/anuunchin',\n",
              "   'html_url': 'https://github.com/anuunchin',\n",
              "   'followers_url': 'https://api.github.com/users/anuunchin/followers',\n",
              "   'following_url': 'https://api.github.com/users/anuunchin/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/anuunchin/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/anuunchin/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/anuunchin/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/anuunchin/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/anuunchin/repos',\n",
              "   'events_url': 'https://api.github.com/users/anuunchin/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/anuunchin/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': {'login': 'anuunchin',\n",
              "   'id': 88698977,\n",
              "   'node_id': 'MDQ6VXNlcjg4Njk4OTc3',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/88698977?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/anuunchin',\n",
              "   'html_url': 'https://github.com/anuunchin',\n",
              "   'followers_url': 'https://api.github.com/users/anuunchin/followers',\n",
              "   'following_url': 'https://api.github.com/users/anuunchin/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/anuunchin/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/anuunchin/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/anuunchin/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/anuunchin/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/anuunchin/repos',\n",
              "   'events_url': 'https://api.github.com/users/anuunchin/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/anuunchin/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'assignees': [{'login': 'anuunchin',\n",
              "    'id': 88698977,\n",
              "    'node_id': 'MDQ6VXNlcjg4Njk4OTc3',\n",
              "    'avatar_url': 'https://avatars.githubusercontent.com/u/88698977?v=4',\n",
              "    'gravatar_id': '',\n",
              "    'url': 'https://api.github.com/users/anuunchin',\n",
              "    'html_url': 'https://github.com/anuunchin',\n",
              "    'followers_url': 'https://api.github.com/users/anuunchin/followers',\n",
              "    'following_url': 'https://api.github.com/users/anuunchin/following{/other_user}',\n",
              "    'gists_url': 'https://api.github.com/users/anuunchin/gists{/gist_id}',\n",
              "    'starred_url': 'https://api.github.com/users/anuunchin/starred{/owner}{/repo}',\n",
              "    'subscriptions_url': 'https://api.github.com/users/anuunchin/subscriptions',\n",
              "    'organizations_url': 'https://api.github.com/users/anuunchin/orgs',\n",
              "    'repos_url': 'https://api.github.com/users/anuunchin/repos',\n",
              "    'events_url': 'https://api.github.com/users/anuunchin/events{/privacy}',\n",
              "    'received_events_url': 'https://api.github.com/users/anuunchin/received_events',\n",
              "    'type': 'User',\n",
              "    'user_view_type': 'public',\n",
              "    'site_admin': False}],\n",
              "  'milestone': None,\n",
              "  'comments': 1,\n",
              "  'created_at': '2025-12-04T09:27:01Z',\n",
              "  'updated_at': '2025-12-04T09:30:58Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'CONTRIBUTOR',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'draft': True,\n",
              "  'pull_request': {'url': 'https://api.github.com/repos/dlt-hub/dlt/pulls/3431',\n",
              "   'html_url': 'https://github.com/dlt-hub/dlt/pull/3431',\n",
              "   'diff_url': 'https://github.com/dlt-hub/dlt/pull/3431.diff',\n",
              "   'patch_url': 'https://github.com/dlt-hub/dlt/pull/3431.patch',\n",
              "   'merged_at': None},\n",
              "  'body': 'This PR adjusts the `def merge_columns()` function that previously handled things only additively. This caused an issue where compound properties, such as the `merge_key`, were being replaced with new values. \\r\\n\\r\\n\\r\\n\\r\\n\\r\\nResolved #3040 ',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3431/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3431/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3429',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3429/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3429/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3429/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/issues/3429',\n",
              "  'id': 3690853950,\n",
              "  'node_id': 'I_kwDOGvRYu87b_fY-',\n",
              "  'number': 3429,\n",
              "  'title': 'Incorrect assumption of NUMERIC data type with sql_database to Bigquery',\n",
              "  'user': {'login': 'lfpll',\n",
              "   'id': 6969067,\n",
              "   'node_id': 'MDQ6VXNlcjY5NjkwNjc=',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/6969067?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/lfpll',\n",
              "   'html_url': 'https://github.com/lfpll',\n",
              "   'followers_url': 'https://api.github.com/users/lfpll/followers',\n",
              "   'following_url': 'https://api.github.com/users/lfpll/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/lfpll/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/lfpll/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/lfpll/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/lfpll/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/lfpll/repos',\n",
              "   'events_url': 'https://api.github.com/users/lfpll/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/lfpll/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': None,\n",
              "  'assignees': [],\n",
              "  'milestone': None,\n",
              "  'comments': 0,\n",
              "  'created_at': '2025-12-03T16:04:47Z',\n",
              "  'updated_at': '2025-12-03T16:10:22Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'NONE',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'sub_issues_summary': {'total': 0, 'completed': 0, 'percent_completed': 0},\n",
              "  'issue_dependencies_summary': {'blocked_by': 0,\n",
              "   'total_blocked_by': 0,\n",
              "   'blocking': 0,\n",
              "   'total_blocking': 0},\n",
              "  'body': '### dlt version\\n\\n1.19.1\\n\\n### Describe the problem\\n\\nMigrating existing BigQuery table to dlt. A view has `NUMERIC` type, needs to be `BIGNUMERIC` in bigquery.\\n\\nBecause of a miss configuration where there is no precision or scale (because is a view on postgres), the column is not properly migrated to bigquery.\\nIt assumes a Numeric(38, 9) type which breaks the pipeline.\\n### Expected behavior\\n\\nThis is with jsonl, I tried with parquet and also doesn\\'t work.\\n\\nI tried with also different engines and I have multiple different errors\\n\\nMy error:\\n```\\n\\nJob with `job_id=test_numeric_view.9d08b3a21e.jsonl.gz` and `load_id=1764777469.951264` failed terminally with message: {\"error_result\":{\"reason\":\"invalid\",\"message\":\"Error while reading data, error message: JSON table encountered too many errors, giving up. Rows: 1; errors: 1. Please look into the errors[] collection for more details.\"},\"errors\":[{\"reason\":\"invalid\",\"message\":\"Error while reading data, error message: JSON table encountered too many errors, giving up. Rows: 1; errors: 1. Please look into the errors[] collection for more details.\"},{\"reason\":\"invalid\",\"message\":\"Error while reading data, error message: JSON processing encountered too many errors, giving up. Rows: 1; errors: 1; max bad: 0; error percent: 0\"},{\"reason\":\"invalidQuery\",\"location\":\"query\",\"message\":\"Invalid NUMERIC value: 999999999999999999.9999999999999999999 Field: large_decimal; Value: 999999999999999999.9999999999999999999\"}],\"job_start\":\"2025-12-03T15:57:54.571000Z\",\"job_end\":\"2025-12-03T15:57:56.156000Z\",\"job_id\":\"test_numeric_view_9d08b3a21e_0_jsonl_gz\"}. The package is aborted and cannot be retried.\\n```\\n\\n\\n\\n### Steps to reproduce\\n\\n\\n## Reproduction\\n\\n**`docker-compose.yml`**\\n```yaml\\nservices:\\n  postgres:\\n    image: postgres:15\\n    network_mode: host\\n    environment:\\n      POSTGRES_DB: testdb\\n      POSTGRES_USER: testuser\\n      POSTGRES_PASSWORD: testpass\\n    ports:\\n      - \"5432:5432\"\\n    volumes:\\n      - ./init.sql:/docker-entrypoint-initdb.d/init.sql\\n```\\n\\n**`init.sql`**\\n```sql\\nCREATE VIEW test_numeric_view AS\\nSELECT \\n    1 as id,\\n    999999999999999999.9999999999999999999::numeric as large_decimal;\\n```\\n\\n**`pipeline.py`**\\n```python\\n#!/usr/bin/env python3\\nimport dlt\\nfrom dlt.sources.sql_database import sql_database\\nfrom dlt.sources.credentials import ConnectionStringCredentials\\n\\npipeline = dlt.pipeline(\\n    pipeline_name=\"numeric_issue_test\",\\n    destination=\"bigquery\",\\n    dataset_name=\"test_numeric_issue\",\\n)\\n\\nsource = sql_database(\\n    table_names=[\"test_numeric_view\"],\\n    credentials=ConnectionStringCredentials(\\n        \"postgresql://testuser:testpass@localhost:5432/testdb\"\\n    ),\\n    backend=\"sqlalchemy\",\\n    include_views=True,\\n    reflection_level=\"full_with_precision\",\\n)\\n\\ninfo = pipeline.run(source)\\nprint(f\"Result: {info}\")\\n```\\n\\n\\n# Steps\\n\\n# Test Reproduction for DLT NUMERIC/BIGNUMERIC Issue\\n\\nThis folder contains all files needed to reproduce the issue.\\n\\n## Setup\\n\\n### 1. Configure BigQuery Credentials\\n\\nConfigure the config.toml with your bigquery details\\n\\nEdit `.dlt/secrets.toml` with your actual BigQuery credentials:\\n- `project_id`: Your GCP project ID\\n- `client_email`: Service account email\\n- `private_key`: Service account private key\\n\\n### 2. Start PostgreSQL\\n\\n```bash\\ndocker-compose up -d\\n```\\n\\nWait a few seconds for PostgreSQL to initialize.\\n\\n### 3. Install Python Dependencies\\n\\n```bash\\nuv init \\nuv add \"dlt[bigquery,sql-database,sqlalchemy]>=1.19.1\"\\n```\\n\\n## Reproduction Steps\\n\\n### Step 1: Create Existing BigQuery Table\\n\\nThis simulates the migration scenario where a table already exists:\\n\\n```bash\\nbq mk --table \\\\\\n  your-project:test_numeric_issue.test_numeric_view \\\\\\n  id:INTEGER,large_decimal:BIGNUMERIC\\n```\\n\\nReplace `your-project` with your actual GCP project ID.\\n\\n### Step 2: First DLT Run (Should Work)\\n\\n```bash\\npython pipeline.py\\n```\\n\\nThis should work fine and load data into the existing table.\\n\\n### Step 3: Run \\n\\nRun the pipeline\\n\\n```bash\\nuv run pipeline.py\\n```\\n\\nThis will popup the error\\n\\n## Cleanup\\n\\n```bash\\n# Stop PostgreSQL\\ndocker-compose down -v\\n\\n# Delete BigQuery table\\nbq rm -t your-project:test_numeric_issue.test_numeric_view\\n\\n# Delete dataset if needed\\nbq rm -r -d your-project:test_numeric_issue\\n```\\n\\n\\n\\n\\n\\n### Operating system\\n\\nLinux\\n\\n### Runtime environment\\n\\nLocal\\n\\n### Python version\\n\\n3.12\\n\\n### dlt data source\\n\\n_No response_\\n\\n### dlt destination\\n\\n_No response_\\n\\n### Other deployment details\\n\\n_No response_\\n\\n### Additional information\\n\\n_No response_',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3429/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3429/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3428',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3428/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3428/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3428/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/pull/3428',\n",
              "  'id': 3690747278,\n",
              "  'node_id': 'PR_kwDOGvRYu8623FK5',\n",
              "  'number': 3428,\n",
              "  'title': '(chore) adds hub extra',\n",
              "  'user': {'login': 'rudolfix',\n",
              "   'id': 17202864,\n",
              "   'node_id': 'MDQ6VXNlcjE3MjAyODY0',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/17202864?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/rudolfix',\n",
              "   'html_url': 'https://github.com/rudolfix',\n",
              "   'followers_url': 'https://api.github.com/users/rudolfix/followers',\n",
              "   'following_url': 'https://api.github.com/users/rudolfix/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/rudolfix/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/rudolfix/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/rudolfix/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/rudolfix/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/rudolfix/repos',\n",
              "   'events_url': 'https://api.github.com/users/rudolfix/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/rudolfix/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': {'login': 'rudolfix',\n",
              "   'id': 17202864,\n",
              "   'node_id': 'MDQ6VXNlcjE3MjAyODY0',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/17202864?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/rudolfix',\n",
              "   'html_url': 'https://github.com/rudolfix',\n",
              "   'followers_url': 'https://api.github.com/users/rudolfix/followers',\n",
              "   'following_url': 'https://api.github.com/users/rudolfix/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/rudolfix/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/rudolfix/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/rudolfix/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/rudolfix/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/rudolfix/repos',\n",
              "   'events_url': 'https://api.github.com/users/rudolfix/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/rudolfix/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'assignees': [{'login': 'rudolfix',\n",
              "    'id': 17202864,\n",
              "    'node_id': 'MDQ6VXNlcjE3MjAyODY0',\n",
              "    'avatar_url': 'https://avatars.githubusercontent.com/u/17202864?v=4',\n",
              "    'gravatar_id': '',\n",
              "    'url': 'https://api.github.com/users/rudolfix',\n",
              "    'html_url': 'https://github.com/rudolfix',\n",
              "    'followers_url': 'https://api.github.com/users/rudolfix/followers',\n",
              "    'following_url': 'https://api.github.com/users/rudolfix/following{/other_user}',\n",
              "    'gists_url': 'https://api.github.com/users/rudolfix/gists{/gist_id}',\n",
              "    'starred_url': 'https://api.github.com/users/rudolfix/starred{/owner}{/repo}',\n",
              "    'subscriptions_url': 'https://api.github.com/users/rudolfix/subscriptions',\n",
              "    'organizations_url': 'https://api.github.com/users/rudolfix/orgs',\n",
              "    'repos_url': 'https://api.github.com/users/rudolfix/repos',\n",
              "    'events_url': 'https://api.github.com/users/rudolfix/events{/privacy}',\n",
              "    'received_events_url': 'https://api.github.com/users/rudolfix/received_events',\n",
              "    'type': 'User',\n",
              "    'user_view_type': 'public',\n",
              "    'site_admin': False}],\n",
              "  'milestone': None,\n",
              "  'comments': 1,\n",
              "  'created_at': '2025-12-03T15:36:12Z',\n",
              "  'updated_at': '2025-12-03T16:30:13Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'COLLABORATOR',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'draft': False,\n",
              "  'pull_request': {'url': 'https://api.github.com/repos/dlt-hub/dlt/pulls/3428',\n",
              "   'html_url': 'https://github.com/dlt-hub/dlt/pull/3428',\n",
              "   'diff_url': 'https://github.com/dlt-hub/dlt/pull/3428.diff',\n",
              "   'patch_url': 'https://github.com/dlt-hub/dlt/pull/3428.patch',\n",
              "   'merged_at': None},\n",
              "  'body': '<!--\\r\\nThank you for submitting a pull request! Please provide a brief description of your changes below.\\r\\n-->\\r\\n### Description\\r\\nThis adds `hub` extra that points to plugins:\\r\\n- `dlthub` with matching on major+minor version\\r\\n- (runtime client cli) - still waiting\\r\\n\\r\\n**NOTE 1**\\r\\nI do not add any dev style dependencies to this extra like: fastmcp, marimo, pyarrow or duckdb. Those are still available via **workspace** extra. Users should be able to use this extra in production dependencies ie. when deploying pipelines.\\r\\n\\r\\n**NOTE 2**\\r\\n`hub` extra has very strict dependency specifier for plugins: they must match up to minor version (only patch may differ). So plugins (at least for now) are bound to dlt versions.\\r\\n\\r\\n`dlthub` plugin **does not** have dlt in the dependencies. Instead it checks `dlt` version on import and raises on mismatch. This prevents commands like this (we have it in our docs)\\r\\n```\\r\\npip install -U dlthub\\r\\n```\\r\\nfrom upgrading not only plugin but also dlt which is not what user wants. The mismatch message error gives instructions how to install in a correct way and how to upgrade just a minor version of a plugin.\\r\\n',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3428/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3428/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3427',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3427/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3427/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3427/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/issues/3427',\n",
              "  'id': 3690599194,\n",
              "  'node_id': 'I_kwDOGvRYu87b-hMa',\n",
              "  'number': 3427,\n",
              "  'title': 'State restoration fails silently when pipeline name contains double underscores (`__`)',\n",
              "  'user': {'login': 'timvink',\n",
              "   'id': 5570380,\n",
              "   'node_id': 'MDQ6VXNlcjU1NzAzODA=',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/5570380?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/timvink',\n",
              "   'html_url': 'https://github.com/timvink',\n",
              "   'followers_url': 'https://api.github.com/users/timvink/followers',\n",
              "   'following_url': 'https://api.github.com/users/timvink/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/timvink/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/timvink/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/timvink/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/timvink/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/timvink/repos',\n",
              "   'events_url': 'https://api.github.com/users/timvink/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/timvink/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': None,\n",
              "  'assignees': [],\n",
              "  'milestone': None,\n",
              "  'comments': 0,\n",
              "  'created_at': '2025-12-03T14:57:49Z',\n",
              "  'updated_at': '2025-12-03T15:06:03Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'NONE',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'sub_issues_summary': {'total': 0, 'completed': 0, 'percent_completed': 0},\n",
              "  'issue_dependencies_summary': {'blocked_by': 0,\n",
              "   'total_blocked_by': 0,\n",
              "   'blocking': 0,\n",
              "   'total_blocking': 0},\n",
              "  'body': '### dlt version\\n\\n1.19.1\\n\\n### Describe the problem\\n\\nWhen a pipeline name contains double underscores (`__`), state files are written successfully but cannot be restored on subsequent runs. This causes incremental loading to silently fail, re-processing all data from the beginning.\\n\\nThis issue only manifests when state sync from destination is required (e.g., running on job runners like Databricks, Airflow, or CI systems that start with a clean filesystem). When running locally with a persistent pipeline working directory, the local state file is used and everything works fine - which made this bug quite hard to find.\\n\\n## Root Cause\\n\\nThe filesystem destination uses `__` as `FILENAME_SEPARATOR` when naming state files:\\n\\nhttps://github.com/dlt-hub/dlt/blob/10cd9089d9da9c449a4195370d1547d9dfd2801e/dlt/destinations/impl/filesystem/filesystem.py#L87\\nhttps://github.com/dlt-hub/dlt/blob/10cd9089d9da9c449a4195370d1547d9dfd2801e/dlt/destinations/impl/filesystem/filesystem.py#L769-L774\\n\\nWhen parsing state files, dlt splits by `__` and expects exactly 3 parts:\\n\\nhttps://github.com/dlt-hub/dlt/blob/10cd9089d9da9c449a4195370d1547d9dfd2801e/dlt/destinations/impl/filesystem/filesystem.py#L727-L741\\n\\n(notice this bit:\\n\\n```python\\n        if len(fileparts) != 3:\\n            continue  # <-- Silently skips files that don\\'t have exactly 3 parts\\n```\\n\\nIf the pipeline name itself contains `__`, the split produces more than 3 parts, and the state file is silently skipped.\\n\\n### Example\\n\\nWith pipeline name `my_pipeline__resource`:\\n- State file written: `my_pipeline__resource__1234567890.123__abc123hash.jsonl`\\n- When split by `__`: `[\"my_pipeline\", \"resource\", \"1234567890.123\", \"abc123hash\"]` → 4 parts\\n- Expected: 3 parts → file is skipped\\n- Result: \"The state was not found in the destination\" even though it exists\\n\\n## Impact\\n\\n- Incremental loading breaks silently - users may not notice until they see duplicate data or performance issues\\n- The warning message \"state was not found\" is misleading since the state file does exist\\n- This is especially problematic when pipeline names are constructed dynamically (e.g., `f\"{source}_{table}\"` where components may contain underscores that get normalized to `__`)\\n- Debugging is difficult because the issue doesn\\'t reproduce locally\\n\\n## Environment\\n\\n- dlt version: 1.19.1 (likely affects all versions using filesystem destination)\\n- Destination: filesystem\\n- State sync: enabled (restore_from_destination=True)\\n- Runtime: Databricks job runner (clean filesystem between runs)\\n\\n### Expected behavior\\n\\nWe need to fail the run. And add validation in `dlt.pipeline()` to reject pipeline names containing `__`. \\n\\n### Steps to reproduce\\n\\nThis bash script:\\n\\n```bash\\n# Clean up from previous runs\\nrm -rf /tmp/test_dlt_destination\\nrm -rf ~/.dlt/pipelines/my_pipeline__with__underscores\\n\\n# Create the Python reproduction script\\ncat > /tmp/test_dlt_double_underscore.py << \\'EOF\\'\\n# /// script\\n# requires-python = \">=3.10\"\\n# dependencies = [\\n#     \"dlt[filesystem]==1.19.1\",\\n# ]\\n# ///\\nimport os\\n\\n# Enable dlt logging before importing dlt\\nos.environ[\"RUNTIME__LOG_LEVEL\"] = \"INFO\"\\n\\nimport dlt\\nfrom dlt.destinations import filesystem\\n\\n# Pipeline name with double underscores (common when normalizing dots or other characters)\\npipeline = dlt.pipeline(\\n    pipeline_name=\"my_pipeline__with__underscores\",\\n    destination=filesystem(bucket_url=\"/tmp/test_dlt_destination\"),\\n    dataset_name=\"test_data\"\\n)\\n\\n@dlt.resource\\ndef my_data():\\n    yield {\"id\": 1, \"value\": \"test\"}\\n\\n# Run the pipeline\\npipeline.run(my_data())\\nprint(\"Pipeline run completed\")\\nEOF\\n\\necho \"=== First run (writes state) ===\"\\nuv run /tmp/test_dlt_double_underscore.py 2>&1 | grep -i \"state\" || true\\n\\necho \"\"\\necho \"=== Checking state file exists ===\"\\nfind /tmp/test_dlt_destination -name \"*.jsonl\" -path \"*_dlt_pipeline_state*\" | head -5\\n\\necho \"\"\\necho \"=== Simulating clean job runner (deleting local pipeline state) ===\"\\nrm -rf ~/.dlt/pipelines/my_pipeline__with__underscores\\n\\necho \"\"\\necho \"=== Second run (should restore state but fails) ===\"\\nuv run /tmp/test_dlt_double_underscore.py 2>&1 | grep -i \"state\"\\n\\necho \"\"\\necho \"=== Notice: \\'state was not found\\' even though the state file exists! ===\"\\n```\\n\\nThis gives:\\n\\n```\\n=== First run (writes state) ===\\n2025-12-03 15:53:02,844|[INFO]|43342|8393318720|dlt|pipeline.py|_restore_state_from_destination:1606|The state was not found in the destination filesystem (dlt.destinations.filesystem):test_data\\n\\n=== Checking state file exists ===\\n/tmp/test_dlt_destination/test_data/_dlt_pipeline_state/my_pipeline__with__underscores__1764773497.998475__bb32ee1f4580d6fbb5aa3f8696c18dfd99e04f12f1507eb21b483bee81661dea.jsonl\\n/tmp/test_dlt_destination/test_data/_dlt_pipeline_state/my_pipeline__with__underscores__1764773520.456136__bb32ee1f4580d6fbb5aa3f8696c18dfd99e04f12f1507eb21b483bee81661dea.jsonl\\n\\n=== Simulating clean job runner (deleting local pipeline state) ===\\n\\n=== Second run (should restore state but fails) ===\\n2025-12-03 15:53:03,270|[INFO]|43348|8393318720|dlt|pipeline.py|_restore_state_from_destination:1606|The state was not found in the destination filesystem (dlt.destinations.filesystem):test_data\\n2025-12-03 15:53:03,314|[INFO]|43348|8393318720|dlt|worker.py|_get_items_normalizer:132|A file format for table _dlt_pipeline_state was specified to preferred in the resource so jsonl format being used.\\n2025-12-03 15:53:03,315|[INFO]|43348|8393318720|dlt|validate.py|validate_and_update_schema:26|Updating schema for table _dlt_pipeline_state with 1 deltas\\n2025-12-03 15:53:03,315|[INFO]|43348|8393318720|dlt|normalize.py|clean_x_normalizer:174|Table _dlt_pipeline_state has seen data for the first time with load id 1764773583.275845\\n2025-12-03 15:53:03,324|[INFO]|43348|8393318720|dlt|load.py|submit_job:172|Will load file 1764773583.275845/new_jobs/_dlt_pipeline_state.fdfa61a7f3.0.jsonl.gz with table name _dlt_pipeline_state\\n2025-12-03 15:53:03,324|[INFO]|43348|8393318720|dlt|load.py|complete_jobs:471|Job for _dlt_pipeline_state.fdfa61a7f3.jsonl.gz completed in load 1764773583.275845\\n\\n=== Notice: \\'state was not found\\' even though the state file exists! ===\\n```\\n\\n### Operating system\\n\\nLinux, macOS\\n\\n### Runtime environment\\n\\nOther\\n\\n### Python version\\n\\n3.12\\n\\n### dlt data source\\n\\ndoesn\\'t matter.\\n\\n### dlt destination\\n\\nFilesystem & buckets\\n\\n### Other deployment details\\n\\n_No response_\\n\\n### Additional information\\n\\n_No response_',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3427/reactions',\n",
              "   'total_count': 1,\n",
              "   '+1': 1,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3427/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3423',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3423/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3423/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3423/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/issues/3423',\n",
              "  'id': 3688289114,\n",
              "  'node_id': 'I_kwDOGvRYu87b1tNa',\n",
              "  'number': 3423,\n",
              "  'title': 'the pipeline show marimo add dark or light theme choose',\n",
              "  'user': {'login': 'bytepure',\n",
              "   'id': 221013699,\n",
              "   'node_id': 'U_kgDODSxmww',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/221013699?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/bytepure',\n",
              "   'html_url': 'https://github.com/bytepure',\n",
              "   'followers_url': 'https://api.github.com/users/bytepure/followers',\n",
              "   'following_url': 'https://api.github.com/users/bytepure/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/bytepure/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/bytepure/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/bytepure/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/bytepure/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/bytepure/repos',\n",
              "   'events_url': 'https://api.github.com/users/bytepure/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/bytepure/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': None,\n",
              "  'assignees': [],\n",
              "  'milestone': None,\n",
              "  'comments': 0,\n",
              "  'created_at': '2025-12-03T02:53:51Z',\n",
              "  'updated_at': '2025-12-03T02:53:51Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'NONE',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'sub_issues_summary': {'total': 0, 'completed': 0, 'percent_completed': 0},\n",
              "  'issue_dependencies_summary': {'blocked_by': 0,\n",
              "   'total_blocked_by': 0,\n",
              "   'blocking': 0,\n",
              "   'total_blocking': 0},\n",
              "  'body': '### Feature description\\n\\n``` bash\\ndlt pipeline github_api_pipeline show\\n```\\nthen look below\\n<img width=\"2752\" height=\"1471\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/e5cd4792-4ac0-46bb-8cca-59b499e78870\" />\\n\\n<img width=\"2316\" height=\"1307\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/988a98ed-14d6-4dc5-8865-d06fee526b38\" />\\n\\nso , add theme choose and improve dark theme \\n\\n### Are you a dlt user?\\n\\nYes, I\\'m already a dlt user.\\n\\n### Use case\\n\\n_No response_\\n\\n### Proposed solution\\n\\n_No response_\\n\\n### Related issues\\n\\n_No response_',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3423/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3423/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3422',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3422/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3422/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3422/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/pull/3422',\n",
              "  'id': 3688098488,\n",
              "  'node_id': 'PR_kwDOGvRYu862uK_a',\n",
              "  'number': 3422,\n",
              "  'title': 'docs: LLM workflow update',\n",
              "  'user': {'login': 'zilto',\n",
              "   'id': 68975210,\n",
              "   'node_id': 'MDQ6VXNlcjY4OTc1MjEw',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/68975210?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/zilto',\n",
              "   'html_url': 'https://github.com/zilto',\n",
              "   'followers_url': 'https://api.github.com/users/zilto/followers',\n",
              "   'following_url': 'https://api.github.com/users/zilto/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/zilto/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/zilto/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/zilto/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/zilto/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/zilto/repos',\n",
              "   'events_url': 'https://api.github.com/users/zilto/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/zilto/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [{'id': 3767923856,\n",
              "    'node_id': 'LA_kwDOGvRYu87glfSQ',\n",
              "    'url': 'https://api.github.com/repos/dlt-hub/dlt/labels/documentation',\n",
              "    'name': 'documentation',\n",
              "    'color': '0075ca',\n",
              "    'default': True,\n",
              "    'description': 'Improvements or additions to documentation'}],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': None,\n",
              "  'assignees': [],\n",
              "  'milestone': None,\n",
              "  'comments': 1,\n",
              "  'created_at': '2025-12-03T01:24:00Z',\n",
              "  'updated_at': '2025-12-04T01:40:50Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'COLLABORATOR',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'draft': True,\n",
              "  'pull_request': {'url': 'https://api.github.com/repos/dlt-hub/dlt/pulls/3422',\n",
              "   'html_url': 'https://github.com/dlt-hub/dlt/pull/3422',\n",
              "   'diff_url': 'https://github.com/dlt-hub/dlt/pull/3422.diff',\n",
              "   'patch_url': 'https://github.com/dlt-hub/dlt/pull/3422.patch',\n",
              "   'merged_at': None},\n",
              "  'body': 'Please remove the comments and use squash commits before merging to `devel`',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3422/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3422/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3421',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3421/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3421/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3421/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/pull/3421',\n",
              "  'id': 3687517318,\n",
              "  'node_id': 'PR_kwDOGvRYu862sQDG',\n",
              "  'number': 3421,\n",
              "  'title': 'Fix/3159 pydantic model incorrect serialization',\n",
              "  'user': {'login': 'tetelio',\n",
              "   'id': 22517436,\n",
              "   'node_id': 'MDQ6VXNlcjIyNTE3NDM2',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/22517436?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/tetelio',\n",
              "   'html_url': 'https://github.com/tetelio',\n",
              "   'followers_url': 'https://api.github.com/users/tetelio/followers',\n",
              "   'following_url': 'https://api.github.com/users/tetelio/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/tetelio/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/tetelio/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/tetelio/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/tetelio/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/tetelio/repos',\n",
              "   'events_url': 'https://api.github.com/users/tetelio/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/tetelio/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': None,\n",
              "  'assignees': [],\n",
              "  'milestone': None,\n",
              "  'comments': 1,\n",
              "  'created_at': '2025-12-02T20:59:41Z',\n",
              "  'updated_at': '2025-12-02T21:14:33Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'NONE',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'draft': False,\n",
              "  'pull_request': {'url': 'https://api.github.com/repos/dlt-hub/dlt/pulls/3421',\n",
              "   'html_url': 'https://github.com/dlt-hub/dlt/pull/3421',\n",
              "   'diff_url': 'https://github.com/dlt-hub/dlt/pull/3421.diff',\n",
              "   'patch_url': 'https://github.com/dlt-hub/dlt/pull/3421.patch',\n",
              "   'merged_at': None},\n",
              "  'body': '<!--\\r\\nThank you for submitting a pull request! Please provide a brief description of your changes below.\\r\\n-->\\r\\n### Description\\r\\nThis PR addresses an unexpected behavior when using a resource with a pydantic validator:\\r\\n\\r\\n```python\\r\\n@dlt.resource(columns=Model)\\r\\ndef rows():\\r\\n   yield Model\\r\\n```\\r\\n\\r\\nRight now, `dlt`:\\r\\n1. validates the data  \\r\\n2. always converts the validated object to a dict\\r\\n3. transformers receive dicts instead of model instances  \\r\\n4. user has to reconstruct Pydantic objects manually in every transformer  \\r\\n\\r\\nThis PR changes only the serialization behavior of the Pydantic validator:\\r\\n\\r\\n- If tuser yields a Pydantic model, dlt now returns a validated Pydantic model instead of a dict\\r\\n- If the user yields dicts, dlt continues to return dicts (unchanged)\\r\\n- If the validated model is not structurally compatible with the original model  \\r\\n  (i.e., some original fields were dropped by schema contract),  \\r\\n  then dlt returns a dict, to avoid returning partially invalid or broken model instances, so:\\r\\n\\r\\n```\\r\\nfields(original_model) ⊆ fields(validated_model)\\r\\n```\\r\\n\\r\\n<!--\\r\\nPlease link any related issues. This helps us keep the PR focused and merge it faster.\\r\\n-->\\r\\n### Related Issues\\r\\n\\r\\n- Fixes #3159\\r\\n\\r\\n<!--\\r\\nProvide any additional context about the PR here.\\r\\n-->\\r\\n\\r\\n<!--\\r\\nPlease ensure that\\r\\n    - you have read the [Contributing to dlt](../CONTRIBUTING.md) guide.\\r\\n    - you have run the tests locally and they have passed before submitting your PR.\\r\\n-->\\r\\n',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3421/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3421/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3416',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3416/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3416/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3416/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/issues/3416',\n",
              "  'id': 3686807077,\n",
              "  'node_id': 'I_kwDOGvRYu87bwDYl',\n",
              "  'number': 3416,\n",
              "  'title': 'Implement autogeneration and enable autocheck in CI for plugin docs (dlthub and dlt_runtime)',\n",
              "  'user': {'login': 'ivasio',\n",
              "   'id': 25770064,\n",
              "   'node_id': 'MDQ6VXNlcjI1NzcwMDY0',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/25770064?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/ivasio',\n",
              "   'html_url': 'https://github.com/ivasio',\n",
              "   'followers_url': 'https://api.github.com/users/ivasio/followers',\n",
              "   'following_url': 'https://api.github.com/users/ivasio/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/ivasio/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/ivasio/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/ivasio/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/ivasio/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/ivasio/repos',\n",
              "   'events_url': 'https://api.github.com/users/ivasio/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/ivasio/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': None,\n",
              "  'assignees': [],\n",
              "  'milestone': None,\n",
              "  'comments': 0,\n",
              "  'created_at': '2025-12-02T17:16:05Z',\n",
              "  'updated_at': '2025-12-02T17:16:05Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'CONTRIBUTOR',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'sub_issues_summary': {'total': 0, 'completed': 0, 'percent_completed': 0},\n",
              "  'issue_dependencies_summary': {'blocked_by': 0,\n",
              "   'total_blocked_by': 0,\n",
              "   'blocking': 0,\n",
              "   'total_blocking': 0},\n",
              "  'body': '\\nIn an attempt to implement it in the runtime branch (see https://github.com/dlt-hub/dlt/commit/b62f1dd63715b3810bfc5afb032acf0ba76b7b1a), a couple of problems surfaced:\\n- `license` command is not recognised in the workspace context, so dlthub CLI reference docs cannot be generated unless this command is excluded from the explicit list of commands\\n- `license` command is inconsistently appears and disappears in the OSS CLI reference docs\\n\\nAs a result of this task, consistent generation and CI check of plugged-in CLI commands should be implemented',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3416/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3416/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3414',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3414/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3414/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3414/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/pull/3414',\n",
              "  'id': 3686007669,\n",
              "  'node_id': 'PR_kwDOGvRYu862nIDw',\n",
              "  'number': 3414,\n",
              "  'title': 'feat/3187 add engine args sqlalchemy source and destination',\n",
              "  'user': {'login': 'tetelio',\n",
              "   'id': 22517436,\n",
              "   'node_id': 'MDQ6VXNlcjIyNTE3NDM2',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/22517436?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/tetelio',\n",
              "   'html_url': 'https://github.com/tetelio',\n",
              "   'followers_url': 'https://api.github.com/users/tetelio/followers',\n",
              "   'following_url': 'https://api.github.com/users/tetelio/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/tetelio/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/tetelio/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/tetelio/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/tetelio/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/tetelio/repos',\n",
              "   'events_url': 'https://api.github.com/users/tetelio/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/tetelio/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': None,\n",
              "  'assignees': [],\n",
              "  'milestone': None,\n",
              "  'comments': 1,\n",
              "  'created_at': '2025-12-02T14:03:30Z',\n",
              "  'updated_at': '2025-12-03T09:24:02Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'NONE',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'draft': False,\n",
              "  'pull_request': {'url': 'https://api.github.com/repos/dlt-hub/dlt/pulls/3414',\n",
              "   'html_url': 'https://github.com/dlt-hub/dlt/pull/3414',\n",
              "   'diff_url': 'https://github.com/dlt-hub/dlt/pull/3414.diff',\n",
              "   'patch_url': 'https://github.com/dlt-hub/dlt/pull/3414.patch',\n",
              "   'merged_at': None},\n",
              "  'body': '<!--\\r\\nThank you for submitting a pull request! Please provide a brief description of your changes below.\\r\\n-->\\r\\n### Description\\r\\ndlt users want to pass SQLAlchemy engine arguments for:\\r\\n1. SQL sources (sql_database and sql_table)\\r\\n2. SQL destination\\r\\n\\r\\n\\r\\n### Summary of Changes\\r\\n\\r\\nThis PR introduces support for configuring SQLAlchemy engine arguments across both sources and destinations, addressing a long-standing usability gap:\\r\\n\\r\\n- **Sources (`sql_database`, `sql_table`)**\\r\\n  - Added a new parameter `engine_kwargs`, forwarded directly to `sqlalchemy.create_engine()`.\\r\\n  - Supports configuring reflection behavior and extraction for the SQLAlchemy backend.\\r\\n  - Includes comprehensive tests:\\r\\n    - invalid kwargs propagation  \\r\\n    - reflection failure  \\r\\n    - SQLite timeout enforcement  \\r\\n    - interaction with backend_kwargs when using non-SQLAlchemy backends  \\r\\n    - SQLAlchemy echo logging behavior\\r\\n\\r\\n- **Destination (SQLAlchemy)**\\r\\n  - Adds missing documentation for existing `engine_args`.\\r\\n  - Adds tests to verify correct behavior for both in-memory and file-based SQLite destinations using `engine_args`.\\r\\n\\r\\n- **Documentation**\\r\\n  - Adds new docs for source-side `engine_kwargs`.\\r\\n  - Adds new docs for destination-side `engine_args`.\\r\\n  - Clarifies that source and destination engines are separate and configured independently.\\r\\n\\r\\n**Question:** To reduce user confusion, should the source parameter be named `engine_kwargs` (current) or unified as `engine_args` (matching the destination)?\\r\\n\\r\\n<!--\\r\\nPlease link any related issues. This helps us keep the PR focused and merge it faster.\\r\\n-->\\r\\n### Related Issues\\r\\n\\r\\n- Resolves #3187\\r\\n\\r\\n<!--\\r\\nProvide any additional context about the PR here.\\r\\n-->\\r\\n\\r\\n<!--\\r\\nPlease ensure that\\r\\n    - you have read the Contributing to dlt guide.\\r\\n    - you have run the tests locally and they have passed before submitting your PR.\\r\\n-->\\r\\n',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3414/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3414/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3413',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3413/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3413/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3413/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/pull/3413',\n",
              "  'id': 3685984233,\n",
              "  'node_id': 'PR_kwDOGvRYu862nC_4',\n",
              "  'number': 3413,\n",
              "  'title': 'data quality checks cell in dashboard',\n",
              "  'user': {'login': 'djudjuu',\n",
              "   'id': 9882716,\n",
              "   'node_id': 'MDQ6VXNlcjk4ODI3MTY=',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/9882716?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/djudjuu',\n",
              "   'html_url': 'https://github.com/djudjuu',\n",
              "   'followers_url': 'https://api.github.com/users/djudjuu/followers',\n",
              "   'following_url': 'https://api.github.com/users/djudjuu/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/djudjuu/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/djudjuu/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/djudjuu/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/djudjuu/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/djudjuu/repos',\n",
              "   'events_url': 'https://api.github.com/users/djudjuu/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/djudjuu/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': {'login': 'djudjuu',\n",
              "   'id': 9882716,\n",
              "   'node_id': 'MDQ6VXNlcjk4ODI3MTY=',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/9882716?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/djudjuu',\n",
              "   'html_url': 'https://github.com/djudjuu',\n",
              "   'followers_url': 'https://api.github.com/users/djudjuu/followers',\n",
              "   'following_url': 'https://api.github.com/users/djudjuu/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/djudjuu/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/djudjuu/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/djudjuu/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/djudjuu/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/djudjuu/repos',\n",
              "   'events_url': 'https://api.github.com/users/djudjuu/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/djudjuu/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'assignees': [{'login': 'djudjuu',\n",
              "    'id': 9882716,\n",
              "    'node_id': 'MDQ6VXNlcjk4ODI3MTY=',\n",
              "    'avatar_url': 'https://avatars.githubusercontent.com/u/9882716?v=4',\n",
              "    'gravatar_id': '',\n",
              "    'url': 'https://api.github.com/users/djudjuu',\n",
              "    'html_url': 'https://github.com/djudjuu',\n",
              "    'followers_url': 'https://api.github.com/users/djudjuu/followers',\n",
              "    'following_url': 'https://api.github.com/users/djudjuu/following{/other_user}',\n",
              "    'gists_url': 'https://api.github.com/users/djudjuu/gists{/gist_id}',\n",
              "    'starred_url': 'https://api.github.com/users/djudjuu/starred{/owner}{/repo}',\n",
              "    'subscriptions_url': 'https://api.github.com/users/djudjuu/subscriptions',\n",
              "    'organizations_url': 'https://api.github.com/users/djudjuu/orgs',\n",
              "    'repos_url': 'https://api.github.com/users/djudjuu/repos',\n",
              "    'events_url': 'https://api.github.com/users/djudjuu/events{/privacy}',\n",
              "    'received_events_url': 'https://api.github.com/users/djudjuu/received_events',\n",
              "    'type': 'User',\n",
              "    'user_view_type': 'public',\n",
              "    'site_admin': False}],\n",
              "  'milestone': None,\n",
              "  'comments': 1,\n",
              "  'created_at': '2025-12-02T13:56:54Z',\n",
              "  'updated_at': '2025-12-03T21:24:31Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'CONTRIBUTOR',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'draft': True,\n",
              "  'pull_request': {'url': 'https://api.github.com/repos/dlt-hub/dlt/pulls/3413',\n",
              "   'html_url': 'https://github.com/dlt-hub/dlt/pull/3413',\n",
              "   'diff_url': 'https://github.com/dlt-hub/dlt/pull/3413.diff',\n",
              "   'patch_url': 'https://github.com/dlt-hub/dlt/pull/3413.patch',\n",
              "   'merged_at': None},\n",
              "  'body': '- [x] show conditional cell\\r\\n- [x] render dq tables',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3413/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3413/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3410',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3410/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3410/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3410/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/pull/3410',\n",
              "  'id': 3682192608,\n",
              "  'node_id': 'PR_kwDOGvRYu862aQK8',\n",
              "  'number': 3410,\n",
              "  'title': 'Add `huggingface()` destination',\n",
              "  'user': {'login': 'lhoestq',\n",
              "   'id': 42851186,\n",
              "   'node_id': 'MDQ6VXNlcjQyODUxMTg2',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/42851186?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/lhoestq',\n",
              "   'html_url': 'https://github.com/lhoestq',\n",
              "   'followers_url': 'https://api.github.com/users/lhoestq/followers',\n",
              "   'following_url': 'https://api.github.com/users/lhoestq/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/lhoestq/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/lhoestq/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/lhoestq/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/lhoestq/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/lhoestq/repos',\n",
              "   'events_url': 'https://api.github.com/users/lhoestq/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/lhoestq/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': None,\n",
              "  'assignees': [],\n",
              "  'milestone': None,\n",
              "  'comments': 0,\n",
              "  'created_at': '2025-12-01T17:57:33Z',\n",
              "  'updated_at': '2025-12-02T15:26:08Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'NONE',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'draft': False,\n",
              "  'pull_request': {'url': 'https://api.github.com/repos/dlt-hub/dlt/pulls/3410',\n",
              "   'html_url': 'https://github.com/dlt-hub/dlt/pull/3410',\n",
              "   'diff_url': 'https://github.com/dlt-hub/dlt/pull/3410.diff',\n",
              "   'patch_url': 'https://github.com/dlt-hub/dlt/pull/3410.patch',\n",
              "   'merged_at': None},\n",
              "  'body': '<!--\\r\\nThank you for submitting a pull request! Please provide a brief description of your changes below.\\r\\n-->\\r\\n### Description\\r\\n\\r\\nAdd `huggingface()` destination\\r\\n\\r\\n```python\\r\\nfrom dlt.destinations import huggingface\\r\\n\\r\\ndestination = huggingface(dataset_name=\"lhoestq/dlt-experiment\")\\r\\n```\\r\\n\\r\\nIt writes the files as `\"data/{split}-xxxxx-of-nnnnn.parquet\"` to match the usual/default HF structure, but we could use whatever structure and add the associated file pattern in YAML (needed for the Viewer) instead if it makes sense.\\r\\n\\r\\nIt also adds the default Parquet value:\\r\\n- `use_content_defined_chunking=True` -> for [Parquet CDC](https://huggingface.co/blog/parquet-cdc)\\r\\n- `write_page_index=True` -> this allows better filtering and random access, esp. in the HF Dataset Viewer\\r\\n\\r\\n**Question 1**: `use_content_defined_chunking` is only available in recent versions of `pyarrow`, how would you like this to be handled ?\\r\\n\\r\\n**Question 2**: is there a way for the JobClient to retrieve information from the jobs ? I would like each job to pre-upload the files and the JobClient to `commit()` once the jobs are all done, and for this I need each job to return some information about the pre-upload step (more precisely, the id of each remote file after pre-upload)\\r\\n\\r\\n<!--\\r\\nPlease link any related issues. This helps us keep the PR focused and merge it faster.\\r\\n-->\\r\\n### Related Issues\\r\\n\\r\\n- related to https://github.com/dlt-hub/dlt/pull/3391 which adds HF filesystem support\\r\\n- continuation of https://github.com/dlt-hub/dlt/issues/1227\\r\\n\\r\\n<!--\\r\\nProvide any additional context about the PR here.\\r\\n-->\\r\\n### Additional Context\\r\\n\\r\\n<!--\\r\\nPlease ensure that\\r\\n    - you have read the [Contributing to dlt](../CONTRIBUTING.md) guide.\\r\\n    - you have run the tests locally and they have passed before submitting your PR.\\r\\n-->\\r\\n',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3410/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3410/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3405',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3405/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3405/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3405/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/issues/3405',\n",
              "  'id': 3678160043,\n",
              "  'node_id': 'I_kwDOGvRYu87bPESr',\n",
              "  'number': 3405,\n",
              "  'title': '[WIP] allow to add custom properties to pipeline runs',\n",
              "  'user': {'login': 'rudolfix',\n",
              "   'id': 17202864,\n",
              "   'node_id': 'MDQ6VXNlcjE3MjAyODY0',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/17202864?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/rudolfix',\n",
              "   'html_url': 'https://github.com/rudolfix',\n",
              "   'followers_url': 'https://api.github.com/users/rudolfix/followers',\n",
              "   'following_url': 'https://api.github.com/users/rudolfix/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/rudolfix/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/rudolfix/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/rudolfix/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/rudolfix/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/rudolfix/repos',\n",
              "   'events_url': 'https://api.github.com/users/rudolfix/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/rudolfix/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': None,\n",
              "  'assignees': [],\n",
              "  'milestone': None,\n",
              "  'comments': 0,\n",
              "  'created_at': '2025-11-30T18:10:55Z',\n",
              "  'updated_at': '2025-11-30T18:10:55Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'COLLABORATOR',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'sub_issues_summary': {'total': 0, 'completed': 0, 'percent_completed': 0},\n",
              "  'issue_dependencies_summary': {'blocked_by': 0,\n",
              "   'total_blocked_by': 0,\n",
              "   'blocking': 0,\n",
              "   'total_blocking': 0},\n",
              "  'body': '**Background**\\nWe want to be able to add custom properties to pipeline runs. Those would be:\\n* passed in pipeline trace in each participating steps\\n* passed to a load package that will carried along the execution in package state and stored in the destination.\\n\\nImmediate use case is to distinguish run types ie #3092 we run internal dataset pipeline to write data to a table.\\n\\nProperties are str:str mapping\\n\\nIn this ticket we just implement internal mechanism, user interface must be documented as experimental ie.\\n```py\\npipeline = dlt.pipeline(\"_ds_write\", _meta={\"_dlt_run_type\": \"write\"})\\n```\\n\\n**Tasks**\\n1. * [ ] Store `_meta` in pipeline instance.\\n2. * [ ] add it to load package state when a new package is created\\n3. * [ ] extract it into a `_dlt_load_properties` table and place in every package (see how state is extracted)\\n4. * [ ] add to each load steps trace when creating steps',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3405/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3405/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3404',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3404/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3404/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3404/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/pull/3404',\n",
              "  'id': 3676832011,\n",
              "  'node_id': 'PR_kwDOGvRYu862IgEQ',\n",
              "  'number': 3404,\n",
              "  'title': \"fix: 3276 - toggling dev_mode doesn't reset dataset_name causing inconsistencies of dataset naming behavior when creating a pipeline without dev_mode\",\n",
              "  'user': {'login': 'alkaline-0',\n",
              "   'id': 29667074,\n",
              "   'node_id': 'MDQ6VXNlcjI5NjY3MDc0',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/29667074?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/alkaline-0',\n",
              "   'html_url': 'https://github.com/alkaline-0',\n",
              "   'followers_url': 'https://api.github.com/users/alkaline-0/followers',\n",
              "   'following_url': 'https://api.github.com/users/alkaline-0/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/alkaline-0/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/alkaline-0/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/alkaline-0/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/alkaline-0/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/alkaline-0/repos',\n",
              "   'events_url': 'https://api.github.com/users/alkaline-0/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/alkaline-0/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': None,\n",
              "  'assignees': [],\n",
              "  'milestone': None,\n",
              "  'comments': 1,\n",
              "  'created_at': '2025-11-29T14:59:50Z',\n",
              "  'updated_at': '2025-12-01T10:03:00Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'CONTRIBUTOR',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'draft': False,\n",
              "  'pull_request': {'url': 'https://api.github.com/repos/dlt-hub/dlt/pulls/3404',\n",
              "   'html_url': 'https://github.com/dlt-hub/dlt/pull/3404',\n",
              "   'diff_url': 'https://github.com/dlt-hub/dlt/pull/3404.diff',\n",
              "   'patch_url': 'https://github.com/dlt-hub/dlt/pull/3404.patch',\n",
              "   'merged_at': None},\n",
              "  'body': '## Problem\\r\\n\\r\\nWhen a pipeline runs with dev_mode=True, its dataset_name is suffixed with a unique pipeline_instance_id. On subsequent runs with dev_mode=False, the suffix could persist via restored state, causing:\\r\\n\\r\\n1. Confusing dataset naming (dev suffix remains in non-dev runs)\\r\\n2. Inconsistent behavior across attaches/initializations\\r\\n3. State/schema mismatches after local wipes\\r\\n\\r\\n## Suggested solution\\r\\n\\r\\n1. We persist a _local[\"dev_mode\"] flag to detect the toggle and decide the reset early.\\r\\n2. Dev→non-dev is now a “hard reset”: we wipe the pipeline working directory and start from a clean state.\\r\\n3. After reset, we recompute dataset_name as if it were the first initialization, without the dev suffix.\\r\\n\\r\\n## Implementation details\\r\\nAdd dev_mode to TPipelineLocalState. \\r\\nHard reset on dev→non-dev\\r\\nIn _configure, If _local[\"dev_mode\"] was True and current self.dev_mode is False:\\r\\n\\r\\n- Wipe the working dir (_create_pipeline).\\r\\n- Replace the injected in-memory state (if available) with default_pipeline_state() to reflect a true fresh start.\\r\\n- recompute dataset_name (non-dev), then save.\\r\\n\\r\\n## Issues\\r\\nsolves #3276 \\r\\n\\r\\n',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3404/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3404/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3403',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3403/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3403/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3403/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/issues/3403',\n",
              "  'id': 3675951569,\n",
              "  'node_id': 'I_kwDOGvRYu87bGpHR',\n",
              "  'number': 3403,\n",
              "  'title': 'feat: add `dlt.Relation.join()`',\n",
              "  'user': {'login': 'zilto',\n",
              "   'id': 68975210,\n",
              "   'node_id': 'MDQ6VXNlcjY4OTc1MjEw',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/68975210?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/zilto',\n",
              "   'html_url': 'https://github.com/zilto',\n",
              "   'followers_url': 'https://api.github.com/users/zilto/followers',\n",
              "   'following_url': 'https://api.github.com/users/zilto/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/zilto/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/zilto/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/zilto/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/zilto/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/zilto/repos',\n",
              "   'events_url': 'https://api.github.com/users/zilto/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/zilto/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [{'id': 3767923858,\n",
              "    'node_id': 'LA_kwDOGvRYu87glfSS',\n",
              "    'url': 'https://api.github.com/repos/dlt-hub/dlt/labels/enhancement',\n",
              "    'name': 'enhancement',\n",
              "    'color': 'a2eeef',\n",
              "    'default': True,\n",
              "    'description': 'New feature or request'}],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': {'login': 'tetelio',\n",
              "   'id': 22517436,\n",
              "   'node_id': 'MDQ6VXNlcjIyNTE3NDM2',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/22517436?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/tetelio',\n",
              "   'html_url': 'https://github.com/tetelio',\n",
              "   'followers_url': 'https://api.github.com/users/tetelio/followers',\n",
              "   'following_url': 'https://api.github.com/users/tetelio/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/tetelio/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/tetelio/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/tetelio/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/tetelio/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/tetelio/repos',\n",
              "   'events_url': 'https://api.github.com/users/tetelio/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/tetelio/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'assignees': [{'login': 'tetelio',\n",
              "    'id': 22517436,\n",
              "    'node_id': 'MDQ6VXNlcjIyNTE3NDM2',\n",
              "    'avatar_url': 'https://avatars.githubusercontent.com/u/22517436?v=4',\n",
              "    'gravatar_id': '',\n",
              "    'url': 'https://api.github.com/users/tetelio',\n",
              "    'html_url': 'https://github.com/tetelio',\n",
              "    'followers_url': 'https://api.github.com/users/tetelio/followers',\n",
              "    'following_url': 'https://api.github.com/users/tetelio/following{/other_user}',\n",
              "    'gists_url': 'https://api.github.com/users/tetelio/gists{/gist_id}',\n",
              "    'starred_url': 'https://api.github.com/users/tetelio/starred{/owner}{/repo}',\n",
              "    'subscriptions_url': 'https://api.github.com/users/tetelio/subscriptions',\n",
              "    'organizations_url': 'https://api.github.com/users/tetelio/orgs',\n",
              "    'repos_url': 'https://api.github.com/users/tetelio/repos',\n",
              "    'events_url': 'https://api.github.com/users/tetelio/events{/privacy}',\n",
              "    'received_events_url': 'https://api.github.com/users/tetelio/received_events',\n",
              "    'type': 'User',\n",
              "    'user_view_type': 'public',\n",
              "    'site_admin': False}],\n",
              "  'milestone': None,\n",
              "  'comments': 0,\n",
              "  'created_at': '2025-11-28T21:17:18Z',\n",
              "  'updated_at': '2025-11-28T21:18:39Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'COLLABORATOR',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'sub_issues_summary': {'total': 0, 'completed': 0, 'percent_completed': 0},\n",
              "  'issue_dependencies_summary': {'blocked_by': 0,\n",
              "   'total_blocked_by': 0,\n",
              "   'blocking': 0,\n",
              "   'total_blocking': 0},\n",
              "  'body': '# Goal\\n`dlt.Schema.references` includes references that are explicit (e.g., defined by user via `@dlt.resource(references=...)` and implicit (e.g., child-parent and child-root, created during normalization). When interacting with loaded data via `dlt.Dataset` and `dlt.Relation` we can significantly simplify writing SQL joins for end users by leveraging existing references\\n\\nA lot of the work is completed in #2960 (see review comments)\\n\\n# Implementation\\n## Public API\\nThe user experience should look something like\\n```python\\ndataset = pipeline.dataset()\\n# parent-child\\ndataset.table(\"customers\").join(\"customers__items\")\\n# root-child\\ndataset.table(\"customers\").join(\"customers__items__details\")\\n# reference\\ndataset.table(\"customers\").join(\"orders\")\\n```\\n\\n- the same join logic from #3402 should be usable here\\n- names of the resulting table need to be properly namespaced (e.g., prefix `customers__`). This should use the same naming convention set on the schema (`dlt.Schema.naming`)\\n- For parent-child and root-child, should probably be a `LEFT` join by default. Could be overriden.\\n- The operation is not symmetric for non-, e.g., `dataset.table(\"customers\").join(\"customers__items\") != dataset.table(\"customers__items\").join(\"customers\")`\\n- Ideally, the column order is predictable. It seems convenient to have the join keys as left-most columns\\n\\n# Assumptions / test\\n- if a user manually edits data loaded by dlt, their might be joins bugs\\n\\n\\n# Currently out-of-scope / TODO\\n- Recursive `dataset.table(\"customers\").join(\"orders\").join(\"customers__items\")`. It\\'s ambiguous what the \"spine\" is where `orders` and `customer__items` are joined. Also, the result of `dataset.table(\"customers\").join(\"orders\")` now has namespaced columns that don\\'t match the original reference between `customers` and `customers__items`\\n- multi-join `dataset.table(\"customers\").join([\"orders\", \"customers__items\"])`. Here, the \"spine\" is unambiguous\\n- recursive and multi-join are more complex to namespace and avoid collisions',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3403/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3403/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3402',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3402/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3402/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3402/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/issues/3402',\n",
              "  'id': 3675914306,\n",
              "  'node_id': 'I_kwDOGvRYu87bGgBC',\n",
              "  'number': 3402,\n",
              "  'title': 'feat: internal `filter_loads(dataset, table_name, load_ids)`',\n",
              "  'user': {'login': 'zilto',\n",
              "   'id': 68975210,\n",
              "   'node_id': 'MDQ6VXNlcjY4OTc1MjEw',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/68975210?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/zilto',\n",
              "   'html_url': 'https://github.com/zilto',\n",
              "   'followers_url': 'https://api.github.com/users/zilto/followers',\n",
              "   'following_url': 'https://api.github.com/users/zilto/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/zilto/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/zilto/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/zilto/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/zilto/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/zilto/repos',\n",
              "   'events_url': 'https://api.github.com/users/zilto/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/zilto/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [{'id': 3767923858,\n",
              "    'node_id': 'LA_kwDOGvRYu87glfSS',\n",
              "    'url': 'https://api.github.com/repos/dlt-hub/dlt/labels/enhancement',\n",
              "    'name': 'enhancement',\n",
              "    'color': 'a2eeef',\n",
              "    'default': True,\n",
              "    'description': 'New feature or request'}],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': {'login': 'tetelio',\n",
              "   'id': 22517436,\n",
              "   'node_id': 'MDQ6VXNlcjIyNTE3NDM2',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/22517436?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/tetelio',\n",
              "   'html_url': 'https://github.com/tetelio',\n",
              "   'followers_url': 'https://api.github.com/users/tetelio/followers',\n",
              "   'following_url': 'https://api.github.com/users/tetelio/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/tetelio/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/tetelio/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/tetelio/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/tetelio/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/tetelio/repos',\n",
              "   'events_url': 'https://api.github.com/users/tetelio/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/tetelio/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'assignees': [{'login': 'tetelio',\n",
              "    'id': 22517436,\n",
              "    'node_id': 'MDQ6VXNlcjIyNTE3NDM2',\n",
              "    'avatar_url': 'https://avatars.githubusercontent.com/u/22517436?v=4',\n",
              "    'gravatar_id': '',\n",
              "    'url': 'https://api.github.com/users/tetelio',\n",
              "    'html_url': 'https://github.com/tetelio',\n",
              "    'followers_url': 'https://api.github.com/users/tetelio/followers',\n",
              "    'following_url': 'https://api.github.com/users/tetelio/following{/other_user}',\n",
              "    'gists_url': 'https://api.github.com/users/tetelio/gists{/gist_id}',\n",
              "    'starred_url': 'https://api.github.com/users/tetelio/starred{/owner}{/repo}',\n",
              "    'subscriptions_url': 'https://api.github.com/users/tetelio/subscriptions',\n",
              "    'organizations_url': 'https://api.github.com/users/tetelio/orgs',\n",
              "    'repos_url': 'https://api.github.com/users/tetelio/repos',\n",
              "    'events_url': 'https://api.github.com/users/tetelio/events{/privacy}',\n",
              "    'received_events_url': 'https://api.github.com/users/tetelio/received_events',\n",
              "    'type': 'User',\n",
              "    'user_view_type': 'public',\n",
              "    'site_admin': False}],\n",
              "  'milestone': None,\n",
              "  'comments': 0,\n",
              "  'created_at': '2025-11-28T20:55:22Z',\n",
              "  'updated_at': '2025-11-28T20:55:46Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'COLLABORATOR',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'sub_issues_summary': {'total': 0, 'completed': 0, 'percent_completed': 0},\n",
              "  'issue_dependencies_summary': {'blocked_by': 0,\n",
              "   'total_blocked_by': 0,\n",
              "   'blocking': 0,\n",
              "   'total_blocking': 0},\n",
              "  'body': '# Goal\\nWhen interacting with a `dlt.Dataset`, it\\'s useful to select data associated with specific specific `_dlt_load_id` value(s). This includes: incremental transformations, unit tests, metrics, data quality checks.\\n\\nWe should implement an internal function for this specific purpose.\\n\\n# Implementation\\nThe challenge is that only **root tables** have a `_dlt_load_id`. But all tables can be joined back to a root table.\\n\\nThe main function should look like:\\n```python\\ndef filter_loads(\\n  dataset: dlt.Dataset,\\n  table_name: str,\\n  load_ids: str | Collection[str],\\n) -> dlt.Relation: ...\\n```\\n\\nThis needs to handle 3 main distinct cases, which should have their own function:\\n1. `table_name` is a root table:  It includes a `_dlt_load_id` column. Filter directly\\n2. `table_name` is a child table with a `root_key`: It can be joined to its root table directly. Filter the root table and inner join with child table\\n3. `table_name` is a child table without a `root_key` but has a `parent_key`: Need to traverse child-parent relationships up to the root table. Filter the root table and apply inner joins in a loop\\n\\nAssertions / tests:\\n- before the main logic, we should check that provided `load_ids` exist in `_dlt_loads.load_id`\\n- raise if `table_name` doesn\\'t exist on `dataset.tables`. Use `ValueErrorWithKnownValues` and return the list of table names\\n- properly handle a list of load ids or a single string input; remember a `str` is a `Sequence`\\n- In case 3), it\\'s possible that the chain of joins \"breaks\" because there\\'s no matching `_dlt_load_id`. You can exist early and return an empty table\\n\\nOthers:\\n- the final table should have the same columns as `dataset.table()`, but also include a `_dlt_load_id` column\\n- use `sqlglot` for writing the queries\\n- the bulk of the code can be added to `dlt/dataset/relation.py` or `dlt/destinations/queries.py` for lower level sqlglot code\\n\\n# Relevant code / references\\n- A good amount of the necessary work exists in #2960;  But contrary to this PR, we don\\'t want to return joined tables.\\n- `dlt.Schema.references` returns the implicit references (parent-child, root-child)\\n- `dlt/common/schema/utils.py` has several functions to create references, identify root/parent/child key column, identify if table is root, retrieve children of table, etc. \\n\\n# Follow-up\\nIf we are confident, we could also introduce a new public API that uses this internal method\\n\\n```python\\nclass Relation:\\n  def filter_loads(self, load_ids: str | list[str]) -> dlt.Relation:\\n    return filter_loads(\\n     dataset=self._dataset,\\n     table_name=self._table_name,  # this only exists for tables created via `.table()`\\n     load_ids=load_ids,\\n   )\\n\\n# this would allow this API\\ndataset = pipeline.dataset()\\ncustomers_latest = dataset.table(\"customers\").filter_loads(...)\\n``` ',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3402/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3402/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3400',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3400/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3400/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3400/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/issues/3400',\n",
              "  'id': 3675524186,\n",
              "  'node_id': 'I_kwDOGvRYu87bFAxa',\n",
              "  'number': 3400,\n",
              "  'title': 'split the load step into separate staging and commit steps',\n",
              "  'user': {'login': 'rudolfix',\n",
              "   'id': 17202864,\n",
              "   'node_id': 'MDQ6VXNlcjE3MjAyODY0',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/17202864?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/rudolfix',\n",
              "   'html_url': 'https://github.com/rudolfix',\n",
              "   'followers_url': 'https://api.github.com/users/rudolfix/followers',\n",
              "   'following_url': 'https://api.github.com/users/rudolfix/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/rudolfix/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/rudolfix/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/rudolfix/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/rudolfix/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/rudolfix/repos',\n",
              "   'events_url': 'https://api.github.com/users/rudolfix/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/rudolfix/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': None,\n",
              "  'assignees': [],\n",
              "  'milestone': None,\n",
              "  'comments': 0,\n",
              "  'created_at': '2025-11-28T17:21:24Z',\n",
              "  'updated_at': '2025-11-28T17:21:24Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'COLLABORATOR',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'sub_issues_summary': {'total': 0, 'completed': 0, 'percent_completed': 0},\n",
              "  'issue_dependencies_summary': {'blocked_by': 0,\n",
              "   'total_blocked_by': 0,\n",
              "   'blocking': 0,\n",
              "   'total_blocking': 0},\n",
              "  'body': \"**Background**\\nCurrently load jobs are executed in parallel and only sequences if they depend on each other (followup jobs, table chain jobs). This optimizes for speed but there's a tradeoff for robustness: it may happen that staging job fails while the final destination data got modified. This puts the dataset in inconsistent state. In this ticket we add an option to run all staging jobs before committing the data to final dataset.\\n\\nImplementation should hide the split inside `Load.run`. All the calling code should see the same result then split is activated and when not. That also includes the returned metrics. This should facilitate testing. Split can be activated with new option `LoaderConfoguration` \\n\\n**Tasks**\\n1. Add a mode to `Load` (load.py) that will execute only:\\n- jobs that load to staging dataset and jobs that load to staging destination (we already flag staging jobs so the code is there) (**staging mode**)\\n- load only jobs that are not staging jobs (and immediately fails if it encounters any staging job) (**commit mode**)\\nWhen mode is not set we fall back to current operation.\\n2. If the mode is set, modify `Load.run` so it executes each loaded package in two calls: staging mode and commit mode \\n3. Make sure that metrics from two jobs are properly merged\\n4. Do not complete package in staging mode. let the commit mode run and do that!\\n5. We need to test this properly. For example all the dummy tests should pass in both modes\\n\\nThe complication is that package may contain ie. 20k jobs and we should find out a quick way to classify jobs (see `submit_job` code looks pretty fast and can be optimized to process a list)\",\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3400/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3400/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3397',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3397/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3397/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3397/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/issues/3397',\n",
              "  'id': 3674121405,\n",
              "  'node_id': 'I_kwDOGvRYu87a_qS9',\n",
              "  'number': 3397,\n",
              "  'title': 'cursor_path does not update pipeline state when using nested JSON (Apple Search Ads case)',\n",
              "  'user': {'login': 'victoredo',\n",
              "   'id': 185230044,\n",
              "   'node_id': 'U_kgDOCwpi3A',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/185230044?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/victoredo',\n",
              "   'html_url': 'https://github.com/victoredo',\n",
              "   'followers_url': 'https://api.github.com/users/victoredo/followers',\n",
              "   'following_url': 'https://api.github.com/users/victoredo/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/victoredo/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/victoredo/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/victoredo/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/victoredo/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/victoredo/repos',\n",
              "   'events_url': 'https://api.github.com/users/victoredo/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/victoredo/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [{'id': 3767923856,\n",
              "    'node_id': 'LA_kwDOGvRYu87glfSQ',\n",
              "    'url': 'https://api.github.com/repos/dlt-hub/dlt/labels/documentation',\n",
              "    'name': 'documentation',\n",
              "    'color': '0075ca',\n",
              "    'default': True,\n",
              "    'description': 'Improvements or additions to documentation'},\n",
              "   {'id': 3767923867,\n",
              "    'node_id': 'LA_kwDOGvRYu87glfSb',\n",
              "    'url': 'https://api.github.com/repos/dlt-hub/dlt/labels/question',\n",
              "    'name': 'question',\n",
              "    'color': 'd876e3',\n",
              "    'default': True,\n",
              "    'description': 'Further information is requested'}],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': None,\n",
              "  'assignees': [],\n",
              "  'milestone': None,\n",
              "  'comments': 3,\n",
              "  'created_at': '2025-11-28T08:57:27Z',\n",
              "  'updated_at': '2025-11-30T21:29:51Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'NONE',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'sub_issues_summary': {'total': 0, 'completed': 0, 'percent_completed': 0},\n",
              "  'issue_dependencies_summary': {'blocked_by': 0,\n",
              "   'total_blocked_by': 0,\n",
              "   'blocking': 0,\n",
              "   'total_blocking': 0},\n",
              "  'body': '### dlt version\\n\\ndlt 1.17.1\\n\\n### Describe the problem\\n\\nWhen using DLT’s REST API connector with nested JSON (Apple Search Ads reporting API), the cursor_path does not update correctly in pipeline state.\\nBecause of this, DLT does not recognize new dates inside the granularity[] array, even though they represent the actual incremental grain.\\n\\nThis results in:\\n\\nMissing days\\n\\nNo progression of the incremental state\\n\\nInconsistent re-runs and partial backfills\\n\\nForcing us to replace DLT’s incremental with our own manual date-range logic\\n\\nThis seems like a bug or missing capability in how cursor paths are resolved inside normalized nested records.\\n\\nContext\\n\\nApple Search Ads reporting API returns data like this:\\n```\\n{\\n  \"rows\": [\\n    {\\n      \"metadata\": {\\n        \"adGroupId\": 123,\\n        \"campaignId\": 456,\\n        \"orgId\": 999,\\n        \"countryOrRegion\": \"US\"\\n      },\\n      \"granularity\": [\\n        { \"date\": \"2024-05-12\", \"impressions\": 10 },\\n        { \"date\": \"2024-05-13\", \"impressions\": 20 }\\n      ]\\n    }\\n  ]\\n}\\n\\n```\\nThe actual incremental cursor must come from:\\ngranularity[*].date\\n\\nHowever, DLT stores incremental state based only on the parent object, not the nested metrics array. Because the parent metadata is static, DLT thinks nothing has changed and the incremental state never moves forward.\\n\\n```\\n{\\n                \"name\": \"adgroups\",\\n                \"endpoint\": {\\n                    \"path\": \"/reports/campaigns/{resources.campaigns.id}/adgroups\",\\n                    \"method\": \"POST\",\\n                    \"data_selector\": \"data.reportingDataResponse.row\",\\n                    \"incremental\": {\\n                        \"cursor_path\": \"[*].granularity[*].date\",\\n                        \"initial_value\": BACKFILL_START_DATE,\\n                        \"lag\": LAG_DAYS,\\n                  \\n                    },\\n                    \"headers\": {\\n                        \"X-AP-Context\": \"orgId={resources.campaigns.orgId}\",\\n                        \"Content-Type\": \"application/json\",\\n                        \"Accept\": \"application/json\",\\n                    },\\n                    \"json\": {\\n                        \"startTime\": adgroups_start_time\\n                        or \"{incremental.start_value}\",  \\n                        \"endTime\": adgroups_end_time or str(datetime.date.today()),\\n                        \"selector\": {\\n                            \"orderBy\": [{\"field\": \"countryOrRegion\", \"sortOrder\": \"ASCENDING\"}],\\n                            \"pagination\": {\"offset\": 0, \"limit\": json_page_limit},\\n                        },\\n                        \"groupBy\": [\"countryOrRegion\"],\\n                        \"granularity\": \"DAILY\",\\n                        \"timeZone\": \"UTC\",\\n                        \"returnRecordsWithNoMetrics\": False,\\n                        \"returnRowTotals\": False,\\n                        \"returnGrandTotals\": False,\\n                    },\\n                    \"paginator\": \"single_page\",\\n                    \"response_actions\": [\\n                        # the following error code are related to if a campaign from the campaign\\n                        # resource goes into the adgroup resource, but it does not exist\\n                        {\"status_code\": 400, \"action\": \"ignore\"},\\n                        {\"status_code\": 404, \"action\": \"ignore\"},\\n                    ],\\n                },\\n                \"include_from_parent\": [\"orgId\", \"id\"],\\n                \"parallelized\": True,\\n            },\\n```\\n\\n### Expected behavior\\n\\nDLT should:\\n\\n1. Correctly resolve cursor_path=\"granularity[*].date\"\\n\\n…even when nested inside arrays and normalized tables.\\n\\n2. Update pipeline state to the max date found in the child array.\\n3. Support cursor paths on child tables even when using a transformer or normalizer.\\n\\nActual Behavior\\n\\nPipeline state never advances\\n\\nDLT repeatedly fetches overlapping ranges\\n\\nMissing dates occur because state is not updated\\n\\nIncremental ingestion becomes non-deterministic\\n\\n### Steps to reproduce\\n\\n```\\n{\\n                \"name\": \"adgroups\",\\n                \"endpoint\": {\\n                    \"path\": \"/reports/campaigns/{resources.campaigns.id}/adgroups\",\\n                    \"method\": \"POST\",\\n                    \"data_selector\": \"data.reportingDataResponse.row\",\\n                    \"incremental\": {\\n                        \"cursor_path\": \"[*].granularity[*].date\",\\n                        \"initial_value\": BACKFILL_START_DATE,\\n                        \"lag\": LAG_DAYS,\\n                  \\n                    },\\n                    \"headers\": {\\n                        \"X-AP-Context\": \"orgId={resources.campaigns.orgId}\",\\n                        \"Content-Type\": \"application/json\",\\n                        \"Accept\": \"application/json\",\\n                    },\\n                    \"json\": {\\n                        \"startTime\": adgroups_start_time\\n                        or \"{incremental.start_value}\",  \\n                        \"endTime\": adgroups_end_time or str(datetime.date.today()),\\n                        \"selector\": {\\n                            \"orderBy\": [{\"field\": \"countryOrRegion\", \"sortOrder\": \"ASCENDING\"}],\\n                            \"pagination\": {\"offset\": 0, \"limit\": json_page_limit},\\n                        },\\n                        \"groupBy\": [\"countryOrRegion\"],\\n                        \"granularity\": \"DAILY\",\\n                        \"timeZone\": \"UTC\",\\n                        \"returnRecordsWithNoMetrics\": False,\\n                        \"returnRowTotals\": False,\\n                        \"returnGrandTotals\": False,\\n                    },\\n                    \"paginator\": \"single_page\",\\n                    \"response_actions\": [\\n                        # the following error code are related to if a campaign from the campaign\\n                        # resource goes into the adgroup resource, but it does not exist\\n                        {\"status_code\": 400, \"action\": \"ignore\"},\\n                        {\"status_code\": 404, \"action\": \"ignore\"},\\n                    ],\\n                },\\n                \"include_from_parent\": [\"orgId\", \"id\"],\\n                \"parallelized\": True,\\n            },\\n```\\n\\nProposed Solution\\nEnhance DLT’s incremental engine to support cursor paths inside nested arrays\\n\\nFor example:\\n\\ncursor_path=\"granularity[*].date\"\\n\\nshould:\\n\\nWalk the array\\n\\nResolve all dates\\n\\nTake the max\\n\\nPersist that value into pipeline state\\n\\nexactly as if the array had been flattened.\\n\\nAlternatively:\\nAllow a cursor path to refer to the flattened / normalized table produced by DLT’s own normalizer\\n\\nE.g.:\\n\\ncursor_path=\"adgroups_report__granularity.date\"\\n\\n\\nDLT already normalizes nested arrays into separate tables — so this should be referenceable.\\n\\nWhy This Matters\\n\\nMany real-world APIs (Apple, Google Ads, Meta, Amazon) return:\\n\\nParent metadata\\n\\nNested arrays containing the actual reporting grain\\n\\nThis pattern is extremely common.\\nWithout reliable nested cursor support, incremental ingestion fails for most adtech/reporting APIs.\\n\\n### Operating system\\n\\nOther\\n\\n### Runtime environment\\n\\nAirflow\\n\\n### Python version\\n\\n3.13\\n\\n### dlt data source\\n\\nApple search ads API\\n\\n### dlt destination\\n\\n_No response_\\n\\n### Other deployment details\\n\\n_No response_\\n\\n### Additional information\\n\\n_No response_',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3397/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3397/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3396',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3396/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3396/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3396/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/issues/3396',\n",
              "  'id': 3672281507,\n",
              "  'node_id': 'I_kwDOGvRYu87a4pGj',\n",
              "  'number': 3396,\n",
              "  'title': 'Support for Partition-level \"replace\" strategy (BigQuery OVERWRITE PARTITIONS)',\n",
              "  'user': {'login': 'bitrut',\n",
              "   'id': 283447,\n",
              "   'node_id': 'MDQ6VXNlcjI4MzQ0Nw==',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/283447?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/bitrut',\n",
              "   'html_url': 'https://github.com/bitrut',\n",
              "   'followers_url': 'https://api.github.com/users/bitrut/followers',\n",
              "   'following_url': 'https://api.github.com/users/bitrut/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/bitrut/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/bitrut/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/bitrut/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/bitrut/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/bitrut/repos',\n",
              "   'events_url': 'https://api.github.com/users/bitrut/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/bitrut/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [{'id': 3767923858,\n",
              "    'node_id': 'LA_kwDOGvRYu87glfSS',\n",
              "    'url': 'https://api.github.com/repos/dlt-hub/dlt/labels/enhancement',\n",
              "    'name': 'enhancement',\n",
              "    'color': 'a2eeef',\n",
              "    'default': True,\n",
              "    'description': 'New feature or request'},\n",
              "   {'id': 6597295039,\n",
              "    'node_id': 'LA_kwDOGvRYu88AAAABiTq7vw',\n",
              "    'url': 'https://api.github.com/repos/dlt-hub/dlt/labels/destination',\n",
              "    'name': 'destination',\n",
              "    'color': 'C5DEF5',\n",
              "    'default': False,\n",
              "    'description': 'Issue with a specific destination'}],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': None,\n",
              "  'assignees': [],\n",
              "  'milestone': None,\n",
              "  'comments': 0,\n",
              "  'created_at': '2025-11-27T16:13:24Z',\n",
              "  'updated_at': '2025-12-01T13:05:12Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'NONE',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'sub_issues_summary': {'total': 0, 'completed': 0, 'percent_completed': 0},\n",
              "  'issue_dependencies_summary': {'blocked_by': 0,\n",
              "   'total_blocked_by': 0,\n",
              "   'blocking': 0,\n",
              "   'total_blocking': 0},\n",
              "  'body': '### Feature description\\n\\nCurrently, the `write_disposition=\"replace\"` option in dlt performs a full truncation of the destination table before loading new data.\\n\\nHowever, a widespread pattern in data engineering (especially with BigQuery) is idempotent partition loading.\\n\\n### Are you a dlt user?\\n\\nYes, I run dlt in production.\\n\\n### Use case\\n\\nI receive a file daily containing events data for a specific date (e.g., `2023-01-01`).\\n\\nMy goal is to load this data into a partitioned BigQuery table. If I re-run the pipeline for that date, I want to overwrite only the `events$20230101` partition, preserving the history in the rest of the table.\\n\\nCurrent dlt behavior: If I use \"replace\", I lose my history (entire table gets dropped and populated with this one given date). If I use \"append\", I get duplicates if I re-run the job.\\n\\nI\\'ve tried to achieve that using `$` in a resource name, like this:\\n```python\\n@dlt.source\\ndef source():\\nfor day in gen_recent_days(3):\\n   yield resource(day=day).with_name(\\n        f\"events${day:%Y%m%d}\"\\n    )\\n```\\nAs a result, I\\'m getting sharding tables instead. The dollar sign gets replaced with `_` and for each resource gets loaded into a separate table: `events_20230101`, `events_20230102`, `events_20230103` etc.\\nI\\'d like each resource to get loaded to the same table `events` to another partition.\\n\\n### Proposed solution\\n\\nI would like a mechanism to instruct dlt to replace only the data corresponding to specific partition values, rather than the whole table.\\n\\nCurrently, the only way to achieve this in dlt is a manual workaround:\\n\\n1. Load data into a staging table with write_disposition=\"replace\".\\n2. Use pipeline.sql_client() to manually execute a BigQuery transaction that deletes the specific partition from the destination table and inserts from the staging table.\\n\\nExample:\\n```python\\nimport dlt\\nfrom dlt.destinations import bigquery\\nfrom google.cloud import bigquery\\n\\n# 1. Define your data and the target partition date\\ndata = [{\"id\": 1, \"name\": \"Alice\", \"date\": \"2023-01-01\"}, \\n        {\"id\": 2, \"name\": \"Bob\", \"date\": \"2023-01-01\"}]\\npartition_date = \"2023-01-01\"\\n\\n# 2. Configure the pipeline\\npipeline = dlt.pipeline(\\n    pipeline_name=\"my_partition_pipeline\",\\n    destination=\"bigquery\",\\n    dataset_name=\"my_dataset\"\\n)\\n\\n# 3. Load data to a STAGING table first\\n# We force the table name to be \\'my_table_staging\\'\\ninfo = pipeline.run(\\n    data,\\n    table_name=\"my_table_staging\", \\n    write_disposition=\"replace\" \\n)\\n\\nclient = bigquery.Client()\\n\\n# Define the query that selects your new data\\nsql = \"SELECT * FROM `project.dataset.my_table_staging`\"\\n\\n# Configure destination to be the SPECIFIC partition\\njob_config = bigquery.QueryJobConfig(\\n    destination=\"project.dataset.target_table$20230101\",\\n    write_disposition=\"WRITE_TRUNCATE\" # This overwrites ONLY the partition defined above\\n)\\n\\nquery_job = client.query(sql, job_config=job_config)\\nquery_job.result()\\nprint(\"Partition overwritten.\")\\n```\\n\\nThis works but requires writing custom, dialect-specific SQL for every pipeline, which reduces the benefit of using dlt as an abstraction layer.\\n\\nThis feature is critical for building idempotent incremental pipelines.\\n\\nHaving this abstractly available in dlt would be a massive improvement for incremental loading workflows.\\n\\n### Related issues\\n\\n_No response_',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3396/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3396/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3395',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3395/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3395/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3395/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/issues/3395',\n",
              "  'id': 3672232901,\n",
              "  'node_id': 'I_kwDOGvRYu87a4dPF',\n",
              "  'number': 3395,\n",
              "  'title': 'refine how workspace run context reports on profiles',\n",
              "  'user': {'login': 'rudolfix',\n",
              "   'id': 17202864,\n",
              "   'node_id': 'MDQ6VXNlcjE3MjAyODY0',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/17202864?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/rudolfix',\n",
              "   'html_url': 'https://github.com/rudolfix',\n",
              "   'followers_url': 'https://api.github.com/users/rudolfix/followers',\n",
              "   'following_url': 'https://api.github.com/users/rudolfix/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/rudolfix/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/rudolfix/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/rudolfix/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/rudolfix/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/rudolfix/repos',\n",
              "   'events_url': 'https://api.github.com/users/rudolfix/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/rudolfix/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': None,\n",
              "  'assignees': [],\n",
              "  'milestone': None,\n",
              "  'comments': 0,\n",
              "  'created_at': '2025-11-27T16:00:17Z',\n",
              "  'updated_at': '2025-11-27T16:00:17Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'COLLABORATOR',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'sub_issues_summary': {'total': 0, 'completed': 0, 'percent_completed': 0},\n",
              "  'issue_dependencies_summary': {'blocked_by': 0,\n",
              "   'total_blocked_by': 0,\n",
              "   'blocking': 0,\n",
              "   'total_blocking': 0},\n",
              "  'body': 'We need one more list of profiles that correspond to those that are actually present in workspace. Right now we are listing all pre-defined profiles + the one that is pinned.\\n\\n1. make config providers to list configured profiles (by inspecting the files that got found)\\n2. make workspace to scan `.var` to find our profiles that were used to actually run pipelines (for standard layout)\\n3. make workspace dashboard to show configured profiles instead of available ones.\\n\\n',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3395/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3395/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3394',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3394/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3394/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3394/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/issues/3394',\n",
              "  'id': 3672215181,\n",
              "  'node_id': 'I_kwDOGvRYu87a4Y6N',\n",
              "  'number': 3394,\n",
              "  'title': 'improve `pipeline.attach` error message',\n",
              "  'user': {'login': 'rudolfix',\n",
              "   'id': 17202864,\n",
              "   'node_id': 'MDQ6VXNlcjE3MjAyODY0',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/17202864?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/rudolfix',\n",
              "   'html_url': 'https://github.com/rudolfix',\n",
              "   'followers_url': 'https://api.github.com/users/rudolfix/followers',\n",
              "   'following_url': 'https://api.github.com/users/rudolfix/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/rudolfix/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/rudolfix/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/rudolfix/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/rudolfix/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/rudolfix/repos',\n",
              "   'events_url': 'https://api.github.com/users/rudolfix/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/rudolfix/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [{'id': 8461236092,\n",
              "    'node_id': 'LA_kwDOGvRYu88AAAAB-FQ3fA',\n",
              "    'url': 'https://api.github.com/repos/dlt-hub/dlt/labels/QoL',\n",
              "    'name': 'QoL',\n",
              "    'color': '39B2C7',\n",
              "    'default': False,\n",
              "    'description': 'Quality of Life: improve the developer experience'}],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': None,\n",
              "  'assignees': [],\n",
              "  'milestone': None,\n",
              "  'comments': 0,\n",
              "  'created_at': '2025-11-27T15:55:43Z',\n",
              "  'updated_at': '2025-11-27T15:55:52Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'COLLABORATOR',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'sub_issues_summary': {'total': 0, 'completed': 0, 'percent_completed': 0},\n",
              "  'issue_dependencies_summary': {'blocked_by': 0,\n",
              "   'total_blocked_by': 0,\n",
              "   'blocking': 0,\n",
              "   'total_blocking': 0},\n",
              "  'body': \"**Background**\\nA common problem with `pipeline.attach` is the lack of local pipeline state from which properties like destination and dataset names could be extracted. There's a fallback that will attempt to sync from destination if destination is provided (explicitly or configured).\\n\\nCurrent exception message does not explain what happened and how to remedy this\\nPipeline with `pipeline_name=eth_transform_pipeline` and `pipelines_dir=/tmp/.tmpIjHOe4/app/.dlt/.var/prod/pipelines/eth_transform_pipeline` could not be restored: the pipeline was not found in /tmp/.tmpIjHOe4/app/.dlt/.var/prod/pipelines/eth_transform_pipeline found and no destination was provided to restore from.\\n\\n**Tasks**\\nThere a few cases in the `pipeline.attach` that need improvement (see all the exception raised). We need to explain each case better.\\n1. no pipeline working dir and no destination: explain that user tried to attach to an existing pipeline and it seems that the pipeline was never run because its state and working dir is missing. Explain that we can obtain the state from destination but we need destination name/type and dataset name (in args to attach or via config)\\n2. in case when sync_destination happened: explain the above and that we tried destination + dataset but we could find pipeline state there (add names to exception message)\\n\\nIn both cases suggest that if user wants just a dataset, user should use `dlt.dataset()` which does not require pipeline state to access data\",\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3394/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3394/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3392',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3392/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3392/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3392/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/pull/3392',\n",
              "  'id': 3669324160,\n",
              "  'node_id': 'PR_kwDOGvRYu861vrzG',\n",
              "  'number': 3392,\n",
              "  'title': 'Implement advanced data masking for SQL sources',\n",
              "  'user': {'login': 'shanto12',\n",
              "   'id': 1020247,\n",
              "   'node_id': 'MDQ6VXNlcjEwMjAyNDc=',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/1020247?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/shanto12',\n",
              "   'html_url': 'https://github.com/shanto12',\n",
              "   'followers_url': 'https://api.github.com/users/shanto12/followers',\n",
              "   'following_url': 'https://api.github.com/users/shanto12/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/shanto12/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/shanto12/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/shanto12/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/shanto12/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/shanto12/repos',\n",
              "   'events_url': 'https://api.github.com/users/shanto12/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/shanto12/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': None,\n",
              "  'assignees': [],\n",
              "  'milestone': None,\n",
              "  'comments': 0,\n",
              "  'created_at': '2025-11-26T23:12:10Z',\n",
              "  'updated_at': '2025-11-26T23:12:10Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'NONE',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'draft': False,\n",
              "  'pull_request': {'url': 'https://api.github.com/repos/dlt-hub/dlt/pulls/3392',\n",
              "   'html_url': 'https://github.com/dlt-hub/dlt/pull/3392',\n",
              "   'diff_url': 'https://github.com/dlt-hub/dlt/pull/3392.diff',\n",
              "   'patch_url': 'https://github.com/dlt-hub/dlt/pull/3392.patch',\n",
              "   'merged_at': None},\n",
              "  'body': 'Added advanced data masking functionality for SQL databases, allowing flexible column masking with multiple methods and support for various data backends.\\r\\n\\r\\n<!--\\r\\nThank you for submitting a pull request! Please provide a brief description of your changes below.\\r\\n-->\\r\\n### Description\\r\\n\\r\\n\\r\\n<!--\\r\\nPlease link any related issues. This helps us keep the PR focused and merge it faster.\\r\\n-->\\r\\n### Related Issues\\r\\n- Resolves #2116- Fixes #...\\r\\n- Closes #...\\r\\n- Resolves #...\\r\\n\\r\\n<!--\\r\\nProvide any additional context about the PR here.\\r\\n-->\\r\\n### Additional Context\\r\\n\\r\\n<!--\\r\\nPlease ensure that\\r\\n    - you have read the [Contributing to dlt](../CONTRIBUTING.md) guide.\\r\\n    - you have run the tests locally and they have passed before submitting your PR.\\r\\n-->\\r\\n',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3392/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3392/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3391',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3391/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3391/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3391/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/pull/3391',\n",
              "  'id': 3668608950,\n",
              "  'node_id': 'PR_kwDOGvRYu861tTnS',\n",
              "  'number': 3391,\n",
              "  'title': 'Add Hugging Face storage',\n",
              "  'user': {'login': 'lhoestq',\n",
              "   'id': 42851186,\n",
              "   'node_id': 'MDQ6VXNlcjQyODUxMTg2',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/42851186?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/lhoestq',\n",
              "   'html_url': 'https://github.com/lhoestq',\n",
              "   'followers_url': 'https://api.github.com/users/lhoestq/followers',\n",
              "   'following_url': 'https://api.github.com/users/lhoestq/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/lhoestq/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/lhoestq/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/lhoestq/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/lhoestq/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/lhoestq/repos',\n",
              "   'events_url': 'https://api.github.com/users/lhoestq/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/lhoestq/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': None,\n",
              "  'assignees': [],\n",
              "  'milestone': None,\n",
              "  'comments': 3,\n",
              "  'created_at': '2025-11-26T18:24:22Z',\n",
              "  'updated_at': '2025-12-02T11:11:12Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'NONE',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'draft': False,\n",
              "  'pull_request': {'url': 'https://api.github.com/repos/dlt-hub/dlt/pulls/3391',\n",
              "   'html_url': 'https://github.com/dlt-hub/dlt/pull/3391',\n",
              "   'diff_url': 'https://github.com/dlt-hub/dlt/pull/3391.diff',\n",
              "   'patch_url': 'https://github.com/dlt-hub/dlt/pull/3391.patch',\n",
              "   'merged_at': None},\n",
              "  'body': '<!--\\r\\nThank you for submitting a pull request! Please provide a brief description of your changes below.\\r\\n-->\\r\\n### Description\\r\\n\\r\\nAdd support for Hugging Face storage (filesystem)\\r\\n\\r\\n```python\\r\\nfrom dlt.destinations import filesystem\\r\\n\\r\\ndestination = filesystem(f\"hf://datasets/{username}/{dataset_name}\")\\r\\n```\\r\\n\\r\\n<!--\\r\\nPlease link any related issues. This helps us keep the PR focused and merge it faster.\\r\\n-->\\r\\n### Related Issues\\r\\n\\r\\n- close the issue https://github.com/dlt-hub/dlt/issues/1227\\r\\n- this requires https://github.com/huggingface/huggingface_hub/pull/3575 to be merged and released (then happy to add the \"hf\" extra in pyproject.toml if it makes sense)\\r\\n\\r\\n<!--\\r\\nProvide any additional context about the PR here.\\r\\n-->\\r\\n### Additional Context\\r\\n\\r\\nthis allows HF users to make datasets on HF from anywhere, which they can use to train/eval AI models\\r\\n\\r\\nI was also thinking of defining a dedicated `huggingface` destination which would structure the files in \"the HF way\" (train/test separation, dataset viewer configuration, parquet optimizations), lmk if you think this could be a welcome contribution to push this further.\\r\\n\\r\\n```python\\r\\n# idea\\r\\nfrom dlt.destinations import huggingface\\r\\n\\r\\ndestination = huggingface(f\"{username}/{dataset_name}\")\\r\\n```\\r\\n\\r\\n<!--\\r\\nPlease ensure that\\r\\n    - you have read the [Contributing to dlt](../CONTRIBUTING.md) guide.\\r\\n    - you have run the tests locally and they have passed before submitting your PR.\\r\\n-->\\r\\n',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3391/reactions',\n",
              "   'total_count': 1,\n",
              "   '+1': 1,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3391/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3390',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3390/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3390/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3390/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/pull/3390',\n",
              "  'id': 3668129737,\n",
              "  'node_id': 'PR_kwDOGvRYu861rsb9',\n",
              "  'number': 3390,\n",
              "  'title': 'docs/run-in-snowflake-typo-fix',\n",
              "  'user': {'login': 'kaliole',\n",
              "   'id': 103940545,\n",
              "   'node_id': 'U_kgDOBjIBwQ',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/103940545?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/kaliole',\n",
              "   'html_url': 'https://github.com/kaliole',\n",
              "   'followers_url': 'https://api.github.com/users/kaliole/followers',\n",
              "   'following_url': 'https://api.github.com/users/kaliole/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/kaliole/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/kaliole/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/kaliole/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/kaliole/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/kaliole/repos',\n",
              "   'events_url': 'https://api.github.com/users/kaliole/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/kaliole/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': None,\n",
              "  'assignees': [],\n",
              "  'milestone': None,\n",
              "  'comments': 1,\n",
              "  'created_at': '2025-11-26T16:01:37Z',\n",
              "  'updated_at': '2025-11-26T16:07:26Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'CONTRIBUTOR',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'draft': False,\n",
              "  'pull_request': {'url': 'https://api.github.com/repos/dlt-hub/dlt/pulls/3390',\n",
              "   'html_url': 'https://github.com/dlt-hub/dlt/pull/3390',\n",
              "   'diff_url': 'https://github.com/dlt-hub/dlt/pull/3390.diff',\n",
              "   'patch_url': 'https://github.com/dlt-hub/dlt/pull/3390.patch',\n",
              "   'merged_at': None},\n",
              "  'body': '<!--\\r\\nThank you for submitting a pull request! Please provide a brief description of your changes below.\\r\\n-->\\r\\n### Description\\r\\n\\r\\n\\r\\n<!--\\r\\nPlease link any related issues. This helps us keep the PR focused and merge it faster.\\r\\n-->\\r\\n### Related Issues\\r\\n\\r\\n- Fixes #...\\r\\n- Closes #...\\r\\n- Resolves #...\\r\\n\\r\\n<!--\\r\\nProvide any additional context about the PR here.\\r\\n-->\\r\\n### Additional Context\\r\\n\\r\\n<!--\\r\\nPlease ensure that\\r\\n    - you have read the [Contributing to dlt](../CONTRIBUTING.md) guide.\\r\\n    - you have run the tests locally and they have passed before submitting your PR.\\r\\n-->\\r\\n',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3390/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3390/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3382',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3382/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3382/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3382/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/pull/3382',\n",
              "  'id': 3663341974,\n",
              "  'node_id': 'PR_kwDOGvRYu861bseu',\n",
              "  'number': 3382,\n",
              "  'title': 'feat: Support OAuth and base GCP credentials for BigQuery destination',\n",
              "  'user': {'login': 'daniel-nagish',\n",
              "   'id': 211275939,\n",
              "   'node_id': 'U_kgDODJfQow',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/211275939?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/daniel-nagish',\n",
              "   'html_url': 'https://github.com/daniel-nagish',\n",
              "   'followers_url': 'https://api.github.com/users/daniel-nagish/followers',\n",
              "   'following_url': 'https://api.github.com/users/daniel-nagish/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/daniel-nagish/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/daniel-nagish/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/daniel-nagish/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/daniel-nagish/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/daniel-nagish/repos',\n",
              "   'events_url': 'https://api.github.com/users/daniel-nagish/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/daniel-nagish/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': {'login': 'djudjuu',\n",
              "   'id': 9882716,\n",
              "   'node_id': 'MDQ6VXNlcjk4ODI3MTY=',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/9882716?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/djudjuu',\n",
              "   'html_url': 'https://github.com/djudjuu',\n",
              "   'followers_url': 'https://api.github.com/users/djudjuu/followers',\n",
              "   'following_url': 'https://api.github.com/users/djudjuu/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/djudjuu/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/djudjuu/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/djudjuu/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/djudjuu/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/djudjuu/repos',\n",
              "   'events_url': 'https://api.github.com/users/djudjuu/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/djudjuu/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'assignees': [{'login': 'djudjuu',\n",
              "    'id': 9882716,\n",
              "    'node_id': 'MDQ6VXNlcjk4ODI3MTY=',\n",
              "    'avatar_url': 'https://avatars.githubusercontent.com/u/9882716?v=4',\n",
              "    'gravatar_id': '',\n",
              "    'url': 'https://api.github.com/users/djudjuu',\n",
              "    'html_url': 'https://github.com/djudjuu',\n",
              "    'followers_url': 'https://api.github.com/users/djudjuu/followers',\n",
              "    'following_url': 'https://api.github.com/users/djudjuu/following{/other_user}',\n",
              "    'gists_url': 'https://api.github.com/users/djudjuu/gists{/gist_id}',\n",
              "    'starred_url': 'https://api.github.com/users/djudjuu/starred{/owner}{/repo}',\n",
              "    'subscriptions_url': 'https://api.github.com/users/djudjuu/subscriptions',\n",
              "    'organizations_url': 'https://api.github.com/users/djudjuu/orgs',\n",
              "    'repos_url': 'https://api.github.com/users/djudjuu/repos',\n",
              "    'events_url': 'https://api.github.com/users/djudjuu/events{/privacy}',\n",
              "    'received_events_url': 'https://api.github.com/users/djudjuu/received_events',\n",
              "    'type': 'User',\n",
              "    'user_view_type': 'public',\n",
              "    'site_admin': False}],\n",
              "  'milestone': None,\n",
              "  'comments': 0,\n",
              "  'created_at': '2025-11-25T14:38:27Z',\n",
              "  'updated_at': '2025-11-26T10:00:07Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'NONE',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'draft': False,\n",
              "  'pull_request': {'url': 'https://api.github.com/repos/dlt-hub/dlt/pulls/3382',\n",
              "   'html_url': 'https://github.com/dlt-hub/dlt/pull/3382',\n",
              "   'diff_url': 'https://github.com/dlt-hub/dlt/pull/3382.diff',\n",
              "   'patch_url': 'https://github.com/dlt-hub/dlt/pull/3382.patch',\n",
              "   'merged_at': None},\n",
              "  'body': \"Fixes #3380\\r\\n\\r\\n- Add Union type to allow GcpOAuthCredentials and GcpCredentials\\r\\n- Maintains backward compatibility with GcpServiceAccountCredentials\\r\\n- Enables OAuth authentication for Workload Identity Federation\\r\\n- Add tests for OAuth credentials acceptance\\r\\n\\r\\nThis change allows BigQuery destination to work with OAuth tokens from GitHub Actions Workload Identity Federation and other OAuth flows, without breaking existing service account authentication.\\r\\n\\r\\n### Description\\r\\n\\r\\nThis PR enables `BigQueryClientConfiguration` to accept OAuth and base GCP credentials in addition to service account credentials. This change is motivated by the need to support modern authentication patterns like GitHub Actions Workload Identity Federation, where OAuth access tokens are provided via service account impersonation.\\r\\n\\r\\n**Changes:**\\r\\n- Modified `BigQueryClientConfiguration.credentials` type from `GcpServiceAccountCredentials` to `Union[GcpServiceAccountCredentials, GcpOAuthCredentials, GcpCredentials]`\\r\\n- Added necessary imports for OAuth and base credential types\\r\\n- Added two tests to verify OAuth credentials are accepted\\r\\n- Fully backward compatible - all existing service account authentication continues to work\\r\\n\\r\\n**Why this matters:**\\r\\n- Aligns with Google's recommended Workload Identity Federation pattern (avoiding service account keys in secrets)\\r\\n- Matches dbt-bigquery's multi-auth approach (`service-account`, `oauth`, `oauth-secrets`)\\r\\n- Enables cloud-native CI/CD authentication without storing credentials\\r\\n\\r\\n### Related Issues\\r\\n\\r\\n- Fixes #3380\\r\\n\\r\\n### Additional Context\\r\\n\\r\\nThis change has been successfully tested in production with:\\r\\n- GitHub Actions + GCP Workload Identity Federation\\r\\n- OAuth access tokens from impersonated service accounts\\r\\n- Successful BigQuery loads using wrapped OAuth credentials\\r\\n- Backward compatibility verified with existing service account credentials\\r\\n\\r\\nThe implementation is minimal (3 lines changed in configuration.py) and maintains full backward compatibility, so existing pipelines are unaffected.\",\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3382/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3382/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3380',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3380/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3380/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3380/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/issues/3380',\n",
              "  'id': 3663302506,\n",
              "  'node_id': 'I_kwDOGvRYu87aWY9q',\n",
              "  'number': 3380,\n",
              "  'title': 'Support OAuth and base GCP credentials for BigQuery destination when using workload federation',\n",
              "  'user': {'login': 'daniel-nagish',\n",
              "   'id': 211275939,\n",
              "   'node_id': 'U_kgDODJfQow',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/211275939?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/daniel-nagish',\n",
              "   'html_url': 'https://github.com/daniel-nagish',\n",
              "   'followers_url': 'https://api.github.com/users/daniel-nagish/followers',\n",
              "   'following_url': 'https://api.github.com/users/daniel-nagish/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/daniel-nagish/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/daniel-nagish/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/daniel-nagish/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/daniel-nagish/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/daniel-nagish/repos',\n",
              "   'events_url': 'https://api.github.com/users/daniel-nagish/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/daniel-nagish/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': None,\n",
              "  'assignees': [],\n",
              "  'milestone': None,\n",
              "  'comments': 0,\n",
              "  'created_at': '2025-11-25T14:28:39Z',\n",
              "  'updated_at': '2025-11-25T14:28:39Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'NONE',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'sub_issues_summary': {'total': 0, 'completed': 0, 'percent_completed': 0},\n",
              "  'issue_dependencies_summary': {'blocked_by': 0,\n",
              "   'total_blocked_by': 0,\n",
              "   'blocking': 0,\n",
              "   'total_blocking': 0},\n",
              "  'body': '### Feature description\\n\\nCurrently, `BigQueryClientConfiguration` only accepts `GcpServiceAccountCredentials`, which limits authentication options for BigQuery destinations. This prevents using OAuth credentials or other GCP credential types that are useful in CI/CD environments with Workload Identity Federation.\\n- Similar to how dbt-bigquery supports multiple auth methods: `service-account`, `oauth`, `oauth-secrets`\\n- Aligns with Google\\'s recommendation to use Workload Identity Federation instead of service account keys\\n\\n### Are you a dlt user?\\n\\nYes, I\\'m already a dlt user.\\n\\n### Use case\\n\\nWhen using GitHub Actions with GCP Workload Identity Federation, the `google-github-actions/auth` action provides OAuth access tokens via service account impersonation. These tokens work perfectly with BigQuery but cannot be used with dlt because `BigQueryClientConfiguration.credentials` only accepts service account credentials.\\n**Current behavior:**\\n```python\\n# dlt/destinations/impl/bigquery/configuration.py\\nclass BigQueryClientConfiguration(DestinationClientDwhWithStagingConfiguration):\\n    credentials: GcpServiceAccountCredentials = None  # Only accepts service accounts\\n```\\n\\n**Error when trying to use OAuth credentials:**\\n```python\\nfrom dlt.common.configuration.specs import GcpOAuthCredentials\\nfrom dlt.destinations import bigquery\\n\\ncreds = GcpOAuthCredentials()\\ncreds.token = \"ya29....\"\\ncreds.project_id = \"my-project\"\\n\\ndestination = bigquery(credentials=creds)  # Type error\\n```\\n\\n### Proposed solution\\n\\n```python\\ncredentials: Union[GcpServiceAccountCredentials, GcpOAuthCredentials, GcpCredentials] = None\\n```\\nThis change:\\n- is backward compatible - service account credentials continue to work exactly as before\\n- enables OAuth authentication - useful for Workload Identity Federation in CI/CD\\n- Follows dlt patterns - aligns with dlt\\'s GCP credential hierarchy (`GcpCredentials` base class)\\n- Matches industry patterns - similar to dbt-bigquery\\'s `oauth-secrets` authentication method\\n\\n### Related issues\\n\\n_No response_',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3380/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3380/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3377',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3377/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3377/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3377/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/issues/3377',\n",
              "  'id': 3662256835,\n",
              "  'node_id': 'I_kwDOGvRYu87aSZrD',\n",
              "  'number': 3377,\n",
              "  'title': 'BigQuery schema evolution failing with partitioned table',\n",
              "  'user': {'login': 'kien-truong-pguru',\n",
              "   'id': 144192449,\n",
              "   'node_id': 'U_kgDOCJgzwQ',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/144192449?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/kien-truong-pguru',\n",
              "   'html_url': 'https://github.com/kien-truong-pguru',\n",
              "   'followers_url': 'https://api.github.com/users/kien-truong-pguru/followers',\n",
              "   'following_url': 'https://api.github.com/users/kien-truong-pguru/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/kien-truong-pguru/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/kien-truong-pguru/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/kien-truong-pguru/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/kien-truong-pguru/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/kien-truong-pguru/repos',\n",
              "   'events_url': 'https://api.github.com/users/kien-truong-pguru/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/kien-truong-pguru/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [{'id': 3767923855,\n",
              "    'node_id': 'LA_kwDOGvRYu87glfSP',\n",
              "    'url': 'https://api.github.com/repos/dlt-hub/dlt/labels/bug',\n",
              "    'name': 'bug',\n",
              "    'color': 'd73a4a',\n",
              "    'default': True,\n",
              "    'description': \"Something isn't working\"}],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': None,\n",
              "  'assignees': [],\n",
              "  'milestone': None,\n",
              "  'comments': 1,\n",
              "  'created_at': '2025-11-25T09:42:59Z',\n",
              "  'updated_at': '2025-12-04T09:37:42Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'NONE',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'sub_issues_summary': {'total': 0, 'completed': 0, 'percent_completed': 0},\n",
              "  'issue_dependencies_summary': {'blocked_by': 0,\n",
              "   'total_blocked_by': 0,\n",
              "   'blocking': 0,\n",
              "   'total_blocking': 0},\n",
              "  'body': '### dlt version\\n\\n1.18.2\\n\\n### Describe the problem\\n\\nVery similar to: https://github.com/dlt-hub/dlt/issues/3236\\n\\nTrying to add new columns on a BigQuery partitioned table will generate queries such as:\\n\\n```\\nALTER TABLE `project.dataset.table`\\nADD COLUMN `col1` BOOL \\nPARTITION BY RANGE_BUCKET(`PartitionColumn`, GENERATE_ARRAY(1388534400, 2650838400, 31536000));\\n```\\n\\nWhich is invalid and will trigger an error:\\n\\n```\\nSyntax error: Expected end of input but got keyword PARTITION\\n```\\n\\n### Expected behavior\\n\\nNo errors when adding new columns to BigQuery table\\n\\n### Steps to reproduce\\n\\n1. Load data into a BigQuery partitioned table\\n2. Load a new batch of data containing new columns\\n3. The error is thrown\\n\\n```\\nSyntax error: Expected end of input but got keyword PARTITION\\n```\\n\\n### Operating system\\n\\nLinux\\n\\n### Runtime environment\\n\\nLocal\\n\\n### Python version\\n\\n3.13\\n\\n### dlt data source\\n\\n_No response_\\n\\n### dlt destination\\n\\nGoogle BigQuery\\n\\n### Other deployment details\\n\\n_No response_\\n\\n### Additional information\\n\\n_No response_',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3377/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3377/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3376',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3376/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3376/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3376/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/issues/3376',\n",
              "  'id': 3660813756,\n",
              "  'node_id': 'I_kwDOGvRYu87aM5W8',\n",
              "  'number': 3376,\n",
              "  'title': 'append pipeline state to the relevant load package when extracting',\n",
              "  'user': {'login': 'rudolfix',\n",
              "   'id': 17202864,\n",
              "   'node_id': 'MDQ6VXNlcjE3MjAyODY0',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/17202864?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/rudolfix',\n",
              "   'html_url': 'https://github.com/rudolfix',\n",
              "   'followers_url': 'https://api.github.com/users/rudolfix/followers',\n",
              "   'following_url': 'https://api.github.com/users/rudolfix/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/rudolfix/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/rudolfix/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/rudolfix/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/rudolfix/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/rudolfix/repos',\n",
              "   'events_url': 'https://api.github.com/users/rudolfix/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/rudolfix/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [{'id': 3767923855,\n",
              "    'node_id': 'LA_kwDOGvRYu87glfSP',\n",
              "    'url': 'https://api.github.com/repos/dlt-hub/dlt/labels/bug',\n",
              "    'name': 'bug',\n",
              "    'color': 'd73a4a',\n",
              "    'default': True,\n",
              "    'description': \"Something isn't working\"}],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': None,\n",
              "  'assignees': [],\n",
              "  'milestone': None,\n",
              "  'comments': 0,\n",
              "  'created_at': '2025-11-24T22:50:34Z',\n",
              "  'updated_at': '2025-11-24T22:50:34Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'COLLABORATOR',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'sub_issues_summary': {'total': 0, 'completed': 0, 'percent_completed': 0},\n",
              "  'issue_dependencies_summary': {'blocked_by': 0,\n",
              "   'total_blocked_by': 0,\n",
              "   'blocking': 0,\n",
              "   'total_blocking': 0},\n",
              "  'body': '**Background**\\nPipeline state is extracted and loaded together with the data. The idea is the load packages are self contained and loading them fully keeps the dataset and pipeline in consistent state. Packages should not depend on other packages to be consistent.\\nCurrently extract method will store the state with the package that contains data for the default schema and this state is extracted only once at the end after extraction of data ended. For multi schema pipelines state is not attached to packages for the other schemas. This needs to be fixed.\\n\\n**Tasks**\\n\\n1. * [ ] pipeline extract source by source using `for source in data_to_sources(` we must order sources returned `data_to_sources` by schema so if there are many instances of the same source they will be extracted one after another.\\n2. * [ ] call `_bump_version_and_extract_state` in a loop when you detect that schema name changed. this makes sure we write state only once per package. also pass the schema from the source to direct extracted state to the right package.\\n3. * [ ] if `pipeline.config.use_single_dataset` is False we must keep old behavior (state extracted only once to the default schema\\n\\n**Tests**\\nThere are very detailed tests in `test_pipeline_state` and test for many schemas may fail. Those failing tests must be carefully analyzed and fixed. On top of that:\\n\\n1. we must make sure that `data_to_sources` orders returned sources by schema name\\n2. we must make sure that if we extract multiple sources with the same schema, state is extracted only once\\n3. if we extract multiple source with different names (schemas) each package contains state (if source extraction modified it!). this also means that both schemas will contain `_dlt_pipeline_state` which actually makes sense\\n4. if sources do not modify state (ie. no inremental loading) only the first package contain state (the initial version)\\n\\nand probably more',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3376/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3376/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3375',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3375/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3375/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3375/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/issues/3375',\n",
              "  'id': 3660762428,\n",
              "  'node_id': 'I_kwDOGvRYu87aMs08',\n",
              "  'number': 3375,\n",
              "  'title': 'abort packages manually by default',\n",
              "  'user': {'login': 'rudolfix',\n",
              "   'id': 17202864,\n",
              "   'node_id': 'MDQ6VXNlcjE3MjAyODY0',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/17202864?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/rudolfix',\n",
              "   'html_url': 'https://github.com/rudolfix',\n",
              "   'followers_url': 'https://api.github.com/users/rudolfix/followers',\n",
              "   'following_url': 'https://api.github.com/users/rudolfix/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/rudolfix/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/rudolfix/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/rudolfix/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/rudolfix/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/rudolfix/repos',\n",
              "   'events_url': 'https://api.github.com/users/rudolfix/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/rudolfix/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [{'id': 3767923855,\n",
              "    'node_id': 'LA_kwDOGvRYu87glfSP',\n",
              "    'url': 'https://api.github.com/repos/dlt-hub/dlt/labels/bug',\n",
              "    'name': 'bug',\n",
              "    'color': 'd73a4a',\n",
              "    'default': True,\n",
              "    'description': \"Something isn't working\"},\n",
              "   {'id': 7381676682,\n",
              "    'node_id': 'LA_kwDOGvRYu88AAAABt_tyig',\n",
              "    'url': 'https://api.github.com/repos/dlt-hub/dlt/labels/breaking',\n",
              "    'name': 'breaking',\n",
              "    'color': 'D47928',\n",
              "    'default': False,\n",
              "    'description': 'This issue introduces breaking change'}],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': None,\n",
              "  'assignees': [],\n",
              "  'milestone': None,\n",
              "  'comments': 0,\n",
              "  'created_at': '2025-11-24T22:31:07Z',\n",
              "  'updated_at': '2025-11-24T22:52:04Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'COLLABORATOR',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'sub_issues_summary': {'total': 0, 'completed': 0, 'percent_completed': 0},\n",
              "  'issue_dependencies_summary': {'blocked_by': 0,\n",
              "   'total_blocked_by': 0,\n",
              "   'blocking': 0,\n",
              "   'total_blocking': 0},\n",
              "  'body': '**Background**\\nCurrently any job that fails with terminal error will abort the package automatically. This is too opinionated and we should give users a choice what to do. The same mechanism can be also used to abort pending packages (ie. with retries) properly. Current mechanism does not ie. restore pipeline state.\\n\\nI consider this change between fixing a bug and changing behavior. We must correct how packages are aborted but we leave a flag that will do auto abort on terminal exceptions.\\n\\n**Tasks**\\n\\n1. * [ ] Instead of failing packages, job with terminal exception should be moved to new jobs with retry count increased. We should start draining the package immediately.\\n2. * [ ] We need a new exception (not `LoadClientJobFailed`) to raise after drain to indicate that there was a terminal exception in one of the jobs.\\n3. * [ ] We need to store `.exception` messages in some kind of separate folder (previously we stored them only in `failed_jobs`). And we should do it for each retry (transient and terminal). exception message should indicate the state of the job in the first line: \"retry\" or \"failed\"\\n4. * [ ] We need to implement new `abort_package` helper which will be similar to `drop` helper (see pipeline top level module).\\n\\nOn job with terminal exception (\"failed\" state):\\n1. We do not move it to \"failed_jobs\" automatically, we retry it and issue pending LoadClientJobFailed. \\n2. Pool drains, load ends with new exception and package is always pending. We do not abort automatically. (we do not complete with aborted flag, we raise)\\n3. User may chose to fail any job (except completed and failed already). Failing jobs moves those jobs to “failed_jobs” and copies exception message (see above) so we are backward compatible.\\n4. User should be able to easily list jobs that were retried and identify those that were retried due to terminal error\\n5. User may always chose to retry (even if some jobs are moved to failed)\\n6. User may chose to abort the package.\\nAbort must be a managed operation similar to `drop` helper. Action must happen via `pipeline.load` and generate run trace.\\n\\n1. only pending package may be aborted (the one being currently loaded)\\n2. you mark package to be aborted ie. via writing a flag to package state and run `load`\\n3. if flag is present, load.py must abort the package\\n\\nAborting means:\\n- all jobs that were retried and are not in terminal state are moved to “failed_jobs”\\n- package is aborted and moved to “loaded” state (this is a terminal state). Existing code should be used to that\\n- when package is aborted: pipeline must be completely wiped out and re-synced. That include any other load package\\n\\nOther tasks\\n- add new load option to auto abort on terminal error (current behavior) but do it as above. this will provide backward compatibility with current behavior\\n- we need a command line that will replace `drop-pending-packages` and that will add `fail-job` command on package command\\n- add new `abort_package` method\\n- replace `def drop_pending_packages(self, with_partial_loads: bool = True) -> None:\\n` with abort_package \\n\\nDocs:\\n- we need to update docs that explains when package is aborted\\n- we need to update entity diagram',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3375/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3375/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None},\n",
              " {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3374',\n",
              "  'repository_url': 'https://api.github.com/repos/dlt-hub/dlt',\n",
              "  'labels_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3374/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3374/comments',\n",
              "  'events_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3374/events',\n",
              "  'html_url': 'https://github.com/dlt-hub/dlt/pull/3374',\n",
              "  'id': 3660707769,\n",
              "  'node_id': 'PR_kwDOGvRYu861S57N',\n",
              "  'number': 3374,\n",
              "  'title': '[fix/3358] add pagination stopping to `JSONResponseCursorPaginator`',\n",
              "  'user': {'login': 'segetsy',\n",
              "   'id': 182826843,\n",
              "   'node_id': 'U_kgDOCuW3Ww',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/182826843?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/segetsy',\n",
              "   'html_url': 'https://github.com/segetsy',\n",
              "   'followers_url': 'https://api.github.com/users/segetsy/followers',\n",
              "   'following_url': 'https://api.github.com/users/segetsy/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/segetsy/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/segetsy/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/segetsy/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/segetsy/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/segetsy/repos',\n",
              "   'events_url': 'https://api.github.com/users/segetsy/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/segetsy/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'labels': [],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': {'login': 'burnash',\n",
              "   'id': 264674,\n",
              "   'node_id': 'MDQ6VXNlcjI2NDY3NA==',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/264674?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/burnash',\n",
              "   'html_url': 'https://github.com/burnash',\n",
              "   'followers_url': 'https://api.github.com/users/burnash/followers',\n",
              "   'following_url': 'https://api.github.com/users/burnash/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/burnash/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/burnash/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/burnash/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/burnash/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/burnash/repos',\n",
              "   'events_url': 'https://api.github.com/users/burnash/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/burnash/received_events',\n",
              "   'type': 'User',\n",
              "   'user_view_type': 'public',\n",
              "   'site_admin': False},\n",
              "  'assignees': [{'login': 'burnash',\n",
              "    'id': 264674,\n",
              "    'node_id': 'MDQ6VXNlcjI2NDY3NA==',\n",
              "    'avatar_url': 'https://avatars.githubusercontent.com/u/264674?v=4',\n",
              "    'gravatar_id': '',\n",
              "    'url': 'https://api.github.com/users/burnash',\n",
              "    'html_url': 'https://github.com/burnash',\n",
              "    'followers_url': 'https://api.github.com/users/burnash/followers',\n",
              "    'following_url': 'https://api.github.com/users/burnash/following{/other_user}',\n",
              "    'gists_url': 'https://api.github.com/users/burnash/gists{/gist_id}',\n",
              "    'starred_url': 'https://api.github.com/users/burnash/starred{/owner}{/repo}',\n",
              "    'subscriptions_url': 'https://api.github.com/users/burnash/subscriptions',\n",
              "    'organizations_url': 'https://api.github.com/users/burnash/orgs',\n",
              "    'repos_url': 'https://api.github.com/users/burnash/repos',\n",
              "    'events_url': 'https://api.github.com/users/burnash/events{/privacy}',\n",
              "    'received_events_url': 'https://api.github.com/users/burnash/received_events',\n",
              "    'type': 'User',\n",
              "    'user_view_type': 'public',\n",
              "    'site_admin': False}],\n",
              "  'milestone': None,\n",
              "  'comments': 1,\n",
              "  'created_at': '2025-11-24T22:12:33Z',\n",
              "  'updated_at': '2025-12-01T17:00:11Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'NONE',\n",
              "  'type': None,\n",
              "  'active_lock_reason': None,\n",
              "  'draft': False,\n",
              "  'pull_request': {'url': 'https://api.github.com/repos/dlt-hub/dlt/pulls/3374',\n",
              "   'html_url': 'https://github.com/dlt-hub/dlt/pull/3374',\n",
              "   'diff_url': 'https://github.com/dlt-hub/dlt/pull/3374.diff',\n",
              "   'patch_url': 'https://github.com/dlt-hub/dlt/pull/3374.patch',\n",
              "   'merged_at': None},\n",
              "  'body': '### Description\\r\\nAdd some methods of stopping pagination to `JSONResponseCursorPaginator`\\r\\n\\r\\n### Related Issues\\r\\n\\r\\n- Fixes https://github.com/dlt-hub/dlt/issues/3358\\r\\n\\r\\nSome tests fail, don\\'t seem related to the PR:\\r\\n<img width=\"1508\" height=\"172\" alt=\"image\" src=\"https://github.com/user-attachments/assets/55c1af24-ea4f-4cd0-9f10-98a0645216d3\" />\\r\\n',\n",
              "  'closed_by': None,\n",
              "  'reactions': {'url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3374/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/dlt-hub/dlt/issues/3374/timeline',\n",
              "  'performed_via_github_app': None,\n",
              "  'state_reason': None}]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from dlt.sources.helpers.rest_client import RESTClient\n",
        "from dlt.sources.helpers.rest_client.auth import BearerTokenAuth\n",
        "from dlt.sources.helpers.rest_client.paginators import JSONLinkPaginator\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "os.environ[\"ACCESS_TOKEN\"] = userdata.get(\"SECRET_KEY\")\n",
        "\n",
        "\n",
        "client = RESTClient(\n",
        "    base_url=\"https://api.github.com\",\n",
        "    headers={\"User-Agent\": \"MyApp/1.0\"},\n",
        "    auth=BearerTokenAuth(dlt.secrets[\"access_token\"]),\n",
        "    paginator=HeaderLinkPaginator(),\n",
        "    data_selector=\"data\",\n",
        "    # session=MyCustomSession()\n",
        ")\n",
        "\n",
        "client.get(\"repos/dlt-hub/dlt/issues\").json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyIw6cY23Ul5"
      },
      "source": [
        "The `RESTClient` class is initialized with the following parameters:\n",
        "\n",
        "- `base_url`: The root URL of the API. All requests will be made relative to this URL.\n",
        "- `headers`: Default headers to include in every request. This can be used to set common headers like `User-Agent` or other custom headers.\n",
        "- `auth`: The authentication configuration. See the [Authentication](https://dlthub.com/docs/general-usage/http/rest-client#authentication) section for more details.\n",
        "- `paginator`: A paginator instance for handling paginated responses. See the [Paginators](https://dlthub.com/docs/general-usage/http/rest-client#paginators) section below.\n",
        "- `data_selector`: A [JSONPath selector](https://github.com/h2non/jsonpath-ng?tab=readme-ov-file#jsonpath-syntax) for extracting data from the responses. This defines a way to extract the data from the response JSON. Only used when paginating.\n",
        "- `session`: An optional session for making requests. This should be a [Requests session](https://requests.readthedocs.io/en/latest/api/#requests.Session) instance that can be used to set up custom request behavior for the client.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQH2cueN2wHP"
      },
      "source": [
        "## **2. Add authentication**\n",
        "\n",
        "The RESTClient supports various authentication strategies, such as bearer tokens, API keys, and HTTP basic auth, configured through the `auth` parameter of both the `RESTClient` and the `paginate()` method.\n",
        "\n",
        "The **available authentication methods** are defined in the `dlt.sources.helpers.rest_client.auth` module:\n",
        "\n",
        "- [BearerTokenAuth](https://dlthub.com/docs/devel/general-usage/http/rest-client#bearer-token-authentication)\n",
        "- [APIKeyAuth](https://dlthub.com/docs/devel/general-usage/http/rest-client#api-key-authentication)\n",
        "- [HttpBasicAuth](https://dlthub.com/docs/devel/general-usage/http/rest-client#http-basic-authentication)\n",
        "- [OAuth2ClientCredentials](https://dlthub.com/docs/devel/general-usage/http/rest-client#oauth-20-authorization)\n",
        "\n",
        "For specific use cases, you can [implement custom authentication](https://dlthub.com/docs/devel/general-usage/http/rest-client#implementing-custom-authentication) by subclassing the `AuthConfigBase` class from the [`dlt.sources.helpers.rest_client.auth`](https://github.com/dlt-hub/dlt/blob/devel/dlt/sources/helpers/rest_client/auth.py) module.\n",
        "For specific flavors of OAuth 2.0, you can [implement custom OAuth 2.0](https://dlthub.com/docs/devel/general-usage/http/rest-client#oauth-20-authorization) by subclassing `OAuth2ClientCredentials`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgbJCKkDpnDi"
      },
      "source": [
        "![Lesson_1_Custom_sources_RestAPI_source_and_RESTClient_img1](https://storage.googleapis.com/dlt-blog-images/dlt-advanced-course/Lesson_1_Custom_sources_RestAPI_source_and_RESTClient_img1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr3cObaU6uxC"
      },
      "source": [
        "\n",
        "### 📰 **NewsAPI overview**\n",
        "\n",
        "- **Base URL:** `https://newsapi.org/v2/`\n",
        "- **Authentication:** API key passed in query string as `apiKey`\n",
        "- **Documentation:** [NewsAPI Docs](https://newsapi.org/docs)\n",
        "\n",
        "| Endpoint          | Description                              | Auth Required | Response     |\n",
        "|-------------------|------------------------------------------|---------------|--------------|\n",
        "| `/everything`     | Search for news articles by query string | ✅ Yes        | JSON object with `articles[]` |\n",
        "| `/top-headlines`  | Latest headlines filtered by region/topic| ✅ Yes        | JSON object with `articles[]` |\n",
        "| `/sources`        | List of available news sources           | ✅ Yes        | JSON object with `sources[]`  |\n",
        "\n",
        "\n",
        "#### **Authentication Details:**\n",
        "\n",
        "To use NewsAPI, you must register for a **free account** and obtain an API key. This key is required for all endpoints and must be included as a query parameter in your request:\n",
        "\n",
        "```http\n",
        "GET /v2/everything?q=python&page=1&apiKey=YOUR_API_KEY\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DIaaG2qNJpM"
      },
      "source": [
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "To securely access the NewsAPI in your dlt project:\n",
        "\n",
        "1. **Sign up** at [https://newsapi.org/register](https://newsapi.org/register)\n",
        "2. Copy your **API key** from your dashboard\n",
        "3. Save your **API key** in Colab Secrets (side-bar on the right) as NEWS_API_KEY\n",
        "\n",
        "\n",
        "### **How we chose the right authenticator for NewsAPI**\n",
        "\n",
        "NewsAPI uses a **simple API key-based scheme**. You sign up, get a key, and send it with every request.\n",
        "\n",
        "There are two supported ways to send this key:\n",
        "\n",
        "- In a **query string**, like `?apiKey=...`\n",
        "- Or in the **Authorization header**, as a Bearer token\n",
        "\n",
        "We are using the **query string method**, because:\n",
        "\n",
        "- It's supported on **all plans**, including the free tier\n",
        "- It's more transparent — you can inspect the request URL and see the key\n",
        "- It's easier to test manually in a browser or terminal\n",
        "\n",
        "\n",
        "**Using `APIKeyAuth` simplifies request setup**\n",
        "\n",
        "Instead of manually appending the key to every URL, we use dlt’s built-in `APIKeyAuth`:\n",
        "\n",
        "```python\n",
        "APIKeyAuth(name=\"apiKey\", api_key=api_key, location=\"query\")\n",
        "```\n",
        "\n",
        "This means:\n",
        "\n",
        "- `name=\"apiKey\"` tells it what the key is called (NewsAPI expects `apiKey`)\n",
        "- `location=\"query\"` means the key will be added to the URL as a query parameter:\n",
        "  \n",
        "  ```\n",
        "  https://newsapi.org/v2/everything?q=python&apiKey=your_key\n",
        "  ```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CI3txWPO5s3q"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install dlt[duckdb]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZTlkE_WG3PW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dlt.sources.helpers.rest_client import RESTClient\n",
        "from dlt.sources.helpers.rest_client.auth import APIKeyAuth\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get(\"NEWS_API_KEY\")\n",
        "\n",
        "client = RESTClient(\n",
        "    base_url=\"https://newsapi.org/v2/\",\n",
        "    auth=APIKeyAuth(name=\"apiKey\", api_key=api_key, location=\"query\"),\n",
        ")\n",
        "\n",
        "response = client.get(\"everything\", params={\"q\": \"python\", \"page\": 1})\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pua_cIbVQ5kh"
      },
      "source": [
        "This authenticates every request by adding `?apiKey=your_key` to the URL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0wx5juj3Ul5"
      },
      "source": [
        "## **3. Add pagination**\n",
        "\n",
        "The `RESTClient` supports automatic pagination of API responses via the `paginate()` method, which can be customized using a built-in or custom paginator.\n",
        "\n",
        "You specify the paginator using the `paginator` parameter of the `RESTClient` or directly in the `paginate()` method.\n",
        "\n",
        "The **available pagination strategies** are defined in the `dlt.sources.helpers.rest_client.paginators` module and cover the most common pagination patterns used in REST APIs:\n",
        "\n",
        "- [`PageNumberPaginator`](https://dlthub.com/docs/general-usage/http/rest-client#pagenumberpaginator) – uses `page=N`, optionally with `pageSize` or `limit`\n",
        "- [`OffsetPaginator`](https://dlthub.com/docs/general-usage/http/rest-client#offsetpaginator) – uses `offset` and `limit`\n",
        "- [`JSONLinkPaginator`](https://dlthub.com/docs/general-usage/http/rest-client#jsonresponsepaginator) – follows a `next` URL in the response body\n",
        "- [`HeaderLinkPaginator`](https://dlthub.com/docs/general-usage/http/rest-client#headerlinkpaginator) – follows a `Link` header (used by GitHub and others)\n",
        "- [`JSONResponseCursorPaginator`](https://dlthub.com/docs/general-usage/http/rest-client#jsonresponsecursorpaginator) – uses a cursor from the response body\n",
        "\n",
        "Each paginator knows how to update the request to get the next page of results, and will continue until:\n",
        "\n",
        "- no more pages are available,\n",
        "- a configurable `maximum_page` or `maximum_offset` is reached,\n",
        "- or the API response is empty (depending on paginator behavior).\n",
        "\n",
        "\n",
        "> If a `paginator` is not specified, the `paginate()` method will attempt to **automatically detect** the pagination mechanism used by the API. If the API uses a standard pagination mechanism like having a `next` link in the response's headers or JSON body, the `paginate()` method will handle this automatically. Otherwise, you can specify a paginator object explicitly or implement a custom paginator.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8_9PsbSyrpp"
      },
      "source": [
        "### **PageData**\n",
        "\n",
        "When using `client.paginate(...)` in dlt, you don’t just get a stream of data — each **page** returned is a rich object called `PageData`, and it gives you full access to the internals of the request, response, and pagination state.\n",
        "\n",
        "This is especially useful for **debugging**, **tracing**, or building custom logic.\n",
        "\n",
        "\n",
        "The `PageData` is a list-like object that contains the following attributes:\n",
        "\n",
        "- `request`: The original request object.\n",
        "- `response`: The response object.\n",
        "- `paginator`: The paginator object used to paginate the response.\n",
        "- `auth`: The authentication object used for the request.\n",
        "\n",
        "Let’s walk through an example."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "page_iterator =  client.paginate(\"everything\", params={\"q\": \"python\", \"page\": 1})\n",
        "\n",
        "print(\"The original request object: {} \".format(next(page_iterator).request))\n",
        "\n",
        "page_iterator =  client.paginate(\"everything\", params={\"q\": \"python\", \"page\": 1})\n",
        "\n",
        "print(\"The raw HTTP response: {} \".format(next(page_iterator).response))\n",
        "\n",
        "page_iterator =  client.paginate(\"everything\", params={\"q\": \"python\", \"page\": 1})\n",
        "\n",
        "print(\"The used paginator: {} \".format(next(page_iterator).paginator))\n",
        "\n",
        "page_iterator =  client.paginate(\"everything\", params={\"q\": \"python\", \"page\": 1})\n",
        "\n",
        "print(\"The used authentication class : {} \".format(next(page_iterator).auth))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjcDoHrQmpI9",
        "outputId": "c3e9094d-ba70-4dbd-cfbf-4822b766a049"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The original request object: <Request [GET]> \n",
            "The raw HTTP response: <Response [200]> \n",
            "The used paginator: PageNumberPaginator at 794d65b67b60: current page: 2 page_param: page total_path: None maximum_value: 2 has_more_path: None \n",
            "The used authentication class : <dlt.sources.helpers.rest_client.auth.APIKeyAuth object at 0x794d65da2a20> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "_1s_uYn1TQB7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "3b8ded3a-1821-461a-e171-269dd1bc6b7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Request [GET]>\n",
            "<Response [200]>\n",
            "PageNumberPaginator at 794d65d096d0: current page: 2 page_param: page total_path: None maximum_value: 2 has_more_path: None\n",
            "<dlt.sources.helpers.rest_client.auth.APIKeyAuth object at 0x794d65da2a20>\n"
          ]
        }
      ],
      "source": [
        "page_iterator = client.paginate(\"everything\", params={\"q\": \"python\", \"page\": 1})\n",
        "# prints the original request object\n",
        "print(next(page_iterator).request)\n",
        "page_iterator = client.paginate(\"everything\", params={\"q\": \"python\", \"page\": 1})\n",
        "# prints the raw HTTP response\n",
        "print(next(page_iterator).response)\n",
        "page_iterator = client.paginate(\"everything\", params={\"q\": \"python\", \"page\": 1})\n",
        "# prints the paginator that was used\n",
        "print(next(page_iterator).paginator)\n",
        "page_iterator = client.paginate(\"everything\", params={\"q\": \"python\", \"page\": 1})\n",
        "# prints the authentication class used\n",
        "print(next(page_iterator).auth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQMrcYXTRAdP"
      },
      "source": [
        "**Log Warning explained**\n",
        "\n",
        "```\n",
        "[WARNING] Fallback paginator used: SinglePagePaginator...\n",
        "```\n",
        "\n",
        "This warning means:\n",
        "\n",
        "- dlt tried to guess the pagination method but failed\n",
        "- It will make only **one request**\n",
        "- You won’t get multiple pages of data unless you configure a paginator explicitly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P2y_ya_LD5X"
      },
      "source": [
        "### **Question 1:**\n",
        "\n",
        "\n",
        "Which paginator is used by `client.paginate()` by default in the example above?\n",
        "\n",
        "\n",
        ">Answer this question and select the correct option in the homework Google Form.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8uKuQ6ISs3u"
      },
      "source": [
        "### **How we chose the right paginator for NewsAPI**\n",
        "\n",
        "When using `RESTClient` to extract data from paginated APIs, one of the first decisions you must make is:  \n",
        "**\"What type of pagination does this API use?\"**  \n",
        "This determines which paginator to plug into the client.\n",
        "\n",
        "---\n",
        "\n",
        "**Step 1: Read the API docs**\n",
        "\n",
        "From the [NewsAPI documentation](https://newsapi.org/docs/endpoints/everything), we learn:\n",
        "\n",
        "- Pagination is done via two query parameters:\n",
        "  - `page` → page number (starts at 1)\n",
        "  - `pageSize` → how many articles per page (max 100)\n",
        "- Example request:\n",
        "  ```\n",
        "  GET /v2/everything?q=bitcoin&page=2&pageSize=20&apiKey=...\n",
        "  ```\n",
        "\n",
        "There is **no \"next\" URL**, no cursor, no `offset`.\n",
        "\n",
        "This is **classic page-number pagination.**\n",
        "\n",
        "---\n",
        "\n",
        "**Step 2: Understand response behavior**\n",
        "\n",
        "Each response includes:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"status\": \"ok\",\n",
        "  \"totalResults\": 1532,\n",
        "  \"articles\": [ ... ]\n",
        "}\n",
        "```\n",
        "\n",
        "But:\n",
        "- The API **does not tell us how many total pages exist**.\n",
        "- We only know how many total results there are.\n",
        "\n",
        "So we either:\n",
        "- Compute total pages: `ceil(totalResults / pageSize)`  \n",
        "  *(But that requires looking into the first page’s body)*  \n",
        "- **Or we keep requesting pages until we get an empty list.**\n",
        "\n",
        "---\n",
        "\n",
        "**Step 3: Choose `PageNumberPaginator`**\n",
        "\n",
        "This is exactly what `PageNumberPaginator` is made for:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "EoIZvs1sKKJM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "c6358867-78a3-4270-9490-bb1ceb9dd42b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lenovo Stockpiling PC Memory Due To 'Unprecedented' AI Squeeze\n",
            "Photos show monster Burmese pythons caught in Florida Python Challenge\n",
            "Simple Tricks To Make Your Python Code Faster\n",
            "Florida man captures giant python, told to ‘put it in trash’\n",
            "KiDoom Brings Classic Shooter to KiCad\n",
            "Remember XBMC? It’s Back!\n",
            "Show HN: Optimizing LiteLLM with Rust – When Expectations Meet Reality\n",
            "The Qtile Window Manager: A Python-Powered Tiling Experience\n",
            "Better pre-commit, re-engineered in Rust\n",
            "Comparing AWS Lambda ARM64 vs. x86_64 Performance Across Runtimes in Late 2025\n",
            "Await Is Not a Context Switch: Understanding Python's Coroutines vs. Tasks\n",
            "Real-Time AI-Powered Texas Hold'em in Python and Flask – Play in the Browser\n",
            "Another Thermal Printer, Conquered\n",
            "Python Software Foundation gets a donor surge after rejecting federal grant\n",
            "A Bird Watching Assistant\n"
          ]
        }
      ],
      "source": [
        "from dlt.sources.helpers.rest_client.paginators import PageNumberPaginator\n",
        "from dlt.sources.helpers.rest_client import RESTClient\n",
        "from dlt.sources.helpers.rest_client.auth import APIKeyAuth\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get(\"NEWS_API_KEY\")\n",
        "\n",
        "\n",
        "client = RESTClient(\n",
        "    base_url=\"https://newsapi.org/v2/\",\n",
        "    auth=APIKeyAuth(name=\"apiKey\", api_key=api_key, location=\"query\"),\n",
        "    paginator=PageNumberPaginator(\n",
        "        base_page=1,  # NewsAPI starts paging from 1\n",
        "        page_param=\"page\",  # Matches the API spec\n",
        "        total_path=None,  # Set it to None explicitly\n",
        "        stop_after_empty_page=True,  # Stop if no articles returned\n",
        "        maximum_page=4,  # Optional limit for dev/testing\n",
        "    ),\n",
        ")\n",
        "\n",
        "for page in client.paginate(\n",
        "    \"everything\", params={\"q\": \"python\", \"pageSize\": 5, \"language\": \"en\"}\n",
        "):\n",
        "    for article in page:\n",
        "        print(article[\"title\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dlt\n",
        "import os\n",
        "import json\n",
        "from dlt.sources.helpers.rest_client import RESTClient\n",
        "from dlt.sources.helpers.rest_client.auth import APIKeyAuth\n",
        "from dlt.sources.helpers.rest_client.paginators import PageNumberPaginator\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "api_key = userdata.get(\"NEWS_API_KEY\")\n",
        "\n",
        "client = RESTClient(\n",
        "    base_url = \"https://newsapi.org/v2/\",\n",
        "    auth = APIKeyAuth(name = \"apiKey\", api_key = api_key, location = \"query\"),\n",
        "    paginator = PageNumberPaginator(\n",
        "        maximum_page = 2,  # Limits response to 2 pages\n",
        "        total_path = None,\n",
        "        base_page = 1,\n",
        "        page_param = \"page\",\n",
        "        stop_after_empty_page = True,\n",
        "\n",
        "    )\n",
        "\n",
        ")\n",
        "\n",
        "#response = client.get(\"everything\", params = {\"q\": \"Jesus\", \"page\": 3})\n",
        "#print(response.json())\n",
        "#res = json.dumps(response.json(), indent = 4)\n",
        "params = {\"q\": \"Jesus\", \"pageSize\": 3}\n",
        "for page in client.paginate(\"everything\", params = params):\n",
        "  res = json.dumps(page, indent = 4)\n",
        "  for article in page:\n",
        "    print(article[\"content\"])\n",
        "  #print(res)\n",
        "#print(response.json())\n",
        "#res = json.dumps(response.json(), indent = 4)\n",
        "#print(res)\n",
        "#print(\"Status code : {} \\n text : {}\".format(response.status_code, response.text))"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuD5L9SCKwSo",
        "outputId": "7021b65c-f77b-49c8-cb45-ac6d6e82f8f8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The victims sister called it one big Greek tragedyand she meant it.\r\n",
            "In September 2016, Nathan Carman took off in a fishing boat with his mother near Block Island. A few days later, a freighter spott… [+1326 chars]\n",
            "We cannot say for sure if Elon Musk dialed up the flattery quotient on his chatbot, Grok, after the author Joyce Carol Oates publicly humiliated him this month. What we can say is that, yesterday, Gr… [+6458 chars]\n",
            "SALT LAKE CITY (AP) Gérald Caussé, a high-ranking official in The Church of Jesus Christ of Latter-day Saints who oversaw a global temple building boom under its previous president, became the faith'… [+2131 chars]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8X8F2rZRDSE"
      },
      "source": [
        "## **4. Wrap into a dlt Resource**\n",
        "\n",
        "Let’s turn this into a dlt pipeline resource:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dlt\n",
        "import os\n",
        "import json\n",
        "from dlt.sources.helpers.rest_client import RESTClient\n",
        "from dlt.sources.helpers.rest_client.auth import APIKeyAuth\n",
        "from dlt.sources.helpers.rest_client.paginators import PageNumberPaginator\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "os.environ[\"API_KEY\"] = userdata.get(\"NEWS_API_KEY\")\n",
        "\n",
        "\n",
        "@dlt.resource(write_disposition = \"replace\", name = \"python_articles\")\n",
        "def get_articles(api_key: str = dlt.secrets.value) -> Iterator[TDataItems]:\n",
        "  client = RESTClient(\n",
        "      base_url = \"https://newsapi.org/v2/\",\n",
        "      auth = APIKeyAuth(name = \"apiKey\", api_key = api_key, location = \"query\"),\n",
        "      paginator = PageNumberPaginator(\n",
        "          base_page = 1,\n",
        "          page_param = \"page\",\n",
        "          total_path = None,\n",
        "          stop_after_empty_page = True,\n",
        "          maximum_page = 3,  # Limits response to 2 pages\n",
        "\n",
        "      )\n",
        "\n",
        "  )\n",
        "  params = {\"q\": \"Jesus\", \"pageSize\": 3, \"language\": \"en\"}\n",
        "  for page in client.paginate(\"everything\", params = params):\n",
        "    yield page\n",
        "\n",
        "@dlt.resource(write_disposition = \"replace\", name = \"top_article\")\n",
        "def get_top_articles(api_key: str = dlt.secrets.value) -> Iterator[TDataItems]:\n",
        "\n",
        "  client = RESTClient(\n",
        "      base_url = \"https://newsapi.org/v2/\",\n",
        "      auth = APIKeyAuth(name = \"apiKey\", api_key = api_key, location = \"query\"),\n",
        "      paginator = PageNumberPaginator(\n",
        "          base_page = 1,\n",
        "          page_param = \"page\",\n",
        "          total_path = None,\n",
        "          stop_after_empty_page = True,\n",
        "          maximum_page = 3,  # Limits response to 2 pages\n",
        "\n",
        "      )\n",
        "\n",
        "  )\n",
        "  params = {\"q\": \"Jesus\", \"pageSize\": 3, \"language\": \"en\"}\n",
        "  for page in client.paginate(\"top-headlines\", params = params):\n",
        "    yield page\n",
        "\n",
        "@dlt.source\n",
        "def news_api_source(api_key: str = dlt.secrets.value) -> Iterable[DltResource]:\n",
        "  return [get_articles(api_key = api_key), get_top_articles(api_key = api_key)]\n",
        "\n",
        "pipeline = dlt.pipeline(\n",
        "      pipeline_name = \"newsapi_pipeline\",\n",
        "      destination = \"duckdb\",\n",
        "      dataset_name = \"news_data\"\n",
        ")\n",
        "load_info = pipeline.run(news_api_source())\n",
        "print(load_info)\n",
        "df_py_art = pipeline.dataset().python_articles.df()\n",
        "df_top_art = pipeline.dataset().python_articles.df()\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4068206-8d38-498a-e870-d12840ed4ebc",
        "id": "KhVZNlV70lKl"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline newsapi_pipeline load step completed in 0.22 seconds\n",
            "1 load package(s) were loaded to destination duckdb and into dataset news_data\n",
            "The duckdb destination used duckdb:////content/newsapi_pipeline.duckdb location to store data\n",
            "Load package 1764862346.431807 is LOADED and contains no failed jobs\n",
            "  source__id         source__name                       author  \\\n",
            "0      wired                Wired           Angela Watercutter   \n",
            "1       None         The Atlantic  Charlie Warzel, Matteo Wong   \n",
            "2       None  Yahoo Entertainment            HANNAH SCHOENBAUM   \n",
            "3       None                ABC 4                  Ryan Bittan   \n",
            "4       None  Yahoo Entertainment               BOBBY ROSS JR.   \n",
            "\n",
            "                                               title  \\\n",
            "0  Premiere: Netflix’s New True Crime Doc Dives I...   \n",
            "1                      Elon Musk: Better Than Jesus?   \n",
            "2  New Mormon apostle led a global temple buildin...   \n",
            "3  LDS Church donating 250 semitrucks worth of fo...   \n",
            "4  Former Mets star Darryl Strawberry thanks Trum...   \n",
            "\n",
            "                                         description  \\\n",
            "0  “The Carman Family Deaths,” based on a 2021 WI...   \n",
            "1  Grok is trying to rewrite history on behalf of...   \n",
            "2  Gérald Caussé, a high-ranking official in The ...   \n",
            "3  The Church of Jesus Christ of Latter-day Saint...   \n",
            "4  Former New York Mets great Darryl Strawberry p...   \n",
            "\n",
            "                                                 url  \\\n",
            "0  https://www.wired.com/story/the-carman-family-...   \n",
            "1  https://www.theatlantic.com/technology/2025/11...   \n",
            "2  https://www.yahoo.com/news/articles/mormon-apo...   \n",
            "3  https://www.abc4.com/news/national/church-dona...   \n",
            "4  https://www.yahoo.com/news/articles/former-met...   \n",
            "\n",
            "                                        url_to_image  \\\n",
            "0  https://media.wired.com/photos/6908feb2841e97d...   \n",
            "1  https://cdn.theatlantic.com/thumbor/0rKqK_1Wjb...   \n",
            "2  https://s.yimg.com/ny/api/res/1.2/A_4oaNa0U90O...   \n",
            "3  https://media.zenfs.com/en/ktvx_articles_781/2...   \n",
            "4  https://s.yimg.com/ny/api/res/1.2/bI0QhBCHp3Xc...   \n",
            "\n",
            "               published_at  \\\n",
            "0 2025-11-04 14:00:00+00:00   \n",
            "1 2025-11-21 18:13:24+00:00   \n",
            "2 2025-11-07 01:36:25+00:00   \n",
            "3 2025-11-13 20:03:38+00:00   \n",
            "4 2025-11-23 21:24:57+00:00   \n",
            "\n",
            "                                             content       _dlt_load_id  \\\n",
            "0  The victims sister called it one big Greek tra...  1764862346.431807   \n",
            "1  We cannot say for sure if Elon Musk dialed up ...  1764862346.431807   \n",
            "2  SALT LAKE CITY (AP) Gérald Caussé, a high-rank...  1764862346.431807   \n",
            "3  SALT LAKE CITY (ABC4) The Church of Jesus Chri...  1764862346.431807   \n",
            "4  TULSA, Okla. (AP) Former New York Mets great D...  1764862346.431807   \n",
            "\n",
            "          _dlt_id  \n",
            "0  9+f0SO67hIwOgA  \n",
            "1  P6WhQBjJNDhOQw  \n",
            "2  LRebQJcfGTx4Iw  \n",
            "3  NLsGUU4CyqCEAg  \n",
            "4  OO+WCtZtctPBtQ  \n",
            "  source__id         source__name                       author  \\\n",
            "0      wired                Wired           Angela Watercutter   \n",
            "1       None         The Atlantic  Charlie Warzel, Matteo Wong   \n",
            "2       None  Yahoo Entertainment            HANNAH SCHOENBAUM   \n",
            "3       None                ABC 4                  Ryan Bittan   \n",
            "4       None  Yahoo Entertainment               BOBBY ROSS JR.   \n",
            "\n",
            "                                               title  \\\n",
            "0  Premiere: Netflix’s New True Crime Doc Dives I...   \n",
            "1                      Elon Musk: Better Than Jesus?   \n",
            "2  New Mormon apostle led a global temple buildin...   \n",
            "3  LDS Church donating 250 semitrucks worth of fo...   \n",
            "4  Former Mets star Darryl Strawberry thanks Trum...   \n",
            "\n",
            "                                         description  \\\n",
            "0  “The Carman Family Deaths,” based on a 2021 WI...   \n",
            "1  Grok is trying to rewrite history on behalf of...   \n",
            "2  Gérald Caussé, a high-ranking official in The ...   \n",
            "3  The Church of Jesus Christ of Latter-day Saint...   \n",
            "4  Former New York Mets great Darryl Strawberry p...   \n",
            "\n",
            "                                                 url  \\\n",
            "0  https://www.wired.com/story/the-carman-family-...   \n",
            "1  https://www.theatlantic.com/technology/2025/11...   \n",
            "2  https://www.yahoo.com/news/articles/mormon-apo...   \n",
            "3  https://www.abc4.com/news/national/church-dona...   \n",
            "4  https://www.yahoo.com/news/articles/former-met...   \n",
            "\n",
            "                                        url_to_image  \\\n",
            "0  https://media.wired.com/photos/6908feb2841e97d...   \n",
            "1  https://cdn.theatlantic.com/thumbor/0rKqK_1Wjb...   \n",
            "2  https://s.yimg.com/ny/api/res/1.2/A_4oaNa0U90O...   \n",
            "3  https://media.zenfs.com/en/ktvx_articles_781/2...   \n",
            "4  https://s.yimg.com/ny/api/res/1.2/bI0QhBCHp3Xc...   \n",
            "\n",
            "               published_at  \\\n",
            "0 2025-11-04 14:00:00+00:00   \n",
            "1 2025-11-21 18:13:24+00:00   \n",
            "2 2025-11-07 01:36:25+00:00   \n",
            "3 2025-11-13 20:03:38+00:00   \n",
            "4 2025-11-23 21:24:57+00:00   \n",
            "\n",
            "                                             content       _dlt_load_id  \\\n",
            "0  The victims sister called it one big Greek tra...  1764862346.431807   \n",
            "1  We cannot say for sure if Elon Musk dialed up ...  1764862346.431807   \n",
            "2  SALT LAKE CITY (AP) Gérald Caussé, a high-rank...  1764862346.431807   \n",
            "3  SALT LAKE CITY (ABC4) The Church of Jesus Chri...  1764862346.431807   \n",
            "4  TULSA, Okla. (AP) Former New York Mets great D...  1764862346.431807   \n",
            "\n",
            "          _dlt_id  \n",
            "0  9+f0SO67hIwOgA  \n",
            "1  P6WhQBjJNDhOQw  \n",
            "2  LRebQJcfGTx4Iw  \n",
            "3  NLsGUU4CyqCEAg  \n",
            "4  OO+WCtZtctPBtQ  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_py_art.head().content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJqRhz0u_rVH",
        "outputId": "635cfee0-835b-4640-dd04-124124dbcd1f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    The victims sister called it one big Greek tra...\n",
            "1    We cannot say for sure if Elon Musk dialed up ...\n",
            "2    SALT LAKE CITY (AP) Gérald Caussé, a high-rank...\n",
            "3    SALT LAKE CITY (ABC4) The Church of Jesus Chri...\n",
            "4    TULSA, Okla. (AP) Former New York Mets great D...\n",
            "Name: content, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_top_art.head().content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEaMBFeA_sUL",
        "outputId": "b7a44235-b304-46ee-f513-d68c7778cffb"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    The victims sister called it one big Greek tra...\n",
            "1    We cannot say for sure if Elon Musk dialed up ...\n",
            "2    SALT LAKE CITY (AP) Gérald Caussé, a high-rank...\n",
            "3    SALT LAKE CITY (ABC4) The Church of Jesus Chri...\n",
            "4    TULSA, Okla. (AP) Former New York Mets great D...\n",
            "Name: content, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "9xAiyW40RCqB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import dlt\n",
        "from dlt.sources.helpers.rest_client import RESTClient\n",
        "from dlt.sources.helpers.rest_client.auth import APIKeyAuth\n",
        "\n",
        "os.environ[\"API_KEY\"] = userdata.get(\"NEWS_API_KEY\")\n",
        "\n",
        "\n",
        "@dlt.resource(write_disposition=\"replace\", name=\"python_articles\")\n",
        "def get_articles(api_key: str = dlt.secrets.value) -> Iterator[TDataItems]:\n",
        "    client = RESTClient(\n",
        "        base_url=\"https://newsapi.org/v2/\",\n",
        "        auth=APIKeyAuth(name=\"apiKey\", api_key=api_key, location=\"query\"),\n",
        "        paginator=PageNumberPaginator(\n",
        "            base_page=1,\n",
        "            page_param=\"page\",\n",
        "            total_path=None,\n",
        "            stop_after_empty_page=True,\n",
        "            maximum_page=4,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    for page in client.paginate(\n",
        "        \"everything\", params={\"q\": \"python\", \"pageSize\": 5, \"language\": \"en\"}\n",
        "    ):\n",
        "        yield page\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t53csiFLRX82"
      },
      "source": [
        "## **5. Add `top-headlines` Resource**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "wjelK9BpRc1h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import dlt\n",
        "from dlt.sources.helpers.rest_client import RESTClient\n",
        "from dlt.sources.helpers.rest_client.auth import APIKeyAuth\n",
        "\n",
        "os.environ[\"API_KEY\"] = userdata.get(\"NEWS_API_KEY\")\n",
        "\n",
        "\n",
        "@dlt.resource(write_disposition=\"replace\", name=\"top_articles\")\n",
        "def get_top_articles(api_key: str = dlt.secrets.value) -> Iterator[TDataItems]:\n",
        "    client = RESTClient(\n",
        "        base_url=\"https://newsapi.org/v2/\",\n",
        "        auth=APIKeyAuth(name=\"apiKey\", api_key=api_key, location=\"query\"),\n",
        "        paginator=PageNumberPaginator(\n",
        "            base_page=1,\n",
        "            page_param=\"page\",\n",
        "            total_path=None,\n",
        "            stop_after_empty_page=True,\n",
        "            maximum_page=4,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    for page in client.paginate(\n",
        "        \"top-headlines\", params={\"pageSize\": 5, \"language\": \"en\"}\n",
        "    ):\n",
        "        yield page\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW5YHeeNRrkJ"
      },
      "source": [
        "\n",
        "## **6. Create a reusable Source**\n",
        "\n",
        "Now bundle both resources into a single `@dlt.source`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "UUxKPDDCRwj4"
      },
      "outputs": [],
      "source": [
        "@dlt.source\n",
        "def newsapi_source(api_key: str = dlt.secrets.value) -> Iterable[DltResource]:\n",
        "    return [get_articles(api_key=api_key), get_top_articles(api_key=api_key)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhfCFKuOR0LS"
      },
      "source": [
        "## **7. Run the pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "o97AzsDZR5oR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a27a400-6bf6-4a2a-c8b4-42c3a417105d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline newsapi_pipeline load step completed in 0.19 seconds\n",
            "1 load package(s) were loaded to destination duckdb and into dataset news_data\n",
            "The duckdb destination used duckdb:////content/newsapi_pipeline.duckdb location to store data\n",
            "Load package 1764862695.6128592 is LOADED and contains no failed jobs\n"
          ]
        }
      ],
      "source": [
        "import dlt\n",
        "\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"newsapi_pipeline\", destination=\"duckdb\", dataset_name=\"news_data\"\n",
        ")\n",
        "\n",
        "info = pipeline.run(newsapi_source())\n",
        "print(info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjsW51LRSEsO"
      },
      "source": [
        "## **8. Explore data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "hbesRXsDSENw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "45771421-8bae-4321-daa8-2d8480cba29b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Five is a sufficiently close approximation to ...\n",
              "1    The Florida Python Challenge is an annual 10-d...\n",
              "2    Python has become one of the most popular prog...\n",
              "3    A Florida resident, with the help of friends, ...\n",
              "4    As the saying goes: if it has a processor and ...\n",
              "Name: content, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Five is a sufficiently close approximation to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The Florida Python Challenge is an annual 10-d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Python has become one of the most popular prog...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A Florida resident, with the help of friends, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>As the saying goes: if it has a processor and ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "pipeline.dataset().python_articles.df().head().content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "1eH1xg-cXWYH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "outputId": "0730d6dc-4810-4bb4-fd42-eeafb134eae2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "('top_articles', \"Table `top_articles` not found. Available table(s): ['python_articles', '_dlt_version', '_dlt_loads', '_dlt_pipeline_state']\")",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/dlt/dataset/dataset.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0;31m# TODO: expect TableNotFound in the future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/dlt/dataset/dataset.py\u001b[0m in \u001b[0;36mtable\u001b[0;34m(self, table_name, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# TODO: raise TableNotFound\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Table `{table_name}` not found. Available table(s): {self.tables}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Table `top_articles` not found. Available table(s): ['python_articles', '_dlt_version', '_dlt_loads', '_dlt_pipeline_state']",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2586428566.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_articles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/dlt/dataset/dataset.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;31m# TODO: expect TableNotFound in the future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSelf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: ('top_articles', \"Table `top_articles` not found. Available table(s): ['python_articles', '_dlt_version', '_dlt_loads', '_dlt_pipeline_state']\")"
          ]
        }
      ],
      "source": [
        "pipeline.dataset().top_articles.df().head().content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Euqj4MyMrNW"
      },
      "source": [
        "\n",
        "# **Create custom source using `dlt` and [`rest_api` source](https://dlthub.com/docs/dlt-ecosystem/verified-sources/rest_api/basic)**\n",
        "\n",
        "`rest_api` is a generic source that you can use to create a `dlt` source from a REST API using a declarative configuration. The majority of REST APIs behave in a similar way; this `dlt` source attempts to provide a declarative way to define a `dlt` source for those APIs.\n",
        "\n",
        "Using a [declarative configuration](https://dlthub.com/docs/dlt-ecosystem/verified-sources/rest_api/basic#source-configuration), you can define:\n",
        "\n",
        "- the API endpoints to pull data from,\n",
        "- their relationships,\n",
        "- how to handle pagination,\n",
        "- authentication,\n",
        "- data transformation,\n",
        "- incremental loading.\n",
        "\n",
        "dlt will take care of the rest: **unnesting the data, inferring the schema**, etc., and **writing to the destination**\n",
        "\n",
        "In previous section you've already met Rest API Client. `dlt`’s **[RESTClient](https://dlthub.com/docs/general-usage/http/rest-client)** is the **low level abstraction** that powers the REST API Source.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8Ok-tRgaU84"
      },
      "source": [
        "## **What you’ll learn**\n",
        "\n",
        "This section will teach you how to create a reusable, authenticated, and paginated pipeline using the `rest_api_source` module in dlt. Our example will use the [NewsAPI](https://newsapi.org), which provides access to thousands of news articles via a REST API.\n",
        "\n",
        "We'll walk step-by-step through:\n",
        "- Setting up the source configuration\n",
        "- Authenticating with an API key\n",
        "- Configuring pagination\n",
        "- Building a working `dlt` pipeline\n",
        "- Inspecting and transforming the response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md50baSW_fyW"
      },
      "source": [
        "## Reminder: **About NewsAPI**\n",
        "\n",
        "- **Base URL:** `https://newsapi.org/v2/`\n",
        "- **Authentication:** API key passed in query string as `apiKey`\n",
        "- **Documentation:** [NewsAPI Docs](https://newsapi.org/docs)\n",
        "\n",
        "| Endpoint          | Description                              | Auth Required | Response     |\n",
        "|-------------------|------------------------------------------|---------------|--------------|\n",
        "| `/everything`     | Search for news articles by keyword      | ✅ Yes        | JSON with `articles[]` |\n",
        "| `/top-headlines`  | Latest headlines filtered by region/topic| ✅ Yes        | JSON with `articles[]` |\n",
        "| `/sources`        | List of available news sources           | ✅ Yes        | JSON with `sources[]`  |\n",
        "\n",
        "To access the API, register for a **free account** at [newsapi.org](https://newsapi.org/register) and copy your personal API key.\n",
        "\n",
        "Add this key to your Colab secrets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI5WjPgncFcz"
      },
      "source": [
        "## **1. Define the source configuration**\n",
        "\n",
        "We'll now build the complete configuration step-by-step. This gives you control over authentication, pagination, filters, and even incremental loading.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCa4tzTxddYp"
      },
      "source": [
        "### **RESTAPIConfig**\n",
        "\n",
        "The central object when working with `rest_api_source` is the `RESTAPIConfig`. This is a declarative Python dictionary that tells dlt everything it needs to know about the API you are connecting to.\n",
        "\n",
        "It defines:\n",
        "- how to connect to the API (base URL, authentication)\n",
        "- what endpoints to call (resources)\n",
        "- how to paginate\n",
        "- how to filter or sort the data\n",
        "- how to extract the actual data from responses\n",
        "\n",
        "```python\n",
        "import dlt\n",
        "from dlt.sources.rest_api import rest_api_source\n",
        "\n",
        "# Define config\n",
        "news_config = {\n",
        "    \"client\": {\n",
        "        \"base_url\": ...,\n",
        "        \"auth\": ...\n",
        "    },\n",
        "    \"resources\": [\n",
        "            ...\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create source\n",
        "news_source = rest_api_source(news_config)\n",
        "\n",
        "# Create pipeline\n",
        "pipeline = dlt.pipeline(\n",
        "  pipeline_name=\"news_pipeline\",\n",
        "  destination=\"duckdb\",\n",
        "  dataset_name=\"news\"\n",
        ")\n",
        "\n",
        "# Run it\n",
        "load_info = pipeline.run(news_source)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SC_Ul1Xben5w"
      },
      "source": [
        "You can start with just these fields and then add pagination, schema hints, transformations, and more as needed.\n",
        "\n",
        "To extract data from a REST API using `dlt`, we define a configuration dictionary that follows the `RESTAPIConfig` structure.\n",
        "This configuration describes:\n",
        "\n",
        "- how to connect to the API (base URL, headers, auth)\n",
        "- what resources to extract (endpoints)\n",
        "- how to paginate, filter, and process responses\n",
        "\n",
        "At a high level, the configuration has two required keys:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ge_Cn4Qaepiw"
      },
      "source": [
        "\n",
        "### **`client`**\n",
        "This defines the shared connection details for all requests:\n",
        "- `base_url`: The root URL for the API\n",
        "- `auth`: (Optional) Authentication method to use — such as API key or token\n",
        "- `headers`: (Optional) Custom headers for requests\n",
        "- `paginator`: (Optional) Default paginator for all resources\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ9vpPVWeeuy"
      },
      "source": [
        "\n",
        "\n",
        "### **`resources`**\n",
        "A list of resource definitions. Each resource becomes a table in your destination.\n",
        "A resource includes:\n",
        "- `name`: Table name for the resource\n",
        "- `endpoint`: Path to the endpoint, query parameters, pagination config\n",
        "- `write_disposition`: How to load the data (`append`, `merge`, `replace`)\n",
        "- `primary_key`: Optional key used when merging\n",
        "- `data_selector`: JSONPath to extract data from the response (e.g., \"articles\")\n",
        "- `processing_steps`: Optional filters and transformations\n",
        "- `response_actions`: Optional hooks to inspect or alter the HTTP response\n",
        "\n",
        "Let’s build a real-world configuration step-by-step using NewsAPI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "LKQYL2L1JLsu"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install dlt[duckdb]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dlt\n",
        "from google.colab import userdata\n",
        "from dlt.sources.rest_api import rest_api_source\n",
        "\n",
        "api_key = userdata.get(\"NEWS_API_KEY\")\n",
        "\n",
        "# Define config\n",
        "news_api_config = {\n",
        "    \"client\": {\n",
        "        \"base_url\": \"https://newsapi.org/v2/\",\n",
        "        \"auth\": {\n",
        "            \"type\": \"api_key\",\n",
        "            \"name\": \"apiKey\",\n",
        "            \"api_key\": api_key,\n",
        "            \"location\": \"query\"\n",
        "         },\n",
        "        \"paginator\": {\n",
        "            \"type\": \"page_number\",\n",
        "            \"page_param\": \"page\",\n",
        "            \"stop_after_empty_page\": True,\n",
        "            \"total_path\": None,\n",
        "            \"maximum_page\": 3,\n",
        "        }\n",
        "    },\n",
        "    \"resource_defaults\": {\n",
        "        \"primary_key\": \"id\",\n",
        "        \"write_disposition\": \"merge\",\n",
        "        \"endoints\": {\n",
        "            \"params\": {\n",
        "                \"per_page\": 100,\n",
        "            },\n",
        "        },\n",
        "\n",
        "    },\n",
        "    \"resources\": [\n",
        "        {\n",
        "           \"name\": \"py_articles\",\n",
        "           \"endpoint\": {\n",
        "               \"path\": \"everything\",\n",
        "               \"params\": {\n",
        "                   \"q\": \"Jesus\",\n",
        "                   \"pageSize\": 20,\n",
        "                   \"language\": \"it\",\n",
        "                   \"from\": {\n",
        "                       \"type\": \"incremental\",\n",
        "                       \"cursor_path\": \"publishedAt\",\n",
        "                       \"initial_value\": \"2025-04-15T00:00:00Z\",\n",
        "                   },\n",
        "               },\n",
        "           },\n",
        "           #\"write_disposition\": \"replace\",\n",
        "        }\n",
        "    ],\n",
        "}\n",
        "\n",
        "news_source = rest_api_source(news_config)\n",
        "\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name = \"news_pipeline\",\n",
        "    destination = \"duckdb\",\n",
        "    dataset_name = \"news_data\"\n",
        ")\n",
        "\n",
        "load_info = pipeline.run(news_source)\n",
        "print(pipeline.last_trace)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2l_eZoyJFnyb",
        "outputId": "23474f31-b444-4f73-b541-10cd43c3c143"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-12-04 16:55:56,745|[WARNING]|306|133375044358144|dlt|client.py|detect_paginator:365|Fallback `paginator` used: `SinglePagePaginator at 794d41a8e330`. Please provide paginator manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run started at 2025-12-04 16:55:56.348113+00:00 and COMPLETED in 0.70 seconds with 4 steps.\n",
            "Step extract COMPLETED in 0.36 seconds.\n",
            "\n",
            "Load package 1764867356.4350426 is EXTRACTED and NOT YET LOADED to the destination and contains no failed jobs\n",
            "\n",
            "Step normalize COMPLETED in 0.05 seconds.\n",
            "Normalized data for the following tables:\n",
            "- news_articles: 100 row(s)\n",
            "\n",
            "Load package 1764867356.4350426 is NORMALIZED and NOT YET LOADED to the destination and contains no failed jobs\n",
            "\n",
            "Step load COMPLETED in 0.23 seconds.\n",
            "Pipeline news_pipeline load step completed in 0.21 seconds\n",
            "1 load package(s) were loaded to destination duckdb and into dataset news_data\n",
            "The duckdb destination used duckdb:////content/news_pipeline.duckdb location to store data\n",
            "Load package 1764867356.4350426 is LOADED and contains no failed jobs\n",
            "\n",
            "Step run COMPLETED in 0.70 seconds.\n",
            "Pipeline news_pipeline load step completed in 0.21 seconds\n",
            "1 load package(s) were loaded to destination duckdb and into dataset news_data\n",
            "The duckdb destination used duckdb:////content/news_pipeline.duckdb location to store data\n",
            "Load package 1764867356.4350426 is LOADED and contains no failed jobs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.dataset().news_articles.df().head()"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "TD22yW6nOU-7",
        "outputId": "9b51c520-ff8d-4d66-985b-d8e332075f0d"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      source__name                                    author  \\\n",
              "0     Slashdot.org                                    msmash   \n",
              "1  Palm Beach Post  Kim Luciani, USA TODAY NETWORK - Florida   \n",
              "2         Hackaday                                 Lewin Day   \n",
              "3        USA Today                  Pete Thomas, For The Win   \n",
              "4         Hackaday                                Maya Posch   \n",
              "\n",
              "                                               title  \\\n",
              "0  Lenovo Stockpiling PC Memory Due To 'Unprecede...   \n",
              "1  Photos show monster Burmese pythons caught in ...   \n",
              "2      Simple Tricks To Make Your Python Code Faster   \n",
              "3  Florida man captures giant python, told to ‘pu...   \n",
              "4             KiDoom Brings Classic Shooter to KiCad   \n",
              "\n",
              "                                         description  \\\n",
              "0  Lenovo is stockpiling memory and other critica...   \n",
              "1  Take a look at some of the biggest Burmese pyt...   \n",
              "2  Python has become one of the most popular prog...   \n",
              "3  Wade Gardner drove home for snare and ax after...   \n",
              "4  As the saying goes: if it has a processor and ...   \n",
              "\n",
              "                                                 url  \\\n",
              "0  https://it.slashdot.org/story/25/11/24/154202/...   \n",
              "1  https://www.palmbeachpost.com/story/news/envir...   \n",
              "2  https://hackaday.com/2025/11/25/simple-tricks-...   \n",
              "3  https://ftw.usatoday.com/story/sports/outdoors...   \n",
              "4  https://hackaday.com/2025/11/26/kidoom-brings-...   \n",
              "\n",
              "                                        url_to_image  \\\n",
              "0             https://a.fsdn.com/sd/topics/it_64.png   \n",
              "1  https://media.zenfs.com/en/palm_beach_post_nat...   \n",
              "2  https://hackaday.com/wp-content/uploads/2025/1...   \n",
              "3  https://media.zenfs.com/en/ftw_outdoors_usa_to...   \n",
              "4  https://hackaday.com/wp-content/uploads/2025/1...   \n",
              "\n",
              "               published_at  \\\n",
              "0 2025-11-24 16:01:00+00:00   \n",
              "1 2025-11-21 11:02:48+00:00   \n",
              "2 2025-11-25 12:00:56+00:00   \n",
              "3 2025-11-20 18:01:23+00:00   \n",
              "4 2025-11-27 06:00:00+00:00   \n",
              "\n",
              "                                             content        _dlt_load_id  \\\n",
              "0  Five is a sufficiently close approximation to ...  1764865805.0959382   \n",
              "1  The Florida Python Challenge is an annual 10-d...  1764865805.0959382   \n",
              "2  Python has become one of the most popular prog...  1764865805.0959382   \n",
              "3  A Florida resident, with the help of friends, ...  1764865805.0959382   \n",
              "4  As the saying goes: if it has a processor and ...  1764865805.0959382   \n",
              "\n",
              "          _dlt_id source__id  \n",
              "0  rX9F2r+vfdKLzQ       None  \n",
              "1  DhWrcvjCG8W2xw       None  \n",
              "2  yRscxxrKiIHl8Q       None  \n",
              "3  Us7Cw2WBhQ4qxw  usa-today  \n",
              "4  RBcuOzNsYSBuEw       None  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e21f65d4-9f12-4b92-a06b-883450134e2b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source__name</th>\n",
              "      <th>author</th>\n",
              "      <th>title</th>\n",
              "      <th>description</th>\n",
              "      <th>url</th>\n",
              "      <th>url_to_image</th>\n",
              "      <th>published_at</th>\n",
              "      <th>content</th>\n",
              "      <th>_dlt_load_id</th>\n",
              "      <th>_dlt_id</th>\n",
              "      <th>source__id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Slashdot.org</td>\n",
              "      <td>msmash</td>\n",
              "      <td>Lenovo Stockpiling PC Memory Due To 'Unprecede...</td>\n",
              "      <td>Lenovo is stockpiling memory and other critica...</td>\n",
              "      <td>https://it.slashdot.org/story/25/11/24/154202/...</td>\n",
              "      <td>https://a.fsdn.com/sd/topics/it_64.png</td>\n",
              "      <td>2025-11-24 16:01:00+00:00</td>\n",
              "      <td>Five is a sufficiently close approximation to ...</td>\n",
              "      <td>1764865805.0959382</td>\n",
              "      <td>rX9F2r+vfdKLzQ</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Palm Beach Post</td>\n",
              "      <td>Kim Luciani, USA TODAY NETWORK - Florida</td>\n",
              "      <td>Photos show monster Burmese pythons caught in ...</td>\n",
              "      <td>Take a look at some of the biggest Burmese pyt...</td>\n",
              "      <td>https://www.palmbeachpost.com/story/news/envir...</td>\n",
              "      <td>https://media.zenfs.com/en/palm_beach_post_nat...</td>\n",
              "      <td>2025-11-21 11:02:48+00:00</td>\n",
              "      <td>The Florida Python Challenge is an annual 10-d...</td>\n",
              "      <td>1764865805.0959382</td>\n",
              "      <td>DhWrcvjCG8W2xw</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hackaday</td>\n",
              "      <td>Lewin Day</td>\n",
              "      <td>Simple Tricks To Make Your Python Code Faster</td>\n",
              "      <td>Python has become one of the most popular prog...</td>\n",
              "      <td>https://hackaday.com/2025/11/25/simple-tricks-...</td>\n",
              "      <td>https://hackaday.com/wp-content/uploads/2025/1...</td>\n",
              "      <td>2025-11-25 12:00:56+00:00</td>\n",
              "      <td>Python has become one of the most popular prog...</td>\n",
              "      <td>1764865805.0959382</td>\n",
              "      <td>yRscxxrKiIHl8Q</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>USA Today</td>\n",
              "      <td>Pete Thomas, For The Win</td>\n",
              "      <td>Florida man captures giant python, told to ‘pu...</td>\n",
              "      <td>Wade Gardner drove home for snare and ax after...</td>\n",
              "      <td>https://ftw.usatoday.com/story/sports/outdoors...</td>\n",
              "      <td>https://media.zenfs.com/en/ftw_outdoors_usa_to...</td>\n",
              "      <td>2025-11-20 18:01:23+00:00</td>\n",
              "      <td>A Florida resident, with the help of friends, ...</td>\n",
              "      <td>1764865805.0959382</td>\n",
              "      <td>Us7Cw2WBhQ4qxw</td>\n",
              "      <td>usa-today</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hackaday</td>\n",
              "      <td>Maya Posch</td>\n",
              "      <td>KiDoom Brings Classic Shooter to KiCad</td>\n",
              "      <td>As the saying goes: if it has a processor and ...</td>\n",
              "      <td>https://hackaday.com/2025/11/26/kidoom-brings-...</td>\n",
              "      <td>https://hackaday.com/wp-content/uploads/2025/1...</td>\n",
              "      <td>2025-11-27 06:00:00+00:00</td>\n",
              "      <td>As the saying goes: if it has a processor and ...</td>\n",
              "      <td>1764865805.0959382</td>\n",
              "      <td>RBcuOzNsYSBuEw</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e21f65d4-9f12-4b92-a06b-883450134e2b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e21f65d4-9f12-4b92-a06b-883450134e2b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e21f65d4-9f12-4b92-a06b-883450134e2b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-33166a34-34b4-43af-8e80-41803a7194c6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-33166a34-34b4-43af-8e80-41803a7194c6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-33166a34-34b4-43af-8e80-41803a7194c6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pipeline\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"source__name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Palm Beach Post\",\n          \"USA Today\",\n          \"Slashdot.org\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"author\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Kim Luciani, USA TODAY NETWORK - Florida\",\n          \"Maya Posch\",\n          \"Lewin Day\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Photos show monster Burmese pythons caught in Florida Python Challenge\",\n          \"KiDoom Brings Classic Shooter to KiCad\",\n          \"Simple Tricks To Make Your Python Code Faster\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Take a look at some of the biggest Burmese pythons caught during Florida's annual invasive snake hunt.\",\n          \"As the saying goes: if it has a processor and a display, it can run DOOM. The corollary here is that if some software displays things, someone will figure out a way to make it render the iconic sho\\u2026\",\n          \"Python has become one of the most popular programming languages out there, particularly for beginners and those new to the hacker/maker world. Unfortunately, while it\\u2019s easy to\\u00a0 get something up \\u2026read more\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"https://www.palmbeachpost.com/story/news/environment/2025/11/21/biggest-burmese-pythons-florida-challenge-hunt-photos-pictures/87377886007/\",\n          \"https://hackaday.com/2025/11/26/kidoom-brings-classic-shooter-to-kicad/\",\n          \"https://hackaday.com/2025/11/25/simple-tricks-to-make-your-python-code-faster/\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url_to_image\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"https://media.zenfs.com/en/palm_beach_post_natl_articles_437/02869036afa85dcdb544c5d245a6b746\",\n          \"https://hackaday.com/wp-content/uploads/2025/11/kidoom-demo-new.gif\",\n          \"https://hackaday.com/wp-content/uploads/2025/11/pythoni.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"published_at\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-11-20 18:01:23+00:00\",\n        \"max\": \"2025-11-27 06:00:00+00:00\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2025-11-21 11:02:48+00:00\",\n          \"2025-11-27 06:00:00+00:00\",\n          \"2025-11-25 12:00:56+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"The Florida Python Challenge is an annual 10-day hunting competition encouraging participants to remove Burmese pythons from eight Florida Fish and Wildlife Conservation Commission-managed lands in S\\u2026 [+1495 chars]\",\n          \"As the saying goes: if it has a processor and a display, it can run DOOM. The corollary here is that if some software displays things, someone will figure out a way to make it render the iconic shoot\\u2026 [+1028 chars]\",\n          \"Python has become one of the most popular programming languages out there, particularly for beginners and those new to the hacker/maker world. Unfortunately, while it\\u2019s easy to\\u00a0 get something up and \\u2026 [+1375 chars]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"_dlt_load_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"1764865805.0959382\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"_dlt_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"DhWrcvjCG8W2xw\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source__id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"usa-today\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "MVzPMBateFgg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "collapsed": true,
        "outputId": "e4652338-f7c0-47ce-f10b-7af074850a59"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colob'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-999340503.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdlt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdlt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrest_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrest_api_source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0macces_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NEWS_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colob'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import dlt\n",
        "from google.colob import userdata\n",
        "from dlt.sources.rest_api import rest_api_source\n",
        "\n",
        "acces_token = userdata.get(\"NEWS_API_KEY\")\n",
        "\n",
        "news_config: RESTAPIConfig = {\n",
        "    \"client\": {\n",
        "        \"base_url\": \"https://newsapi.org/v2/\",\n",
        "    },\n",
        "    \"resources\": [\n",
        "        {\n",
        "            \"name\": \"news_articles\",\n",
        "            \"endpoint\": {\"path\": \"everything\", \"params\": {\"q\": \"python\"}},\n",
        "        }\n",
        "    ],\n",
        "}\n",
        "\n",
        "news_source = rest_api_source(news_config)\n",
        "\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"news_pipeline\", destination=\"duckdb\", dataset_name=\"news\"\n",
        ")\n",
        "\n",
        "load_info = pipeline.run(news_source)\n",
        "print(pipeline.last_trace)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YePPpjKMmX4"
      },
      "source": [
        "### Question 2:\n",
        "\n",
        "What error was thrown in the example above?\n",
        "\n",
        ">Answer this question and select the correct option in the homework Google Form."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7g_V1v2xt1L"
      },
      "source": [
        "## **2. Add authentication**\n",
        "\n",
        "NewsAPI requires an API key to be sent with every request. We use dlt's built-in `api_key` authentication method, which places the key into the query string automatically:\n",
        "\n",
        "```python\n",
        "\"auth\": {\n",
        "    \"type\": \"api_key\",\n",
        "    \"name\": \"apiKey\",\n",
        "    \"api_key\": \"your_key\",\n",
        "    \"location\": \"query\",\n",
        "}\n",
        "```\n",
        "\n",
        "This ensures every request has `?apiKey=...` added. It's simple and secure, especially when storing the key in ENVs or Colab's secret manager.\n",
        "\n",
        "\n",
        "The available authentication methods you can find in [dlt documentation](https://dlthub.com/docs/general-usage/http/rest-client#authentication)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "pIsnhKAo0-bU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "4b40f7cb-01cf-4c5f-aa4f-8307ab71f2ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-12-04 16:29:20,439|[WARNING]|306|133375044358144|dlt|client.py|detect_paginator:365|Fallback `paginator` used: `SinglePagePaginator at 794d8b7a7290`. Please provide paginator manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run started at 2025-12-04 16:29:20.025297+00:00 and COMPLETED in 0.78 seconds with 4 steps.\n",
            "Step extract COMPLETED in 0.40 seconds.\n",
            "\n",
            "Load package 1764865760.1061761 is EXTRACTED and NOT YET LOADED to the destination and contains no failed jobs\n",
            "\n",
            "Step normalize COMPLETED in 0.06 seconds.\n",
            "Normalized data for the following tables:\n",
            "- news_articles: 100 row(s)\n",
            "- _dlt_pipeline_state: 1 row(s)\n",
            "\n",
            "Load package 1764865760.1061761 is NORMALIZED and NOT YET LOADED to the destination and contains no failed jobs\n",
            "\n",
            "Step load COMPLETED in 0.26 seconds.\n",
            "Pipeline news_pipeline load step completed in 0.24 seconds\n",
            "1 load package(s) were loaded to destination duckdb and into dataset news\n",
            "The duckdb destination used duckdb:////content/news_pipeline.duckdb location to store data\n",
            "Load package 1764865760.1061761 is LOADED and contains no failed jobs\n",
            "\n",
            "Step run COMPLETED in 0.78 seconds.\n",
            "Pipeline news_pipeline load step completed in 0.24 seconds\n",
            "1 load package(s) were loaded to destination duckdb and into dataset news\n",
            "The duckdb destination used duckdb:////content/news_pipeline.duckdb location to store data\n",
            "Load package 1764865760.1061761 is LOADED and contains no failed jobs\n"
          ]
        }
      ],
      "source": [
        "import dlt\n",
        "from dlt.sources.rest_api import rest_api_source\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get(\"NEWS_API_KEY\")\n",
        "\n",
        "\n",
        "news_config = {\n",
        "    \"client\": {\n",
        "        \"base_url\": \"https://newsapi.org/v2/\",\n",
        "        \"auth\": {\n",
        "            \"type\": \"api_key\",\n",
        "            \"name\": \"apiKey\",\n",
        "            \"api_key\": api_key,\n",
        "            \"location\": \"query\",\n",
        "        },\n",
        "    },\n",
        "    \"resources\": [\n",
        "        {\n",
        "            \"name\": \"news_articles\",\n",
        "            \"endpoint\": {\"path\": \"everything\", \"params\": {\"q\": \"python\"}},\n",
        "        }\n",
        "    ],\n",
        "}\n",
        "\n",
        "news_source = rest_api_source(news_config)\n",
        "\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"news_pipeline\", destination=\"duckdb\", dataset_name=\"news\"\n",
        ")\n",
        "\n",
        "load_info = pipeline.run(news_source)\n",
        "print(pipeline.last_trace)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ohc-78khYPz8"
      },
      "source": [
        "## **3. Add pagination**\n",
        "\n",
        "The REST API source will try to automatically handle pagination for you. This works by detecting the pagination details from the first API response. Unfortunately, it doesn't work for NewsAPI.\n",
        "\n",
        "NewsAPI uses page-based pagination. We use the built-in `PageNumberPaginator` to automatically paginate through pages until results run out:\n",
        "\n",
        "\n",
        "```python\n",
        "\"paginator\": {\n",
        "    \"type\": \"page_number\",\n",
        "    \"page_param\": \"page\",\n",
        "    \"stop_after_empty_page\": True,\n",
        "    \"total_path\": None,\n",
        "    \"maximum_page\": 3,\n",
        "},\n",
        "```\n",
        "\n",
        "This will fetch up to 3 pages of results, stopping early if a page is empty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeYld6yLYU2H"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "from dlt.sources.rest_api import rest_api_source\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get(\"NEWS_API_KEY\")\n",
        "\n",
        "\n",
        "news_config = {\n",
        "    \"client\": {\n",
        "        \"base_url\": \"https://newsapi.org/v2/\",\n",
        "        \"auth\": {\n",
        "            \"type\": \"api_key\",\n",
        "            \"name\": \"apiKey\",\n",
        "            \"api_key\": api_key,\n",
        "            \"location\": \"query\",\n",
        "        },\n",
        "        \"paginator\": {\n",
        "            \"base_page\": 1,\n",
        "            \"type\": \"page_number\",\n",
        "            \"page_param\": \"page\",\n",
        "            \"total_path\": None,\n",
        "            \"maximum_page\": 3,\n",
        "        },\n",
        "    },\n",
        "    \"resources\": [\n",
        "        {\n",
        "            \"name\": \"news_articles\",\n",
        "            \"endpoint\": {\"path\": \"everything\", \"params\": {\"q\": \"python\"}},\n",
        "        }\n",
        "    ],\n",
        "}\n",
        "\n",
        "news_source = rest_api_source(news_config)\n",
        "\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"news_pipeline\", destination=\"duckdb\", dataset_name=\"news\"\n",
        ")\n",
        "\n",
        "load_info = pipeline.run(news_source)\n",
        "print(pipeline.last_trace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXxNygRFjJbo"
      },
      "outputs": [],
      "source": [
        "pipeline.dataset().news_articles.df().head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-tcsvgbBdr6"
      },
      "source": [
        "## **4. Add order, filtering via params**\n",
        "We can filter articles using query parameters supported by NewsAPI:\n",
        "\n",
        "```python\n",
        "\"params\": {\n",
        "    \"q\": \"python\",\n",
        "    \"language\": \"en\",\n",
        "    \"pageSize\": 20,\n",
        "},\n",
        "```\n",
        "\n",
        "- `q`: search keyword (e.g. \"python\")\n",
        "- `language`: filter by article language\n",
        "- `pageSize`: number of articles per page (max 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvSLrij37WCw"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "from dlt.sources.rest_api import rest_api_source\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get(\"NEWS_API_KEY\")\n",
        "\n",
        "\n",
        "news_config = {\n",
        "    \"client\": {\n",
        "        \"base_url\": \"https://newsapi.org/v2/\",\n",
        "        \"auth\": {\n",
        "            \"type\": \"api_key\",\n",
        "            \"name\": \"apiKey\",\n",
        "            \"api_key\": api_key,\n",
        "            \"location\": \"query\",\n",
        "        },\n",
        "        \"paginator\": {\n",
        "            \"base_page\": 1,\n",
        "            \"type\": \"page_number\",\n",
        "            \"page_param\": \"page\",\n",
        "            \"total_path\": None,\n",
        "            \"maximum_page\": 3,\n",
        "        },\n",
        "    },\n",
        "    \"resources\": [\n",
        "        {\n",
        "            \"name\": \"news_articles\",\n",
        "            \"endpoint\": {\n",
        "                \"path\": \"everything\",\n",
        "                \"params\": {\n",
        "                    \"q\": \"python\",\n",
        "                    \"language\": \"en\",\n",
        "                    \"pageSize\": 20,\n",
        "                },\n",
        "            },\n",
        "        }\n",
        "    ],\n",
        "}\n",
        "\n",
        "news_source = rest_api_source(news_config)\n",
        "\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"news_pipeline\", destination=\"duckdb\", dataset_name=\"news\"\n",
        ")\n",
        "\n",
        "load_info = pipeline.run(news_source)\n",
        "print(pipeline.last_trace)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsd_SPZD7nBj"
      },
      "source": [
        "## **5. Incremental loading**\n",
        "\n",
        "Although NewsAPI does not support true incremental loading via cursors, you can simulate it using the `from` or `to` date filters and dlt's `incremental` loader:\n",
        "\n",
        "```python\n",
        "\"from\": {\n",
        "    \"type\": \"incremental\",\n",
        "    \"cursor_path\": \"publishedAt\",\n",
        "    \"initial_value\": \"2024-01-01T00:00:00Z\",\n",
        "},\n",
        "```\n",
        "\n",
        "This setup means:\n",
        "- dlt will remember the last `publishedAt` seen\n",
        "- On the next run, it will only request articles newer than that\n",
        "\n",
        "This is optional and depends on your usage pattern.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snTSLaT17l97"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "from dlt.sources.rest_api import rest_api_source\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get(\"NEWS_API_KEY\")\n",
        "\n",
        "\n",
        "news_config = {\n",
        "    \"client\": {\n",
        "        \"base_url\": \"https://newsapi.org/v2/\",\n",
        "        \"auth\": {\n",
        "            \"type\": \"api_key\",\n",
        "            \"name\": \"apiKey\",\n",
        "            \"api_key\": api_key,\n",
        "            \"location\": \"query\",\n",
        "        },\n",
        "        \"paginator\": {\n",
        "            \"base_page\": 1,\n",
        "            \"type\": \"page_number\",\n",
        "            \"page_param\": \"page\",\n",
        "            \"total_path\": None,\n",
        "            \"maximum_page\": 3,\n",
        "        },\n",
        "    },\n",
        "    \"resources\": [\n",
        "        {\n",
        "            \"name\": \"news_articles\",\n",
        "            \"endpoint\": {\n",
        "                \"path\": \"everything\",\n",
        "                \"params\": {\n",
        "                    \"q\": \"python\",\n",
        "                    \"language\": \"en\",\n",
        "                    \"pageSize\": 20,\n",
        "                    \"from\": {\n",
        "                        \"type\": \"incremental\",\n",
        "                        \"cursor_path\": \"publishedAt\",\n",
        "                        \"initial_value\": \"2025-04-15T00:00:00Z\",\n",
        "                    },\n",
        "                },\n",
        "            },\n",
        "        }\n",
        "    ],\n",
        "}\n",
        "\n",
        "news_source = rest_api_source(news_config)\n",
        "\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"news_pipeline\", destination=\"duckdb\", dataset_name=\"news\"\n",
        ")\n",
        "\n",
        "load_info = pipeline.run(news_source)\n",
        "print(pipeline.last_trace)\n",
        "\n",
        "# Run the pipeline one more time\n",
        "load_info = pipeline.run(news_source)\n",
        "print(pipeline.last_trace)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRQZ2oswli2m"
      },
      "source": [
        "## **6. Add more endpoints**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr0yYE_or4T7"
      },
      "source": [
        "### Set defaults\n",
        "\n",
        "First, set some defaults for all endpoints:\n",
        "\n",
        "```python\n",
        "\"resource_defaults\": {\n",
        "    \"primary_key\": \"id\",\n",
        "    \"write_disposition\": \"merge\",\n",
        "    \"endpoint\": {\n",
        "        \"params\": {\n",
        "            \"per_page\": 100,\n",
        "        },\n",
        "    },\n",
        "},\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOM5_YfumelO"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "from dlt.sources.rest_api import rest_api_source\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get(\"NEWS_API_KEY\")\n",
        "\n",
        "\n",
        "news_config = {\n",
        "    \"client\": {\n",
        "        \"base_url\": \"https://newsapi.org/v2/\",\n",
        "        \"auth\": {\n",
        "            \"type\": \"api_key\",\n",
        "            \"name\": \"apiKey\",\n",
        "            \"api_key\": api_key,\n",
        "            \"location\": \"query\",\n",
        "        },\n",
        "        \"paginator\": {\n",
        "            \"base_page\": 1,\n",
        "            \"type\": \"page_number\",\n",
        "            \"page_param\": \"page\",\n",
        "            \"total_path\": None,\n",
        "            \"maximum_page\": 3,\n",
        "        },\n",
        "    },\n",
        "    \"resource_defaults\": {\n",
        "        \"write_disposition\": \"append\",\n",
        "        \"endpoint\": {\n",
        "            \"params\": {\n",
        "                \"language\": \"en\",\n",
        "                \"pageSize\": 20,\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "    \"resources\": [\n",
        "        {\n",
        "            \"name\": \"news_articles\",\n",
        "            \"endpoint\": {\n",
        "                \"path\": \"everything\",\n",
        "                \"params\": {\n",
        "                    \"q\": \"python\",\n",
        "                    \"from\": {\n",
        "                        \"type\": \"incremental\",\n",
        "                        \"cursor_path\": \"publishedAt\",\n",
        "                        \"initial_value\": \"2025-04-15T00:00:00Z\",\n",
        "                    },\n",
        "                },\n",
        "            },\n",
        "        }\n",
        "    ],\n",
        "}\n",
        "\n",
        "news_source = rest_api_source(news_config)\n",
        "\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"news_pipeline\", destination=\"duckdb\", dataset_name=\"news\"\n",
        ")\n",
        "\n",
        "load_info = pipeline.run(news_source)\n",
        "print(pipeline.last_trace)\n",
        "\n",
        "# Run the pipeline one more time\n",
        "load_info = pipeline.run(news_source)\n",
        "print(pipeline.last_trace)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB2O6j_bp9kH"
      },
      "source": [
        "`resource_defaults` contains the default values to configure the dlt resources returned by this source.\n",
        "\n",
        "`resources` object contains the configuration for each resource.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F48BwIHZrhFm"
      },
      "source": [
        "### Add same level endpoint\n",
        "\n",
        "To load additional endpoints like `/top-headlines` or `/sources`, you can simply add more entries to the `resources` list:\n",
        "```python\n",
        "{\n",
        "  \"name\": \"top_headlines\",\n",
        "  \"endpoint\": {\n",
        "    \"path\": \"top-headlines\",\n",
        "    \"params\": {\"country\": \"us\", \"pageSize\": 10},\n",
        "    \"paginator\": {\"type\": \"page_number\", \"page_param\": \"page\"}\n",
        "  },\n",
        "  \"primary_key\": \"url\",\n",
        "  \"write_disposition\": \"append\",\n",
        "  \"data_selector\": \"articles\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTB801pPsJ6_"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "from dlt.sources.rest_api import rest_api_source\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get(\"NEWS_API_KEY\")\n",
        "\n",
        "\n",
        "news_config = {\n",
        "    \"client\": {\n",
        "        \"base_url\": \"https://newsapi.org/v2/\",\n",
        "        \"auth\": {\n",
        "            \"type\": \"api_key\",\n",
        "            \"name\": \"apiKey\",\n",
        "            \"api_key\": api_key,\n",
        "            \"location\": \"query\",\n",
        "        },\n",
        "        \"paginator\": {\n",
        "            \"base_page\": 1,\n",
        "            \"type\": \"page_number\",\n",
        "            \"page_param\": \"page\",\n",
        "            \"total_path\": None,\n",
        "            \"maximum_page\": 3,\n",
        "        },\n",
        "    },\n",
        "    \"resource_defaults\": {\n",
        "        \"write_disposition\": \"append\",\n",
        "        \"endpoint\": {\n",
        "            \"params\": {\n",
        "                \"language\": \"en\",\n",
        "                \"pageSize\": 20,\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "    \"resources\": [\n",
        "        {\n",
        "            \"name\": \"news_articles\",\n",
        "            \"endpoint\": {\n",
        "                \"path\": \"everything\",\n",
        "                \"params\": {\n",
        "                    \"q\": \"python\",\n",
        "                    \"from\": {\n",
        "                        \"type\": \"incremental\",\n",
        "                        \"cursor_path\": \"publishedAt\",\n",
        "                        \"initial_value\": \"2025-04-15T00:00:00Z\",\n",
        "                    },\n",
        "                },\n",
        "            },\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"top_headlines\",\n",
        "            \"endpoint\": {\n",
        "                \"path\": \"top-headlines\",\n",
        "                \"params\": {\"country\": \"us\"},\n",
        "            },\n",
        "        },\n",
        "    ],\n",
        "}\n",
        "\n",
        "news_source = rest_api_source(news_config)\n",
        "\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"news_pipeline\", destination=\"duckdb\", dataset_name=\"news\"\n",
        ")\n",
        "\n",
        "load_info = pipeline.run(news_source)\n",
        "print(pipeline.last_trace)\n",
        "\n",
        "pipeline.dataset().top_headlines.df().head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fwb3Zq4eH51Y"
      },
      "source": [
        "## Advanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8vqkmnHMOu1"
      },
      "source": [
        "### Response actions\n",
        "\n",
        "The `response_actions` field in the endpoint configuration allows you to specify how to **handle specific responses** or all responses from the API.\n",
        "\n",
        "For example:\n",
        "- Responses with specific status codes or content substrings can be ignored.\n",
        "- All responses or only responses with specific status codes or content substrings can be transformed with a custom callable, such as a function. This callable is passed on to the requests library as a response hook. The callable can modify the response object and has to return it for the modifications to take effect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqaM75U9IQmW"
      },
      "source": [
        "```python\n",
        "\"resources\": [\n",
        "    {\n",
        "        \"name\": \"news_articles\",\n",
        "        \"endpoint\": {\n",
        "            \"path\": \"everything\",\n",
        "            \"response_actions\": [\n",
        "                {\n",
        "                    \"status_code\": 200,\n",
        "                    \"content\": \"some text\",\n",
        "                    \"action\": do_something,\n",
        "                },\n",
        "            ],\n",
        "        },\n",
        "    },\n",
        "```\n",
        "\n",
        "Fields:\n",
        "\n",
        "* `status_code` (int, optional): The HTTP status code to match.\n",
        "* `content` (str, optional): A substring to search for in the response content.\n",
        "* `action` (str or Callable or List[Callable], optional): The action to take when the condition is met. Currently supported actions:\n",
        "\"ignore\": Ignore the response.\n",
        "a callable accepting and returning the response object.\n",
        "a list of callables, each accepting and returning the response object.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOa-wwPEIope"
      },
      "outputs": [],
      "source": [
        "def debug_response(\n",
        "    response: requests.Response, *args: Any, **kwargs: Any\n",
        ") -> requests.Response:\n",
        "    print(\"Intercepted:\", response.status_code)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M93o6LrepBf1"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "from dlt.sources.rest_api import rest_api_source\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get(\"NEWS_API_KEY\")\n",
        "\n",
        "\n",
        "news_config = {\n",
        "    \"client\": {\n",
        "        \"base_url\": \"https://newsapi.org/v2/\",\n",
        "        \"auth\": {\n",
        "            \"type\": \"api_key\",\n",
        "            \"name\": \"apiKey\",\n",
        "            \"api_key\": api_key,\n",
        "            \"location\": \"query\",\n",
        "        },\n",
        "        \"paginator\": {\n",
        "            \"base_page\": 1,\n",
        "            \"type\": \"page_number\",\n",
        "            \"page_param\": \"page\",\n",
        "            \"total_path\": None,\n",
        "            \"maximum_page\": 3,\n",
        "        },\n",
        "    },\n",
        "    \"resource_defaults\": {\n",
        "        \"write_disposition\": \"append\",\n",
        "        \"endpoint\": {\n",
        "            \"params\": {\n",
        "                \"language\": \"en\",\n",
        "                \"pageSize\": 20,\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "    \"resources\": [\n",
        "        {\n",
        "            \"name\": \"news_articles\",\n",
        "            \"endpoint\": {\n",
        "                \"path\": \"everything\",\n",
        "                \"response_actions\": [\n",
        "                    {\n",
        "                        \"status_code\": 200,\n",
        "                        \"action\": debug_response,  # <--- add some action\n",
        "                    },\n",
        "                ],\n",
        "                \"params\": {\n",
        "                    \"q\": \"python\",\n",
        "                    \"from\": {\n",
        "                        \"type\": \"incremental\",\n",
        "                        \"cursor_path\": \"publishedAt\",\n",
        "                        \"initial_value\": \"2025-04-15T00:00:00Z\",\n",
        "                    },\n",
        "                },\n",
        "            },\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"top_headlines\",\n",
        "            \"endpoint\": {\n",
        "                \"path\": \"top-headlines\",\n",
        "                \"params\": {\"country\": \"us\"},\n",
        "            },\n",
        "        },\n",
        "    ],\n",
        "}\n",
        "\n",
        "news_source = rest_api_source(news_config)\n",
        "\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"news_pipeline\", destination=\"duckdb\", dataset_name=\"news\"\n",
        ")\n",
        "\n",
        "load_info = pipeline.run(news_source)\n",
        "print(pipeline.last_trace)\n",
        "\n",
        "pipeline.dataset().news_articles.df().head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ua6sYLsMLO_"
      },
      "source": [
        "### Processing steps: filter and transform data\n",
        "The `processing_steps` field in the resource configuration allows you to **apply transformations** to the data fetched from the API before it is loaded into your destination.\n",
        "\n",
        "This is useful when you need to\n",
        "- **filter out** certain records,\n",
        "- **modify the data** structure,\n",
        "- **anonymize** sensitive information.\n",
        "\n",
        "Each processing step is a dictionary specifying the type of operation (filter or map) and the function to apply. Steps apply in the order they are listed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j9a6utVMFP3"
      },
      "source": [
        "```python\n",
        " \"resources\": [\n",
        "        {\n",
        "            \"name\": \"news_articles\",\n",
        "            \"processing_steps\": [\n",
        "                {\"filter\": lambda x: len(x[\"author\"]) > 0},\n",
        "                {\"map\": lower_title},\n",
        "            ],\n",
        "        },\n",
        "    ],\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7VUsTmUMo67"
      },
      "outputs": [],
      "source": [
        "def lower_title(record: TDataItem) -> TDataItem:\n",
        "    record[\"title\"] = record[\"title\"].lower()\n",
        "    return record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b2uZ5WvqbSO"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "from dlt.sources.rest_api import rest_api_source\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get(\"NEWS_API_KEY\")\n",
        "\n",
        "\n",
        "news_config = {\n",
        "    \"client\": {\n",
        "        \"base_url\": \"https://newsapi.org/v2/\",\n",
        "        \"auth\": {\n",
        "            \"type\": \"api_key\",\n",
        "            \"name\": \"apiKey\",\n",
        "            \"api_key\": api_key,\n",
        "            \"location\": \"query\",\n",
        "        },\n",
        "        \"paginator\": {\n",
        "            \"base_page\": 1,\n",
        "            \"type\": \"page_number\",\n",
        "            \"page_param\": \"page\",\n",
        "            \"total_path\": None,\n",
        "            \"maximum_page\": 3,\n",
        "        },\n",
        "    },\n",
        "    \"resource_defaults\": {\n",
        "        \"write_disposition\": \"append\",\n",
        "        \"endpoint\": {\n",
        "            \"params\": {\n",
        "                \"language\": \"en\",\n",
        "                \"pageSize\": 20,\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "    \"resources\": [\n",
        "        {\n",
        "            \"name\": \"news_articles\",\n",
        "            \"processing_steps\": [\n",
        "                {\"filter\": lambda x: len(x[\"author\"]) > 0},  # <--- add filter\n",
        "                {\"map\": lower_title},  # <--- add some transformation\n",
        "            ],\n",
        "            \"endpoint\": {\n",
        "                \"path\": \"everything\",\n",
        "                \"response_actions\": [\n",
        "                    {\n",
        "                        \"status_code\": 200,\n",
        "                        \"action\": debug_response,\n",
        "                    },\n",
        "                ],\n",
        "                \"params\": {\n",
        "                    \"q\": \"python\",\n",
        "                    \"from\": {\n",
        "                        \"type\": \"incremental\",\n",
        "                        \"cursor_path\": \"publishedAt\",\n",
        "                        \"initial_value\": \"2025-04-15T00:00:00Z\",\n",
        "                    },\n",
        "                },\n",
        "            },\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"top_headlines\",\n",
        "            \"endpoint\": {\n",
        "                \"path\": \"top-headlines\",\n",
        "                \"params\": {\"country\": \"us\"},\n",
        "            },\n",
        "        },\n",
        "    ],\n",
        "}\n",
        "\n",
        "news_source = rest_api_source(news_config)\n",
        "\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"news_pipeline\", destination=\"duckdb\", dataset_name=\"news\"\n",
        ")\n",
        "\n",
        "load_info = pipeline.run(news_source)\n",
        "print(pipeline.last_trace)\n",
        "\n",
        "pipeline.dataset().news_articles.df().head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeYM5yu6ejuq"
      },
      "source": [
        "# Links\n",
        "\n",
        "More Information about how to build efficient data pipelines you can find in our official documentation:\n",
        "- `dlt` [Getting Started](https://dlthub.com/docs/getting-started),\n",
        "- [REST API Source](https://dlthub.com/docs/dlt-ecosystem/verified-sources/rest_api),\n",
        "- [REST API Client](https://dlthub.com/docs/general-usage/http/rest-client),\n",
        "- `dlt` [Sources](https://dlthub.com/docs/general-usage/source) and [Resources](https://dlthub.com/docs/general-usage/resource),\n",
        "- [Incremental loading](https://dlthub.com/docs/general-usage/incremental-loading),\n",
        "- Our pre-built [Verified Sources](https://dlthub.com/docs/dlt-ecosystem/verified-sources/),\n",
        "- Available [Destinations](https://dlthub.com/docs/dlt-ecosystem/destinations/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWsyS3M7pnEP"
      },
      "source": [
        "![Lesson_1_Custom_sources_RestAPI_source_and_RESTClient_img2](https://storage.googleapis.com/dlt-blog-images/dlt-advanced-course/Lesson_1_Custom_sources_RestAPI_source_and_RESTClient_img2.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwUVbvi0D6Tp"
      },
      "source": [
        "# Exercise 1\n",
        "\n",
        "Your task is to create a `rest_api_source` configuration for the public **Jaffle Shop API**. This exercise will help you apply what you’ve learned:\n",
        "\n",
        "### API details:\n",
        "- **Base URL:** `https://jaffle-shop.scalevector.ai/api/v1`\n",
        "- **Docs:** [https://jaffle-shop.scalevector.ai/docs](https://jaffle-shop.scalevector.ai/docs)\n",
        "\n",
        "### Endpoints to load:\n",
        "- `/orders`\n",
        "\n",
        "### Requirements:\n",
        "1. Use `rest_api_source` to define your source config.\n",
        "2. This API uses **pagination**. Figure out what type is it.\n",
        "3. Add incremental loading to `orders`, starting from `2017-08-01` and using `ordered_at` as the cursor.\n",
        "4. Add `processing_steps` to `orders`:\n",
        "  - Remove records from orders which `order_total` > 500.\n",
        "\n",
        "\n",
        "\n",
        "### Question:\n",
        "How many rows does resulted table `orders` contain?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uOnw1KOGLTx"
      },
      "outputs": [],
      "source": [
        "pipeline.dataset().orders.df()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70D6czgeId7F"
      },
      "source": [
        "✅ ▶ Well done! Go to [the next lesson.](https://colab.research.google.com/drive/1lQ8VkrGJwZMsVtbkuYympcvbv0_CCgYo#forceEdit=true&sandboxMode=true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTCuVPaqIy2o"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}